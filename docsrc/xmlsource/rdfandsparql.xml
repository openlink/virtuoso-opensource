<chapter label="rdfandsparql.xml" id="rdfandsparql"><title>RDF Database and SPARQL</title>
<abstract>
<para>
Starting with version 4.5, Virtuoso provides built-in support for SPARQL, the standard query language for RDF and the semantic web.

Adoption of SPARQL with Virtuoso is effortless, as any  existing SQL client applications and stored procedures can take advantage of SPARQL simply by using it in the place of or inside SQL queries.  Additionally, Virtuoso offers the standard SPARQL protocol to HTTP clients.
From version 5.0.7, Virtuoso can be used  as the RDF store/query processor of the Jena and Sesame RDF frameworks.
</para>

<para>
This chapter discusses Virtuoso's RDF triple storage and query capabilities. 
This discusses storing RDF data as well as mappping existing relational data into RDF for SPARQL access.  Numerous  SPARQL language extensions and standard compliance are coverd.
</para>
</abstract>
<sect1 id="rdfoverview"><title>Overview</title>
<para>The support of SPARQL in Virtuoso consists of two &quot;layers&quot; -- RDF support in SQL engine and SPARQL front-end compiler that translates SPARQL queries to SQL.</para>
<para>In its core, Virtuoso extends relational storage and SQL language with datatypes and language constructs required for handling RDF data like any &quot;traditional&quot; SQL datatype. There exist datatypes for RDF references (IRIs and blank nodes) and for RDF literals with types and languages. These datatypes are widely supported by built-in functions and there exists convenient conversions between RDF literals and other SQL datatypes.</para>
<para>SQL query language is extended with constructs that are convenient for &quot;very heterogeneous&quot; data of RDF graph. In traditional SQL queries, data types of retrieved values are mostly known in advance from the database schema via explicitly declared column types and types of return values of functions. An RDF graph contains mix of literals of all types, so cast errors are very frequent and a typical query should not terminate after some occasional type error in a huge data set.</para>
<para>SQL &quot;user-defined aggregate&quot; feature is used to support SPARQL DESCRIBE and CONSTRUCT statements, so many RDF triples about a subject can be converted into single &quot;dictionary of triples&quot; or can be written into single RDF/XML or TURTLE document.</para>
<para>SQL query language is also extended with BREAKUP construct that is somewhat &quot;inverse&quot; to aggregate functions. While aggregate functions are useful to convert big number of table rows into one or few rows of aggregated data, BREAKUP turns each &quot;wide&quot; and complete row of a relational table into many &quot;narrow&quot; rows of RDF result set.</para>
<para>

The listed features form a solid background for second part of
implementation -- a preprocessor that is called as soon as an input of
SQL compiler contains SPARQL keyword. The preprocessor expects that
the fragment after SPARQL keyword with well-parenthesized code is a
SPARQL query (or SPARUL statement). SPARQL front-end compiler creates
a text of SQL SELECT statement that replaces the original SPARQL
fragment and SQL compiler continues to read its input without seeing
any difference between the replaced part and any other SQL SELECT, so
one may not only send SPARQL queries over all supported protocols like
ODBC and JDBC but place them inside Virtuoso?PL stored procedures or
use them inside SQL SELECT statements where subqueries are allowed by
syntax.</para>


<para>RDF data can be added to the storage by parsing texts of RDF documents (functions
<link linkend="fn_rdf_load_rdfxml"><function>DB.DBA.RDF_LOAD_RDFXML</function></link>,
<link linkend="fn_rdf_load_rdfxml_mt"><function>DB.DBA.RDF_LOAD_RDFXML_MT</function></link>,
<link linkend="fn_ttlp"><function>DB.DBA.TTLP</function></link>,
<link linkend="fn_ttlp_mt"><function>DB.DBA.TTLP_MT</function></link>), by loading remote RDF resources by SPARUL <link linkend="rdfinsertmethodsload">LOAD</link> statement, by extracting and storing metadata while <link linkend="rdfinsertmethodvirtuosocrawler">crawling non-RDF resources</link> and many other <link linkend="rdfinsertmethods">RDF insert methods</link>. After bulk loading, RDF data can be edited using <link linkend="rdfsparul">SPARUL</link> update language.</para>
<para>The most important feature of Virtuoso SPARQL is that relational data may stay unchanged and not be loaded into RDF storage and they still can be accessed by SPARQL queries after creating appropriate <link linkend="rdfviews">RDF Views</link>. This is the best tool for adding RDF capabilities to existing database applications.</para>
<para>

Virtuoso is a &quot;quad store&quot;, not a &quot;triple
store&quot;. Instead of storing subject-predicate-object triples of
each individual graph in a separate storage (and the storage is
explicitly created before first use), Virtuoso stores
graph-subject-predicate-object quads in one common table (<link
linkend="rdfquadtables">DB.DBA.RDF_QUAD</link>). This simplifies
querying when &quot;interesting&quot; graphs are not known in
advance. E.g., while a SPARQL query is running, Virtuoso <link
linkend="virtuososponger">Sponger</link> can iteratively download
additional data in order to provide as complete an answer as
possible.</para>


<para>Quads stored in DB.DBA.RDF_QUAD are usually referred to as
&quot;physical quads&quot; as opposed to &quot;mapped quads&quot; that
are not really stored in any table at all but still are accessible by
SPARQL queries via RDF views.  The SPARQL processor sees no great  difference
between querying physical and mapped quads so a query may operate with
mix of data of all sorts. Thus, an application can easily provide both
access to relational data for SPARQL clients (HTTP and WSDL via <link
linkend="rdfsparqlprotocolendpoint">SPARQL protocol endpoint</link>)
and access to RDF data for traditional RDBMS clients (by passing
SPARQL queries via ODBC, JDBC and the like).</para>


<para>RDF is for data integration. This implies the need of resolving
both inconsistencies in RDF data that comes from various sources of
different nature and problems with queries that work fine on some
sample data from local data warehouse but fail on massive and/or
unexpected data from third party sources. <link
linkend="rdfsparqlrule">RDF inference</link> helps to support variety
of synonyms when different data sources uses different names for same
classes, properties and subjects. Queries can be debugged using all
methods that are traditional for SQL (what's executed is SQL), with
paying some attention to RDF-specific <link
linkend="rdfperformancetuning">performance tuning</link>.</para>

<para>

The whole purpose of SPARQL is to let application developers write
 queries faster than equivalent SQL code. A short SPARQL query
on a mix of physical quads and RDF views of a number of  applications may
sometimes replace pages of SQL text. On the other hand, SQL has more
expressive power than SPARQL so complicated business intelligence
queries should be written in a mixed style, as an SQL query with
SPARQL subqueries. Debugging of mixed-style queries is inconvenient,
reducing overall benefit in development time, so <link
linkend="sparqlbi">Business Intelligence Extensions for SPARQL</link>
can be used in order to greatly reduce the need for wrapping SQL
around SPARQL. Further, SPARQL protocol clients get access to the functionality, without needing SQL privileges. <link linkend="rdfsparqlrulefulltext">Full-text search
in SPARQL</link> eliminates one more reason for mixing SQL and
SPARQL. In addition, extending SPARQL let SPARQL clients to delegate
more business intelligence calculations to the server without any
changes in the infrastructure; this is especially important for web
applications that use AJAX tools (say, <ulink
url="http://oat.openlinksw.com">OAT OpenLink AJAX
Toolkit</ulink>).</para>

</sect1>
<sect1 id="rdfdatarepresentation"><title>Data Representation</title>
<para>This section covers how Virtuoso stores RDF triples. The IRI_ID built-in data type is introduced, along with the default table structures used for triple persistency.
These details are mostly hidden from users of RDF, thus this section is not necessary reading for typical use of Virtuoso with RDF.
</para>
<sect2 id="rdfiriidtype"><title>IRI_ID Type</title>
<para>The central notion of RDF is the IRI, or URI, which serves as the globally unique label of named nodes. The subject and predicate of a triple are always IRI's and the object may be an IRI or any other XML Schema scalar data type. In any case, an IRI is always distinct from any instance of any other data type.</para>
<para>Virtuoso supports a native IRI_ID data type, internally an unsigned 32 bit or unsigned 64 bit integer value.
Small databases can use 32 bit values but if database becomes big then the administrator should execute
<function>DB.DBA.RDF_64BIT_UPGRADE</function>() procedure that will switch to 64-bit values. This procedure takes time so if it is known in advance that the database will grow to billions of nodes then it could be convenient to upgrade it while it is empty.
An IRI_ID is never equal to any instance of any other type.</para>
<para>Thus, the object column of a table storing triples can be declared as ANY and IRI values will be distinguishable without recourse to any extra flag and IRI's will naturally occupy their own contiguous segment in the ANY type collation sequence. Indices can be defined over such columns. An IRI_ID is never automatically cast into any other type nor any other type into IRI_ID.</para>
<para>The functions iri_id_num (in i IRI_ID) and iri_id_from_num (in n INT) convert between signed integers and IRI_ID's. The function isiri_id (in i any) returns nonzero if the argument is of type IRI_ID, zero otherwise.</para>
<para>The syntax for an IRI_ID literal is <emphasis>#i&lt;NNN&gt;</emphasis> or <emphasis>#ib&lt;NNN&gt;</emphasis>, where <emphasis>&lt;NNN&gt;</emphasis> is up to 20 decimal digits. <emphasis>#i12345</emphasis> is equal to <emphasis>iri_id_from_num (12345)</emphasis> and <emphasis>#ib12345</emphasis> is equal to <emphasis>iri_id_from_num (12345) + min_64bit_bnode_iri_id ()</emphasis>.</para>	
<para>When received by a SQL client application, the ODBC driver or
interactive SQL will bind an IRI_ID to a character buffer, producing
the <emphasis>#i&lt;NNN&gt;</emphasis> syntax. When passing IRI_ID's from a client, one can pass an
integer and use the iri_id_from_num () function in the statement to
convert server side. A SQL client will normally not be exposed to
IRI_ID's since the SPARQL implementation returns IRI's in their text
form, not as internal id's. These will however be seen if reading the
internal tables directly.</para>
<note><para>Nobody, even DBA, should write directly to internal RDF tables, because some data from that tables are cached in a special way and cache is not automatically updated when content of tables has changed.</para></note>
</sect2>
<sect2 id="rdfboxtype"><title>RDF_BOX Type</title>
<para>While strings, numbers, dates and XML entities are &quot;native&quot; SQL datatypes,
RDF literal with non-default type or language have no exact matches among standard SQL types.
Virtuoso introduces a special data type called "RDF_BOX" in order to handle that cases.
Instance of RDF_BOX consists of data type, language, the content (or beginning characters of a long content)
and a possible reference to DB.DBA.RDF_OBJ table if the object is too long to be held in-line in some table or should be outlined for free-text indexing.</para>
<para>Usually applications do not need to access internals of an RDF boxes. This datatype is used in system tables but almost all SPARQL and RDF operations use standard SQL datatypes for arguments and return values.</para>
</sect2>
<sect2 id="rdfquadtables"><title>RDF_QUAD and other tables</title>
<para>The main tables of the default RDF storage system are:</para>
<programlisting>
create table DB.DBA.RDF_QUAD (
  G IRI_ID,
  S IRI_ID,
  P IRI_ID,
  O any,
  primary key (G,S,P,O) );
create bitmap index RDF_QUAD_OGPS on DB.DBA.RDF_QUAD (O, G, P, S);
</programlisting>
<para>Each triple (more correctly, each quad) is represented by one row in RDF_QUAD.
The columns represent the graph, subject, predicate and object.
The IRI_ID type columns reference RDF_IRI, which translates the internal id to the external name of the IRI.
The O column is of type ANY.
If the O value is a non-string SQL scalar, such as a number or date or IRI, it is stored in its native binary representation.
If it is a "very short" string (20 characters or less), it is also stored "as is".
Long strings and RDF literal with non-default type or language are stored as RDF_BOX values.
Instance of rdf_box consists of data type, language, the content (or beginning characters of a long content)
and a possible reference to RDF_OBJ if the object is too long to be held in-line in this table or should be outlined for free-text indexing.
</para>
<programlisting>
create table DB.DBA.RDF_PREFIX (
  RP_NAME varchar primary key,
  RP_ID int not null unique );
create table DB.DBA.RDF_IRI (
  RI_NAME varchar primary key,
  RI_ID IRI_ID not null unique );
</programlisting>
<para>These two tables store a mapping between internal IRI id's and their external string form.
A memory-resident cache contains recently used IRIs to reduce access to this table.
Function id_to_iri (in id IRI_ID) returns the IRI by its ID.
Function iri_to_id (in iri varchar, in may_create_new_id) returns an IRI_ID for given string;
if the string is not used before as an IRI then either NULL is returned or a new ID is allocated, depending on the second argument.
</para>
<programlisting>
create table DB.DBA.RDF_OBJ (
  RO_ID integer primary key,
    RO_VAL varchar,
  RO_LONG long varchar,
  RO_DIGEST any
)
create index RO_VAL on DB.DBA.RDF_OBJ (RO_VAL)
create index RO_DIGEST on DB.DBA.RDF_OBJ (RO_DIGEST)
;
</programlisting>
<para>When an O value of RDF_QUAD is longer than a certain limit or should be free-text indexed, the value is stored in this table.
Depending on the length of the value, it goes into the varchar or the long varchar column.
The RO_ID is contained in rdf_box object that is stored in the O column.
Still, the truncated value of O can be used for determining equality and range matching,
even if &lt; and &gt; of closely matching values need to look at the real string in RDF_OBJ.
When RO_LONG is used to store very long value, RO_VAL contains a simple checksum of the value, to accelerate search for identical values when the table is populated by new values.
</para>
<programlisting>
create table DB.DBA.RDF_DATATYPE (
    RDT_IID IRI_ID not null primary key,
    RDT_TWOBYTE integer not null unique,
    RDT_QNAME varchar );
</programlisting>
<para>The XML Schema data type of a typed string O represented as 2 bytes in the O varchar value. This table maps this into the broader IRI space where the type URI is given an IRI number.</para>
<programlisting>
create table DB.DBA.RDF_LANGUAGE (
    RL_ID varchar not null primary key,
    RL_TWOBYTE integer not null unique );
</programlisting>
<para>The varchar representation of a O which is a string with language has a two byte field for language. This table maps the short integer language id to the real language name such as 'en', 'en-US' or 'x-any'.</para>
<para><emphasis>Note that unlike datatype names, language names are not URIs.</emphasis></para>
<para>A short integer value can be used in both RDF_DATATYPE and RDF_LANGUAGE tables for two different purposes. E.g. an integer 257 is for 'unspecified datatype' as well as for 'unspecified language'.</para>
</sect2>
<sect2 id="rdfsqlmodes"><title>Short, Long and SQL Values</title>
<para>When processing an O, the SPARQL implementation may have it in one of three internal formats, called "valmodes". The below cases apply for strings:</para>
<para>The short format is the format where an O is stored in RDF_QUAD.</para>
<para>The long value is similar to short one but an rdf_box object, that consists of six fields:</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>short integer id of type referencing RDT_TWOBYTE, 257   if the type is not specified,</listitem>
<listitem>the string as inlined in O or as stored in RO_VAL or   RO_LONG,</listitem>
<listitem>the RO_ID if the string is from RDF_OBJ (otherwise   zero),</listitem>
<listitem>the short integer id of language referencing RL_TWOBYTE, 257 if the language is not specified,</listitem>
<listitem>flag whether the stored string value is complete or it is only the beginning that is inlined in O.</listitem>
</itemizedlist>
<para>The SQL value is the string as a narrow string representing the UTF8 encoding of the value, stripped of data type and language tag.</para>
<para>The SQL form of an IRI is the string. The long and short forms are the IRI_ID referencing RU_IRI_ID of RDF_URL.</para>
<para>For all non-string, non-IRI types, the short, long and SQL values are the same SQL scalar of the appropriate native SQL type. A SQL host variable meant to receive an O should be of the ANY type.</para>
<para>The SPARQL implementation will usually translate results to the SQL format before returning them.
Internally, it uses the shortest possible form suited to the operation. For equalities and joining, the
short form is always good. For range comparisons, the long form is needed etc. For arithmetic,
all three forms will do since the arguments are expected to be numbers which are stored as their binary
selves in O, thus the O column unaltered and uncast will do as an argument of arithmetic or numeric
comparison with, say, SQL literal constants.</para>
</sect2>
<sect2 id="rdfxmlschemacompat"><title>Special Cases and XML Schema Compatibility</title>
<para>We note that since we store numbers as the equivalent SQL binary type, we do not preserve the distinction of byte, boolean etc. These all become integer. If preserving such detail is for some reason important, then storage as a typed string is possible but is not done at present for reasons of compactness and performance.</para>
</sect2>
<sect2 id="rdfquietcast"><title>SQL Compiler Support - QUIETCAST option</title>
<para>The type cast behaviors of SQL and SPARQL are different. SQL will generally signal an error when an automatic cast fails. For example, a string can be compared to a date column if the string can be parsed as a date but otherwise the comparison should signal an error. In SPARQL, such situations are supposed to silently fail. Generally, SPARQL is much more relaxed with respect to data types.</para>
<para>These differences will be specially noticed if actual SQL data is processed with SPARQL via some sort of schema mapping translating references to triples into native tables and columns.</para>
<para>Also, even when dealing with the triple-oriented RDF_QUAD table, there are cases of joining between S and O such that the O can be a heterogeneous set of IRI's and other data whereas the S is always an IRI. The non-IRI to IRI comparison should not give cast errors but should silently fail. Also, in order to keep queries simple and easily optimizable, it should not be necessary to introduce extra predicates for testing if the O is n IRI before comparing with the S.</para>
<para>Due to these considerations, Virtuoso introduces a SQL statement option called QUIETCAST. When given in the OPTION clause of a SELECT, it switches to silent fail mode for automatic type casting.</para>
<para>The syntax is as follows:</para>
<programlisting>
select ... from .... option (QUIETCAST)
</programlisting>
<para>This option is automatically added by the SPARQL to SQL translator. The scope is the enclosing procedure body.</para>
</sect2>
<sect2 id="rdfdynamiclocal"><title>Dynamic Renaming of Local IRI's</title>
<para>
    There are cases where it is desirable to have IRI's in RDF storage
    that will change to reflect a change of the host name of the containing
    store.  This is specifically true of DAV resource metadata for local
    DAV resources.
    Such IRI's must be stored prefixed with <computeroutput>local:</computeroutput>.  
</para>
<para>
    If a user application makes statements with such a URI, then these statements will be returned with local:
    substituted with a prefix taken from the context as described below.
</para>
<para>
    When returning IRI's from id's, this prefix is replaced by the Host header of the HTTP request 
    and if not running with HTTP, with the DefaultHost from URIQA.  This behavior is always in effect.
</para>

<para>
    When converting strings to IRI id's, the <computeroutput>local:</computeroutput> prefix may or may not be introduced depending on ini file and other context factors.
    If <link linkend="VIRTINI">DynamicLocal</link> defined in the [URIQA] section of the Virtuoso INI file is on and the host part of the IRI matches the Host header of the HTTP request in context or the DefaultHost if outside of HTTP context, then this is replaced with local: before looking up the IRI ID.  Even if DynamicLocal is not on and the <computeroutput>local:</computeroutput> prefix occurs in the IRI string being translated to id, the translating the IRI_ID back to the IRI name will depend on the context as described above.
</para>
<para>
    The effects of DynamicLocal = 1 can be very confusing since many names
    can refer to the exact same thing.  For example, if the DefaultHost is
    dbpedia.org,
    <computeroutput>iri_to_id ('http://dbpedia.org/resource/Paris') = iri_to_id ('local:///resource/Paris) </computeroutput>
    is true and so is  
    <computeroutput>'http://dbpedia.org/resource/Paris' = id_to_iri (iri_to_id ('local://resource/Paris'))</computeroutput>
    These hold in a SQL client context, i.e. also when connected through RDF frameworks like Jena or Sesame.
    When running a SPARQL protocol request, the Host: header influences the behavior, likewise when using web interactive SQL in Conductor.
    Also be careful when loading RDF files that may have URI's corresponding to the local host name.
</para>
</sect2>
</sect1>
<sect1 id="rdfapiandsql"><title>RDF and SPARQL API and SQL</title>
<para>
SPARQL can be used inline wherever SQL can be used.  
The only API functions that one needs to know are the ones for loading RDF data into the store.
Dynamic SQL client applications can issue SPARQL queries against Virtuoso through the regular SQL client API, ODBC, JDBC or other, simply by prefixing the SPARQL query with the SPARQL keyword.  Parameters work just as with dynamic SQL.
Stored procedures can have SPARQL expressions inline and can declare cursors over SPARQL result sets.
</para>

<para>
Value conversions between SQL and SPARQL are most often automatic and
invisible.  In some cases one needs to be aware of the different
SPARQL value representations (valmodes). SPARQL offers declarations
for determining if graphs to be returned are to be represented as XML
or Turtle text serialization or whether these will be hash tables of
triples. See <link linkend="fn_dict_new"><function>dict_new()</function></link> and related functions for a description of the hash table SQL data type.
The use of dict's is convenient for further programmatic processing of graphs.
</para>

<para>RDF-related procedures use Virtuoso/PL vectors
and dictionaries to represent RDF triples and sets of triples.</para>
<para><emphasis>Valmode</emphasis> means the "format of values returned by an
expression", i.e. 'short', 'long' or 'SQL value'.</para>
<para><emphasis>Triple vector</emphasis> is a vector (array) of S, P and O, where all values are in
'long' formats, i.e. IRI_ID's for IRI values, numbers or datetimes for corresponding XMLSchema types, special &quot;RDF box&quot; objects if O is neither string nor IRI.</para>
<para><emphasis>Dictionary of triples</emphasis> or <emphasis>Hash table of triples</emphasis> is an
dictionary object made by the SQL function <emphasis>dict_new ()</emphasis> whose keys are
triple vectors and values are not specified; this is a good storage
format for an unordered set of distinct triples.</para>
<para><emphasis>Dictionary of blank node names</emphasis> is a dictionary used for tricky
processing of a number of TURTLE or RDF /XML descriptions of subgraphs
that come from a common graph. Imagine a situation where  different
descriptions actually refer to the same blank nodes of the original graph
and, moreover, the application that generates these descriptions always
generates the same blank node id string for the same node. A reader of
descriptions can correctly  join described subgraphs into one big
subgraph by filling in a dictionary that contains blank node id strings
as keys and IRI_ID's assigned to that strings as dependent data. As
soon as all readers of an application share the same dictionary of
nodes created before, no blank node is created twice;</para>

<sect2 id="rdfsparqlinline"><title>SPARQL Inline in SQL</title>
<para>Virtuoso extends the SQL 92 syntax with SPARQL queries and subqueries. Instead of writing a SQL SELECT query or subquery, one can write the SPARQL keyword and a SPARQL query after the keyword.</para>
<programlisting>
SQL>sparql select distinct ?p where { graph ?g { ?s ?p ?o } };
p
varchar
----------
http://example.org/ns#b
http://example.org/ns#d
http://xmlns.com/foaf/0.1/name
http://xmlns.com/foaf/0.1/mbox
...


SQL>select distinct subseq ("p", strchr ("p", '#')) as fragment
  from (sparql select distinct ?p where { graph ?g { ?s ?p ?o } } ) as all_predicates
  where "p" like '%#%';
fragment
varchar
----------
#query
#data
#name
#comment
...
</programlisting>
<para>Note that names of variables returned from SPARQL are always case-sensitive and no case mode rules apply to them.
Depending on CaseMode parameter in the virtuoso configuration file, double quotes should be used to refer to them in surrounding SQL code.
</para>
<para>
It is possible to pass parameters to a  SPARQL query via a Virtuoso-specific syntax extension.
<emphasis>??</emphasis> or <emphasis>$?</emphasis> indicates a positional parameter similar to <emphasis>?</emphasis> in plain SQL. <emphasis>??</emphasis> can be used in graph patterns or anywhere in the place of a SPARQL variable.
The value of a parameter should be passed in SQL form, i.e. this should be a number or a untyped string.
An IRI ID can be passed in all cases where an absolute IRI can, except the obvious case when the variable is an argument of a function that requires string.
If the parameter is used in 'graph', 'subject' or 'object' position of the sparql pattern, the string parameter is converted into IRI automatically.
In other cases, IRI string is indistinguishable from string literal, so there is a need in call of <emphasis>iri()</emphasis> built-in SPARQL function, like <emphasis>iri (??)</emphasis>.
Using this notation, any dynamic SQL client, whether ODBC, JDBC or other can execute parametrized SPARQL queries, binding parameters just as with dynamic SQL.
</para>
<programlisting><![CDATA[
SQL> create function param_passing_demo ()
{
  declare stat, msg varchar;
  declare mdata, rset any;
  exec ('sparql select ?s where { graph ?g { ?s ?? ?? }}',
    stat, msg,
    vector ( /* Vector of two parameters */
      'http://www.w3.org/2001/sw/DataAccess/tests/data/Sorting/sort-0#int1',
      4 ),
    10, /* Max no of rows */
    mdata, /* Variable to get metadata */
    rset ); /* Variable to get result-set */
  if (length (rset) = 0)
    signal ('23000',
      'No data found, try demo database with installed Virtuoso tutorials');
  return rset[0][0];
}

SQL> select param_passing_demo ();
callret
VARCHAR
_______________________________________________________________________________

http://www.w3.org/2001/sw/DataAccess/tests/data/Sorting/sort-0#four

1 Rows. -- 00000 msec.
]]></programlisting>
<para>An inline  SPARQL query can refer to SQL variables that are in scope in the SQL query or stored procedure containing it.
Virtuoso extends the SPARQL syntax with a special notation to this effect. A reference to SQL variable X can be written as <emphasis>?:X</emphasis> or <emphasis>$:X</emphasis>.
A reference to column <emphasis>C</emphasis> of table or sub-select with alias <emphasis>T</emphasis> can be written as <emphasis>?:T.C</emphasis> or <emphasis>$:T.C</emphasis>.
Both notations can be used in any place where a variable name is allowed, except 'AS' clause described below.
</para>
<para>A column of a result set of a SPARQL SELECT can be used in SQL code inside a for statement just like any column from a SQL select.
</para>
<para>SQL rules about double-quoted names are applicable to variables that are passed to a SPARQL query or selected  from one.
If a variable name contains unusual characters or should not be normalized according to SQL conventions then the
name should use double quotes for escaping. E.g., the notation <emphasis>?:"OrderLine"</emphasis> will always refer to variable or column
titled <emphasis>OrderLine</emphasis> whereas <emphasis>?:OrderLine</emphasis> can be converted to <emphasis>ORDERLINE</emphasis> or <emphasis>orderline</emphasis>.
</para>
<para>It is safer to avoid using variable names that conflict with column names of RDF system tables, esp. <emphasis>G</emphasis>, <emphasis>S</emphasis>, <emphasis>P</emphasis> and <emphasis>O</emphasis>.
These names are not reserved now but they may cause subtle bugs when an incorrect SPARQL subquery is compiled into SQL code that refers to table columns of same names.
Some of these names may be rejected as syntax errors by future Virtuoso versions.
</para>
<programlisting><![CDATA[
SQL> create procedure sql_vars_demo ()
{
#pragma prefix sort0: <http://www.w3.org/2001/sw/DataAccess/tests/data/Sorting/sort-0#>
  declare RES varchar;
  declare obj integer;
  result_names (RES);
  obj := 4;
  for (sparql select ?subj where { graph ?g { ?subj sort0:int1 ?:obj } } ) do
    result ("subj");
}

SQL> sql_vars_demo ();
RES
VARCHAR
_______________________________________________________________________________

http://www.w3.org/2001/sw/DataAccess/tests/data/Sorting/sort-0#four

1 Rows. -- 00000 msec.
]]></programlisting>
<para>The example also demonstrates the Virtuoso/PL pragma line for procedure-wide declarations of namespace prefixes.
This makes the code more readable and eliminates duplicate declarations of namespace prefixes when the procedure
contains many SPARQL fragments that refer to a common set of namespaces.
</para>
<para>SPARQL ASK query can be used as an argument of the SQL EXISTS predicate.
</para>
<programlisting><![CDATA[
create function sparql_ask_demo () returns varchar
{
  if (exists (sparql ask where { graph ?g { ?s ?p 4}}))
    return 'YES';
  else
    return 'NO';
}

SQL> select sparql_ask_demo ();
_______________________________________________________________________________

YES
]]>
</programlisting>
<sect3 id="rdfcontrollingsparqloutputtypes"><title>Controlling SPARQL Output Data Types</title>
<para>The compilation of a SPARQL query may depend on environment that is usually provided by the SPARQL protocol, including name of default graph URI. Environment settings that come from protocol may override settings in the text of SPARQL query. To let an application configure the environment for a query,
 SPARQL syntax is extended with the 'define' clause:</para>
<programlisting>
define parameter-qname parameter-value
</programlisting>
<para>Examples of supported parameters are <emphasis>output:valmode</emphasis> and <emphasis>output:format</emphasis></para>
<para><emphasis>output:valmode</emphasis> sets the SQL representation used for values in the result set.
In most cases applications need SQL values to be returned by SPARQL.
By default the query returns a result set of values in SQL format and behaves as a typical SQL select.
To compose triple vectors in Virtuoso/PL code application may need data in long format.
If the query contains a</para>
<programlisting>
define output:valmode 'LONG'
</programlisting>
<para>clause then all returned values are in long format. E.g., the following query returns IRI_ID's instead of IRI strings.</para>
<programlisting>
SQL>sparql define output:valmode 'LONG' select distinct ?p where { graph ?g { ?s ?p ?o } };
p
----------
#i1000001
#i1000003
#i1000005
#i1000006
...
</programlisting>
<para><emphasis>output:format</emphasis> instruct SPARQL compiler that the result of the query should be serialized into an RDF document;
that document will be returned as  a single column  of a single  row result set.
<emphasis>output:format</emphasis> is especially useful if SPARQL CONSTRUCT or SPARQL DESCRIBE query is executed directly via ODBC or JDBC database connection
and the client can not receive the resulting dictionary of triples (there's no way to transfer such an object via ODBC).
Using this option, the  client can receive the document that contains the whole result set of a SELECT or the dictionary of triples of a CONSTRUCT/DESCRIBE, and parse it locally.
</para>
<para>
Supported values for <emphasis>output:format</emphasis> are <emphasis>RDF/XML</emphasis> and <emphasis>TURTLE</emphasis> (or <emphasis>TTL</emphasis>).
If both <emphasis>output:valmode</emphasis> and <emphasis>output:format</emphasis> are specified, <emphasis>output:format</emphasis> has higher priority,
raising an error if <emphasis>output:valmode</emphasis> is set to a value other than <emphasis>LONG</emphasis>.
</para>
<para>
When a SPARQL query is compiled, the compiler checks whether the result set is sent to the remote ODBC/JDBC client or used in some other way.
The compiler will automatically set <emphasis>output:format</emphasis> to <emphasis>TURTLE</emphasis> if compiling for execution by an SQL client.
</para>
<para>
The example below demonstrates how different values of <emphasis>output:format</emphasis> affect the result of SPARQL SELECT.
Note 10 rows and 4 columns in the first result, and single LONG VARCHAR in two others.
Using the ISQL  client, use 'set blobs on;' directive to fetch long texts without 'data truncated' warning.
</para>
<programlisting><![CDATA[
SQL> sparql select * where {graph ?g { ?s ?p ?o }} limit 10;
g                                            s                    p                              o
VARCHAR                                      VARCHAR              VARCHAR                        VARCHAR
______________________________________________________________________

http://local.virt/DAV/bound/manifest.rdf     nodeID://1000000000  http://example.com/test#query  http://local.virt/DAV/bound/bound1.rq
. . .
http://local.virt/DAV/examples/manifest.rdf  nodeID://1000000019  http://example.com/test#query  http://local.virt/DAV/examples/ex11.2.3.1_1.rq

10 Rows. -- 00000 msec.

SQL> sparql define output:format "TTL" select * where {graph ?g { ?s ?p ?o }} limit 10;
callret-0
LONG VARCHAR
_______________________________________________________________________________

@prefix :rdf <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix :rs <http://www.w3.org/2005/sparql-results#> .
@prefix :xsd <http://www.w3.org/2001/XMLSchema#> .
[ rdf:type rs:results ;
  rs:result [
      rs:binding [ rs:name "g" ; rs:value <http://local.virt/DAV/bound/manifest.rdf> ] ;
      rs:binding [ rs:name "s" ; rs:value _:nodeID1000000000 ] ;
      rs:binding [ rs:name "p" ; rs:value <http://example.com/test#query> ] ;
      rs:binding [ rs:name "o" ; rs:value <http://local.virt/DAV/bound/bound1.rq> ] ;
      ] ;

. . .

  rs:result [
      rs:binding [ rs:name "g" ; rs:value <http://local.virt/DAV/examples/manifest.rdf> ] ;
      rs:binding [ rs:name "s" ; rs:value _:nodeID1000000019 ] ;
      rs:binding [ rs:name "p" ; rs:value <http://example.com/test#query> ] ;
      rs:binding [ rs:name "o" ; rs:value <http://local.virt/DAV/examples/ex11.2.3.1_1.rq> ] ;
      ] ;
    ] .

1 Rows. -- 00000 msec.

SQL> sparql define output:format "RDF/XML" select * where {graph ?g { ?s ?p ?o }} limit 10;
callret-0
LONG VARCHAR
_______________________________________________________________________________

<rdf:RDF
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:rs="http://www.w3.org/2005/sparql-results#"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#" >
   <rs:results rdf:nodeID="rset">
  <rs:result rdf:nodeID="sol206">
   <rs:binding rdf:nodeID="sol206-0" rs:name="g"><rs:value rdf:resource="http://local.virt/DAV/bound/manifest.rdf"/></rs:binding>
   <rs:binding rdf:nodeID="sol206-1" rs:name="s"><rs:value rdf:nodeID="1000000000"/></rs:binding>
   <rs:binding rdf:nodeID="sol206-2" rs:name="p"><rs:value rdf:resource="http://example.com/test#query"/></rs:binding>
   <rs:binding rdf:nodeID="sol206-3" rs:name="o"><rs:value rdf:resource="http://local.virt/DAV/bound/bound1.rq"/></rs:binding>
  </rs:result>

. . .

  <rs:result rdf:nodeID="sol5737">
   <rs:binding rdf:nodeID="sol5737-0" rs:name="g"><rs:value rdf:resource="http://local.virt/DAV/examples/manifest.rdf"/></rs:binding>
   <rs:binding rdf:nodeID="sol5737-1" rs:name="s"><rs:value rdf:nodeID="1000000019"/></rs:binding>
   <rs:binding rdf:nodeID="sol5737-2" rs:name="p"><rs:value rdf:resource="http://example.com/test#query"/></rs:binding>
   <rs:binding rdf:nodeID="sol5737-3" rs:name="o"><rs:value rdf:resource="http://local.virt/DAV/examples/ex11.2.3.1_1.rq"/></rs:binding>
  </rs:result>
 </rs:results>
</rdf:RDF>

1 Rows. -- 00000 msec. 
]]></programlisting>
<para>SPARQL CONSTRUCT and SPARQL DESCRIBE results are serialized as one would expect:</para>
<programlisting><![CDATA[
SQL> sparql define output:format "TTL" construct { ?s ?p "004" } where {graph ?g { ?s ?p 4 }};
callret-0
LONG VARCHAR
_______________________________________________________________________________

<http://www.w3.org/2001/sw/DataAccess/tests/data/Sorting/sort-0#four> <http://www.w3.org/2001/sw/DataAccess/tests/data/Sorting/sort-0#int1> "004" .
_:b1000000913 <http://www.w3.org/2001/sw/DataAccess/tests/result-set#index> "004" .


1 Rows. -- 00000 msec.

SQL> sparql define output:format "RDF/XML" construct { ?s ?p "004" } where {graph ?g { ?s ?p 4 }};
callret-0
LONG VARCHAR
_______________________________________________________________________________

<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
<rdf:Description about="http://www.w3.org/2001/sw/DataAccess/tests/data/Sorting/sort-0#four"><ns0pred:int1 xmlns:ns0pred="http://www.w3.org/2001/sw/DataAccess/tests/data/Sorting/sort-0#">004</ns0pred:int1></rdf:Description>
<rdf:Description rdf:nodeID="b1000000913"><ns0pred:index xmlns:ns0pred="http://www.w3.org/2001/sw/DataAccess/tests/result-set#">004</ns0pred:index></rdf:Description>
</rdf:RDF>

1 Rows. -- 00000 msec. 
]]></programlisting>
<para>SPARQL ASK returns a non-empty result set if the match is found for graph pattern, empty result-set otherwise. If <emphasis>output:format</emphasis> is specified then the query makes a 'boolean result' document instead:</para>
<programlisting><![CDATA[
SQL> sparql ask where {graph ?g { ?s ?p 4 }};
__ask_retval
INTEGER
_______________________________________________________________________________

1

1 Rows. -- 00000 msec.

SQL> sparql ask where {graph ?g { ?s ?p "no such" }};
__ask_retval
INTEGER
_______________________________________________________________________________


0 Rows. -- 00000 msec.

SQL> sparql define output:format "TTL" ask where {graph ?g { ?s ?p 4 }};
callret
VARCHAR
_______________________________________________________________________________

@prefix :rdf <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
 @prefix :rs <http://www.w3.org/2005/sparql-results#> .
[ rdf:type rs:results ; rs:boolean TRUE ]

1 Rows. -- 00000 msec.

SQL> sparql define output:format "RDF/XML" ask where {graph ?g { ?s ?p 4 }};
callret
VARCHAR
_______________________________________________________________________________

<rdf:RDF
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:rs="http://www.w3.org/2005/sparql-results#"
  xmlns:xsd="http://www.w3.org/2001/XMLSchema#" >
   <rs:results rdf:nodeID="rset">
    <rs:boolean rdf:datatype="http://www.w3.org/2001/XMLSchema#boolean">1</rs:boolean></results></rdf:RDF>

1 Rows. -- 00000 msec.
]]></programlisting>
</sect3>
</sect2>



<sect2 id="rdfapi">
<title>API Functions</title> 

<sect3
id="rdfapidataimport"><title>Data Import</title>
<para>DB.DBA.TTLP() parses TTL (TURTLE or N3 resource) and places its triples into DB.DBA.RDF_QUAD.</para>
<programlisting>
create procedure DB.DBA.TTLP (
    in strg any,       -- text of the resource
    in base varchar,  -- base IRI to resolve relative IRIs to absolute
    in graph varchar, -- target graph IRI, parsed triples will appear in that graph.
    in flags int)   -- bitmask of flags that permit some sorts of syntax errors in resource, use 0.
</programlisting>
<para>For loading a file of any greater length, it is most practical to use
the file_to_string_output function.
</para>
<para>It is important the file to be accessible for the Virtuoso server. You need to have set properly the
<emphasis>DirsAllowed</emphasis> parameter value in section [Parameters] of the Virtuoso database INI file.
For example on Windows it could be:
</para>
<programlisting>
virtuoso.ini file:
[Parameters]
...
DirsAllowed =  .\tmp
...
</programlisting>
<para>So in the example the file you want to import from, should be in the tmp folder or in its subfolder.
Note that this example folder is a subfolder of the Virtuoso Server working directory.
</para>
<programlisting>
SQL> DB.DBA.TTLP (file_to_string_output ('.\tmp\data.ttl'), '', 'http://my_graph', 0);
</programlisting>
<para>The DB.DBA.TTLP_MT() procedure is like DB.DBA.TTLP() but loads the file on multiple threads,
using parallel I/O and multiprocessing if available. The functions does not leave a transaction log.
Hence, after successful load, one should execute the checkpoint statement to make sure that a
server restart does not wipe out the results.
</para>
<programlisting>
create procedure DB.DBA.TTLP_MT (
    in strg any,       -- text of the resource
    in base varchar,   -- base IRI to resolve relative IRIs to absolute
    in graph varchar,  -- target graph IRI, parsed triples will appear in that graph.
    in flags int) -- flags, use 0
</programlisting>
<para>For loading large resources when transactional integrity is not important (loading of a single resource may take more than one transaction)
you can use also the <emphasis>DB.DBA.RDF_LOAD_RDFXML_MT()</emphasis> procedure:</para>
<programlisting>
create procedure DB.DBA.RDF_LOAD_RDFXML_MT (
    in strg varchar,  -- text of the resource
    in base varchar,  -- base IRI to resolve relative IRIs to absolute
    in graph varchar) -- target graph IRI, parsed triples will appear in that graph.
</programlisting>
<para>The following example demonstrates importing data from the rdf resource with URI: http://www.w3.org/People/Berners-Lee/card</para>
<programlisting><![CDATA[
SQL>create procedure MY_LOAD_FILE (in full_uri varchar, in in_resultset integer := 0)
{
  declare REPORT varchar;
  declare graph_uri, dattext varchar;
  declare app_env any;
  app_env := null;
  whenever sqlstate '*' goto err_rep;
  if (not in_resultset)
    result_names (REPORT);
  dattext := cast (XML_URI_GET_AND_CACHE (full_uri) as varchar);
  MY_SPARQL_REPORT (sprintf ('Downloading %s: %d bytes',
      full_uri, length (dattext) ) );
  graph_uri := full_uri;
  delete from RDF_QUAD where G = DB.DBA.RDF_MAKE_IID_OF_QNAME (graph_uri);
  DB.DBA.RDF_LOAD_RDFXML_MT (dattext, full_uri, graph_uri);
  return graph_uri;
err_rep:
  result (sprintf ('%s: %s', __SQL_STATE, __SQL_MESSAGE));
  return graph_uri;
}
;

Done. -- 0 msec.

SQL>create procedure MY_SPARQL_REPORT(in strg varchar)
{
  if (__tag(strg) <> 182)
    strg := cast (strg as varchar) || sprintf (' -- not a string, tag=%d', __tag(strg));
  strg := replace (strg, 'SPARQL_DAV_DATA_URI()', '\044{SPARQL_DAV_DATA_URI()}');
  strg := replace (strg, 'SPARQL_DAV_DATA_PATH()', '\044{SPARQL_DAV_DATA_PATH()}');
  strg := replace (strg, 'SPARQL_FILE_DATA_ROOT()', '\044{SPARQL_FILE_DATA_ROOT()}');
  result (strg);
}
;

Done. -- 0 msec.

SQL> MY_LOAD_FILE('http://www.w3.org/People/Berners-Lee/card');
REPORT
VARCHAR
_______________________________________________________________________________

Downloading http://www.w3.org/People/Berners-Lee/card: 17773 bytes

1 Rows. -- 4046 msec.

SQL>sparql
select *
from <http://www.w3.org/People/Berners-Lee/card>
where {?s ?p ?o} ;

s                                             p                                               o
VARCHAR                                       VARCHAR                                         VARCHAR
__________________________________________________________________________________________________________

http://bblfish.net/people/henry/card#me       http://xmlns.com/foaf/0.1/name                  Henry Story
http://www.w3.org/People/Berners-Lee/card#i   http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://xmlns.com/foaf/0.1/Person
http://www.w3.org/People/Berners-Lee/card#i   http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/2000/10/swap/pim/contact#Male
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/nick                  TimBL
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/nick                  timbl
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/mbox                  mailto:timbl@w3.org
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/mbox_sha1sum          965c47c5a70db7407210cef6e4e6f5374a525c5c
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/knows                 http://bblfish.net/people/henry/card#me
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/knows                 http://hometown.aol.com/chbussler/foaf/chbussler.foaf#me
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/knows                 http://danbri.org/foaf#danbri
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/knows                 http://norman.walsh.name/knows/who#norman-walsh
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/knows                 http://www.aaronsw.com/about.xrdf#aaronsw
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/knows                 http://www.ivan-herman.net/foaf.rdf#me
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/knows                 http://www.w3.org/People/Berners-Lee/card#amy
http://www.w3.org/People/Berners-Lee/card#i   http://xmlns.com/foaf/0.1/knows                 http://dig.csail.mit.edu/People/RRS
..........

]]></programlisting>
<para>The DB.DBA.RDF_TTL2HASH() does not load TTL content, instead it returns a dict of triples in 'long valmode'.</para>
<programlisting>
create function DB.DBA.RDF_TTL2HASH (
    in strg any,
    in base varchar,
    in graph varchar ) returns any
</programlisting>
<para>Parameter <emphasis>flags</emphasis> is useful when syntax of resource is TURTLE-like but not correct TURTLE.
By default, use zero value.
Add 1 to let string literals contain end-of-line characters.
Add 2 to suppress error messages on blank node verbs.
Add 4 to let variables instead of blank nodes.
Add 8 to silently skip triples with literal subjects.
</para>
<para>The DB.DBA.RDF_LOAD_RDFXML() procedure parses RDF/XML and places its triples into DB.DBA.RDF_QUAD.</para>
<programlisting>
create procedure DB.DBA.RDF_LOAD_RDFXML (
    in strg any,           -- text of and XML document
    in base_iri varchar,   -- base IRI to resolve relative IRIs
    in graph_iri varchar ) -- the IRI of destination graph
</programlisting>
<para>To insert a single quad into DB.DBA.RDF_QUAD() table, use one of these procedures:</para>
<programlisting>
-- Simple insertion of a quad where object is a node
create procedure DB.DBA.RDF_QUAD_URI (
  in g_uri varchar, in s_uri varchar, in p_uri varchar,
  in o_uri varchar ) -- IRI string or IRI_ID

-- Simple insertion of a quad where object is a literal value in 'SQL valmode'
create procedure DB.DBA.RDF_QUAD_URI_L (
  in g_uri varchar, in s_uri varchar, in p_uri varchar,
  in o_lit any ) -- string, number or datetime, NULL is not allowed

create procedure DB.DBA.RDF_QUAD_URI_L_TYPED (
  in g_uri varchar, in s_uri varchar, in p_uri varchar,
  in o_lit any,     -- string value of the literal
  in dt any,        -- datatype as IRI string or IRI_ID, can be NULL
  in lang varchar ) -- language as string or NULL
</programlisting>
<para>Arguments g_uri, s_uri and p_uri of these three functions should be IRI strings or IRI_IDs.
All string arguments should be in UTF-8 encoding, otherwise they will be stored but are not queryable via SPARQL.</para>
</sect3>
<sect3 id="rdfapidataexport"><title>Data Export</title>
<para>These two procedures serializes vector of triples into a session, in TURTLE or RDF/XML syntax.
In current version, every triple is printed in separate top-level record (say, in rdf:Description tag), without any pretty-print or nesting optimization.
</para>
<programlisting>
create procedure DB.DBA.RDF_TRIPLES_TO_TTL (
    inout triples any, -- vector of triples in 'long valmode'.
    inout ses any )    -- an output stream in server default encoding

create procedure DB.DBA.RDF_TRIPLES_TO_RDF_XML_TEXT (
    inout triples any,          -- vector of triples in 'long valmode'.
    in print_top_level integer, -- zero if only rdf:Description tags should be written,
      -- non-zero if the rdf:RDF top-level element should also be written
    inout ses any )             -- an output stream in server default encoding
</programlisting>
</sect3>
<sect3 id="rdfapidataquery"><title>Data query</title>
<programlisting>
-- Local execution of SPARQL via SPARQL protocol, produces a result set of SQL values.
create procedure DB.DBA.SPARQL_EVAL (
    in query varchar,      -- text of SPARQL query to execute
    in dflt_graph varchar, -- default graph IRI, if not NULL then this overrides what's specified in query
    in maxrows integer )   -- limit on numbers of rows that should be returned.

-- Similar to SPARQL_EVAL, but returns a vector of vectors of SQL values.
create function DB.DBA.SPARQL_EVAL_TO_ARRAY (
    in query varchar,      -- text of SPARQL query to execute
    in dflt_graph varchar, -- default graph IRI, if not NULL then this overrides what's specified in query
    in maxrows integer )   -- limit on numbers of rows that should be returned.
returns any
</programlisting>
<programlisting>
-- Remote execution of SPARQL via SPARQL protocol, produces a result set of SQL values.
create procedure DB.DBA.SPARQL_REXEC (
    in service varchar,    -- service URI to call via HTTP
    in query varchar,      -- text of SPARQL query to execute
    in dflt_graph varchar, -- default graph IRI, if not NULL then this overrides what's specified in query
    in named_graphs any,   -- vector of named graph IRIs, if not NULL then this overrides what's specified in query
    in req_hdr any,        -- additional HTTP header lines that should be passed to the service; 'Host: ...' is most popular.
    in maxrows integer,    -- limit on numbers of rows that should be returned.
    in bnode_dict any )    -- dictionary of bnode ID references.

-- Similar to SPARQL_REXEC (), but returns a vector of vectors of SQL values.
-- All arguments are the same.
create function DB.DBA.SPARQL_REXEC_TO_ARRAY (
    in service varchar, in query varchar, in dflt_graph varchar, in named_graphs any,
    in req_hdr any, in maxrows integer, in bnode_dict any)
returns any

-- Similar to SPARQL_REXEC (), but fills in output parameters with metadata (like exec metadata) and a vector of vector
s of 'long valmode' values.
-- First seven arguments are the same.
create procedure DB.DBA.SPARQL_REXEC_WITH_META (
    in service varchar, in query varchar, in dflt_graph varchar, in named_graphs any,
    in req_hdr any, in maxrows integer, in bnode_dict any,
    out metadata any,  -- metadata like exec () returns.
    out resultset any) -- results as 'long valmode' value.
</programlisting>
<para>If the query is a CONSTRUCT or DESCRIBE then the result set consists of a single row and single column, the value inside is a dict of triples in 'long valmode'.</para>
</sect3>
</sect2>
<sect2 id="rdfinternalfunctions"><title>Useful Internal Functions</title>
<sect3 id="rdfinternalconversion"><title>Conversion Functions for XMLSchema/RDF Data Serialization Syntax</title>
<para>These functions emulate constructor functions from XQuery Core Function Library.</para>
<programlisting>
create function DB.DBA."http://www.w3.org/2001/XMLSchema#boolean" (in strg any) returns integer
create function DB.DBA."http://www.w3.org/2001/XMLSchema#dateTime" (in strg any) returns datetime
create function DB.DBA."http://www.w3.org/2001/XMLSchema#double" (in strg varchar) returns double precision
create function DB.DBA."http://www.w3.org/2001/XMLSchema#float" (in strg varchar) returns float
create function DB.DBA."http://www.w3.org/2001/XMLSchema#integer" (in strg varchar) returns integer
</programlisting>
</sect3>
<sect3 id="rdfinternalpredicates"><title>RDF-specific Predicates</title>
<programlisting>
-- Returns 1 if string s matches pattern p, 0 otherwise
create function DB.DBA.RDF_REGEX (
    in s varchar,            -- source string to check
    in p varchar,            -- regular expression pattern string
    in coll varchar := null) -- unused for now (modes are not yet implemented)

-- Returns 1 if language identifier r matches lang pattern t
create function DB.DBA.RDF_LANGMATCHES (
  in r varchar, -- language identifies (string or NULL)
  in t varchar) -- language pattern (exact name, first two letters or '*')
</programlisting>
</sect3>
</sect2>
<sect2 id="rdfdefaultgraph"><title>Default and Named Graphs</title>
<para>Sometimes the default graph IRI is not known when the SPARQL query is composed. It can be added at the very last moment by providing the IRI in 'define' clause as follows:</para>
<programlisting>
define input:default-graph-uri &lt;http://example.com&gt;
</programlisting>
<para>Such a definition overrides the default graph URI set in query by 'FROM ...' clause (if any).</para>
<para>The query may contain more than one <emphasis>define input:default-graph-uri</emphasis>.
The set of values of <emphasis>input:default-graph-uri</emphasis> has the highest possible priority and can not be redefined in the rest of the text of the query by FROM clauses.</para>
<para>FROM NAMED clauses can be used multiple in one query:</para>
<programlisting><![CDATA[
SELECT ?id
    FROM NAMED <http://example.com/user1.ttl>
    OPTION (get:soft "soft", get:method "GET")
    FROM NAMED <http://example.com/user2.ttl>
    OPTION (get:soft "soft", get:method "GET")
    WHERE { GRAPH ?g { ?id a ?o } };
]]></programlisting>
<para>Similarly, <emphasis>define input:named-graph-uri &lt;http://example.com&gt;</emphasis> is a replacement for FROM NAMED clause</para>
<para>
When Virtuoso receives a SPARQL request via HTTP, the value of default graph can be set in protocol as <emphasis>default-graph-uri</emphasis> HTTP parameter.
Multiple occurrences of this parameter are allowed. This HTTP parameter is converted into <emphasis>define input:default-graph-uri</emphasis>.
There's similar support for <emphasis>named-graph-uri</emphasis> HTTP parameter.
For debugging purposes, graph names set in protocol are sent back in the reply header as <emphasis>X-SPARQL-default-graph: ...</emphasis> and <emphasis>X-SPARQL-named-graph: ...</emphasis> header lines, one line per graph.
</para>
<para>
Web service endpoint may provide different default configurations for different host names mentioned in HTTP request.
This is configured via table <emphasis>DB.DBA.SYS_SPARQL_HOST</emphasis>.
</para>
<programlisting>
create table DB.DBA.SYS_SPARQL_HOST (
  SH_HOST	varchar not null primary key, -- host mask
  SH_GRAPH_URI	varchar,      -- 'default default' graph uri
  SH_USER_URI	varchar,      -- reserved for any use in applications
  SH_DEFINES	long varchar  -- additional defines for requests
)
</programlisting>
<para>
When the SPARQL web service endpoint receives a request it checks the <emphasis>Host</emphasis> HTTP header line.
This line contains zero or more target host names, delimited by commas.
For every host name in the line the service scans the <emphasis>DB.DBA.SYS_SPARQL_HOST</emphasis> in search of row such that host name is like <emphasis>SH_HOST</emphasis>.
The <emphasis>SH_HOST</emphasis> acts as 'pattern' argument for SQL string operator LIKE. If the row is found then the text of SPARQL request is extended.
If default graph is not explicitly set by HTTP parameters and <emphasis>SH_GRAPH_URI</emphasis> is not null then default graph is set to <emphasis>SH_GRAPH_URI</emphasis>.
If <emphasis>SH_DEFINES</emphasis> is not null then it is added in front of query so this field is a good place for text for any <emphasis>define</emphasis> options.
</para>
<para>
The search in <emphasis>DB.DBA.SYS_SPARQL_HOST</emphasis> stops at the first found row, other possible matches are silently ignored.
</para>
</sect2>
<sect2 id="rdfsqlfromsparql"><title>Calling SQL from SPARQL </title>
<para>A SPARQL expression can contain calls of Virtuoso/PL functions
and built-in SQL functions in both the WHERE clause and in
result set. Two namespace prefixes, <emphasis>bif</emphasis> and <emphasis>sql</emphasis> are reserved for
these purposes. When a function name starts with <emphasis>bif:</emphasis> namespace
prefix, the rest of name is treated as a name of SQL BIF (Built-In
Function). When a function name starts with <emphasis>sql:</emphasis> namespace prefix,
the rest of name is treated as a name of Virtuoso/PL function owned by
DBA with database qualifier DB, e.g. <emphasis>sql:example(...)</emphasis> is
converted into <emphasis>DB.DBA."example"(...)</emphasis>.</para>

<para>In both cases,
the function receives arguments in SQL format ('SQL valmode') and
returns the result also in SQL format.  The SPARQL compiler will
automatically add code for format conversion into the resulting SQL
code so SQL functions can be used even if <emphasis>define output:valmode
'LONG'</emphasis> forces the use of internal RDF representation in the
result set.</para>
<para><emphasis>Example:</emphasis></para>
<programlisting><![CDATA[
SQL>create procedure DB.DBA.ComposeInfo (
  in pname varchar,
  in pnick varchar := '',
  in pbox  varchar := '')
{
   declare ss varchar;
   ss := concat(pname, ' ', pnick, ' ', pbox);
   ss := rtrim (ss, ' ');
   return ss;

};
Done. -- 0 msec.

SQL>sparql
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
select (sql:ComposeInfo (?name, ?nick, ?box))
from <http://www.w3.org/People/Berners-Lee/card>
where
  {
    ?s rdf:type foaf:Person .
    optional{?s foaf:name ?name }.
    optional{?s foaf:nick ?nick }.
    optional{?s foaf:box ?box }.
    filter (?nick like '%TimBL%') .
  };
callret-0
VARCHAR
_______________________________________________________________________________

Timothy Berners-Lee TimBL

1 Rows. -- 30 msec.
]]></programlisting>
</sect2>
</sect1>
   <sect1 id="rdfsparul"><title>SPARUL -- an Update Language For RDF Graphs</title>
     <sect2 id="rdfsparulintro"><title>Introduction</title>
       <para>Starting from version 5.0, Virtuoso supports
<ulink url="http://jena.hpl.hp.com/~afs/SPARQL-Update.html">SPARQL/Update</ulink> extension of SPARQL.
This is sufficient for most of routine data manipulation operations. If <emphasis>SPARQL_UPDATE</emphasis>
role is granted to <emphasis>SPARQL</emphasis> user then data manipulation statements may be
executed via SPARQL web service endpoint as well as data querying.
       </para>
     </sect2>
     <sect2 id="rdfsparulfunc"><title>Manage RDF Storage</title>
       <para>Two functions allow the user to alter RDF storage by inserting or deleting all
triples listed in some vector. Both functions receive an IRI of a graph that should be altered and
a vector of triples that should be added or removed. The graph IRI can be either IRI ID or a string.
The third optional argument of these functions control transactional behavior: the value of parameter is
passed to <link linkend="fn_log_enable"><function>log_enable</function></link> function.
The return values of functions are not defined and should not be used by applications.
       </para>
       <programlisting>
create function DB.DBA.RDF_INSERT_TRIPLES (in graph_iri any, in triples any, in log_mode integer := null)
create function DB.DBA.RDF_DELETE_TRIPLES (in graph_iri any, in triples any, in log_mode integer := null)
       </programlisting>
       <para>Simple operations may be faster if written as low-level SQL code instead of using SPARUL.
E.g, the use of SPARQL DELETE is redundant when the application should delete from RDF_QUAD
by using simple filters like:
       </para>
       <programlisting>
delete from DB.DBA.RDF_QUAD
where G = DB.DBA.RDF_MAKE_IID_OF_QNAME (
    'http://local.virt/DAV/sparql_demo/data/data-xml/source-simple2/source-data-01.rdf' );
       </programlisting>
       <para>On the other hand, simple filters does not work when search criteria refer to triples
that are affected by the modification. Consider a function that deletes all triples whose subjects
are nodes of type 'http://xmlns.com/foaf/0.1/Person'. Type information is stored in triples that
will be deleted, so the simplest function is something like this:
       </para>
<programlisting><![CDATA[
create procedure DELETE_PERSONAL_DATA (in foaf_graph varchar)
{
  declare pdata_dict, pdata_array any;
-- Step 1: select everything that should be deleted
  pdata_dict := ((
      sparql construct { ?s ?p ?o }
      where { graph ?:foaf_graph {
              ?s ?p ?o . ?s rdf:type <http://xmlns.com/foaf/0.1/Person>
            } }
      ));
-- Step 2: delete all found triples
  pdata_array := dict_list_keys (pdata_dict, 1);
  RDF_DELETE_TRIPLES (foaf_graph, pdata_array);
};

DELETE_PERSONAL_DATA (
  'http://local.virt/DAV/sparql_demo/data/data-xml/source-simple2/source-data-01.rdf' );
]]></programlisting>
       <para>Starting from Virtuoso 5.0, application may use SPARUL to do the same in a convenient way:
       </para>
<programlisting><![CDATA[
create procedure DELETE_PERSONAL_DATA (in foaf_graph varchar)
{
  sparql delete { ?s ?p ?o }
      where { graph ?:foaf_graph {
              ?s ?p ?o . ?s rdf:type <http://xmlns.com/foaf/0.1/Person>
           } }
};
]]></programlisting>
     </sect2>
     <sect2 id="rdfsparulexamples"><title>Examples</title>
       <para>The graph where changes take place may be specified by an option in front of query, instead
of being specified in 'insert into graph' clause.
       </para>
<programlisting><![CDATA[
sparql define input:default-graph-uri <A-Named-Graph>
insert { <http://myopenlink.net/dataspace/Kingsley#this>
         <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
         <http://rdfs.org/sioc/ns#User> };
]]></programlisting>
       <para>The following two statements are equivalent but the latter may work faster, especially
if there are many RDF views in the system or if some RDF views may contain triples for the graph
in question. Note that neither of these two affect data that might come from RDF views.
       </para>
<programlisting><![CDATA[
sparql delete from graph <A-Named-Graph> { ?s ?p ?o } from <A-Named-Graph> where { ?s ?p ?o };
sparql clear graph <A-Named-Graph>;
]]></programlisting>
       <para>Keywords 'insert in' and 'insert into' are interchangeable in Virtuoso for backward
compatibility but SPARUL spec lists only 'insert into':
       </para>
<programlisting><![CDATA[
sparql insert into graph <A-Named-Graph> {  <http://myopenlink.net/dataspace/Kingsley#this>
                                            <http://rdfs.org/sioc/ns#id>
                                            <Kingsley> };
sparql insert into graph <A-Named-Graph> {  <http://myopenlink.net/dataspace/Caroline#this>
                                            <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
                                            <http://rdfs.org/sioc/ns#User> };
]]></programlisting>
       <para>It is possible to use various expressions to calculate fields of new triples. This is
 very convenient, even if not a part of the original spec.
       </para>
<programlisting><![CDATA[
sparql insert into graph <A-Named-Graph> { ?s <http://rdfs.org/sioc/ns#id> `iri (bif:concat (str (?o), "Idehen"))` }
where { ?s <http://rdfs.org/sioc/ns#id> ?o };
]]></programlisting>
       <para>'Modify graph' may be used as a sort of 'update' operation.
       </para>
<programlisting><![CDATA[
sparql modify graph <A-Named-Graph> delete { ?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o } insert { ?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type1> ?o } where { ?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> ?o };
sparql delete from graph <A-Named-Graph> { <http://myopenlink.net/dataspace/Caroline#this> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type1> <http://rdfs.org/sioc/ns#User> };
]]></programlisting>
       <para>The RDF information resource URI can be generated via a string expression.</para>
<programlisting><![CDATA[
sparql load bif:concat ("http://", bif:registry_get("URIQADefaultHost"), "/inputs/SparqlDawg/data-xml/Expr1/manifest.rdf")
   into graph <A-Named-Graph>;
]]></programlisting>
       <para>Number of operations can be sent to a web service endpoint as a single statement and
executed in sequence.
       </para>
<programlisting><![CDATA[
sparql
  insert in graph <A-Named-Graph> { <http://myopenlink.net/dataspace/Kingsley#this>
                                    <http://rdfs.org/sioc/ns#id>
                                    <Kingsley> }
  insert into graph <A-Named-Graph> { <http://myopenlink.net/dataspace/Caroline#this>
                                      <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
                                      <http://rdfs.org/sioc/ns#User> }
  insert into graph <A-Named-Graph> { ?s <http://rdfs.org/sioc/ns#id> `iri (bif:concat (str (?o), "Idehen"))` }
              where { ?s <http://rdfs.org/sioc/ns#id> ?o };
  modify graph <A-Named-Graph> delete { ?s <p2> ?o } insert { ?s <p2new> ?o } where { ?s <p2> ?o }
  delete from graph <A-Named-Graph> { <http://myopenlink.net/dataspace/Caroline#this> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type1> <http://rdfs.org/sioc/ns#User> };
  load bif:concat ("http://", bif:registry_get("URIQADefaultHost"), "/inputs/SparqlDawg/data-xml/Expr1/manifest.rdf") into graph <A-Named-Graph>
;
]]></programlisting>

<para>In case of huge RDF data ( for ex. 600 million triples ) loaded in the Virtuoso sever in a single graph,
the fastest operation to drop the graph is:</para>
<programlisting><![CDATA[
sparql clear graph <graph-iri>;
]]></programlisting>
<para>The operation may be accelerated using log_enable (0) or even log_enable (2),
with log_enable(1) after the operation.
</para>
      </sect2>
   <sect2 id="rdfsparulexamples"><title>More Detailed Examples</title>
<para>Adding triples to graph</para>
<programlisting><![CDATA[
sparql
INSERT INTO GRAPH <http://BookStore.com>
{ <http://www.dajobe.org/foaf.rdf#i> <http://purl.org/dc/elements/1.1/title>  "SPARQL and RDF" .
  <http://www.dajobe.org/foaf.rdf#i> <http://purl.org/dc/elements/1.1/date> <1999-01-01T00:00:00>.
  <http://www.w3.org/People/Berners-Lee/card#i> <http://purl.org/dc/elements/1.1/title> "Design notes" .
  <http://www.w3.org/People/Berners-Lee/card#i> <http://purl.org/dc/elements/1.1/date> <2001-01-01T00:00:00>.
  <http://www.w3.org/People/Connolly/#me> <http://purl.org/dc/elements/1.1/title> "Fundamentals of Compiler Design" .
  <http://www.w3.org/People/Connolly/#me> <http://purl.org/dc/elements/1.1/date> <2002-01-01T00:00:00>. };
]]></programlisting>
<para>Adding triples to graph</para>
<programlisting><![CDATA[
]]></programlisting>
<para>A SPARQL/Update request that contains a triple to be deleted and a triple to be added (used here to correct a book title).
</para>
<programlisting><![CDATA[
SPARQL
MODIFY GRAPH <http://BookStore.com>
DELETE
 { <http://www.w3.org/People/Connolly/#me>  <http://purl.org/dc/elements/1.1/title>  "Fundamentals of Compiler Design" }
INSERT
 { <http://www.w3.org/People/Connolly/#me>  <http://purl.org/dc/elements/1.1/title>  "Fundamentals" };
callret-0
VARCHAR
_______________________________________________________________________________

Modify <http://BookStore.com>, delete 1 and insert 1 triples -- done

1 Rows. -- 20 msec.
]]></programlisting>
<para>The example below has a request to delete all records of old books (with date before year 2000)
</para>
<programlisting><![CDATA[
sparql
PREFIX dc:  <http://purl.org/dc/elements/1.1/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
DELETE FROM GRAPH <http://BookStore.com> { ?book ?p ?v }
WHERE
  { GRAPH  <http://BookStore.com>
   { ?book dc:date ?date
     FILTER ( xsd:dateTime(?date) < xsd:dateTime("2000-01-01T00:00:00")).
    ?book ?p ?v.
   }
  };
_______________________________________________________________________________

Delete from <http://BookStore.com>, 6 triples -- done

1 Rows. -- 10 msec.
]]></programlisting>
<para>This snippet copies records from one named graph to another named graph based on a pattern:
</para>
<programlisting><![CDATA[
sparql
PREFIX dc:  <http://purl.org/dc/elements/1.1/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
INSERT INTO GRAPH <http://NewBookStore.com> { ?book ?p ?v }
WHERE
  { GRAPH  <http://BookStore.com>
   { ?book dc:date ?date
     FILTER ( xsd:dateTime(?date) > xsd:dateTime("2000-01-01T00:00:00")).
     ?book ?p ?v.
   }
  };
callret-0
VARCHAR
_______________________________________________________________________________

Insert into <http://NewBookStore.com>, 6 triples -- done

1 Rows. -- 30 msec.
]]></programlisting>
<para>An example to move records from one named graph to another named graph based on a pattern:
</para>
<programlisting><![CDATA[
sparql
PREFIX dc:  <http://purl.org/dc/elements/1.1/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>

INSERT INTO GRAPH <http://NewBookStore.com>
 { ?book ?p ?v }
WHERE
  { GRAPH  <http://BookStore.com>
     { ?book dc:date ?date .
       FILTER ( xsd:dateTime(?date) > xsd:dateTime("2000-01-01T00:00:00")).
       ?book ?p ?v.
     }
  };
_______________________________________________________________________________

Insert into <http://NewBookStore.com>, 6 triples -- done

1 Rows. -- 10 msec.

sparql
PREFIX dc:  <http://purl.org/dc/elements/1.1/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
DELETE FROM GRAPH <http://BookStore.com>
 { ?book ?p ?v }
WHERE
  { GRAPH  <http://BookStore.com>
      { ?book dc:date ?date .
        FILTER ( xsd:dateTime(?date) > xsd:dateTime("2000-01-01T00:00:00")).
        ?book ?p ?v.
      }
  };
_______________________________________________________________________________

Delete from <http://BookStore.com>, 3 triples -- done

1 Rows. -- 10 msec.
]]></programlisting>
   </sect2>
   </sect1>
<sect1 id="rdfinsertmethods"><title>RDF Insert Methods in Virtuoso</title>
    <sect2 id="rdfinsertmethodshttppost"><title>HTTP Post using Content-Type: application/sparql-query</title>
	<para>With POST can be accomplished SPARQL Insert/Update etc.</para>
	<para>The result is in the rdf_quad.</para>
	<para>With GET Methods you can get the triples which are saved.</para>
        <para><emphasis>Examples:</emphasis></para>
        <para><emphasis>Example 1:</emphasis></para>
        <para>Create a DAV collection xx for user demo with password demo.</para>
        <para>Execute the following command:</para>
<programlisting><![CDATA[
curl -i -d "INSERT {<http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this>
<http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://rdfs.org/sioc/ns#User>}" -u "demo:demo"
-H "Content-Type: application/sparql-query" http://localhost:8890/DAV/xx/yy
]]></programlisting>
        <para>The response should be:</para>
<programlisting><![CDATA[
HTTP/1.1 201 Created
Server: Virtuoso/05.00.3023 (Win32) i686-generic-win-32  VDB
Connection: Keep-Alive
Content-Type: text/html; charset=ISO-8859-1
Date: Fri, 28 Dec 2007 12:50:12 GMT
Accept-Ranges: bytes
MS-Author-Via: SPARQL
Content-Length: 0
]]></programlisting>
        <para>The result in the DAV/xx location will be a new WebDAV resource with name "yy" containing the following:</para>
<itemizedlist>
<listitem>if opened with Conductor:
<programlisting><![CDATA[
CONSTRUCT { ?s ?p ?o } FROM <http://localhost:8890/DAV/xx/yy> WHERE { ?s ?p ?o }
]]></programlisting>
</listitem>
<listitem>if opened with GET, then the content will be RDF representation of what was inserted into the graph, i.e.
<programlisting><![CDATA[
<?xml version="1.0" encoding="utf-8" ?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#">
<rdf:Description
rdf:about="http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this">
<ns0pred:type xmlns:ns0pred="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
rdf:resource="http://rdfs.org/sioc/ns#User"/>
</rdf:Description>
</rdf:RDF>
]]></programlisting>
</listitem>
</itemizedlist>
        <para><emphasis>Example 2:</emphasis></para>
        <para>Create a DAV collection, for ex. with name "test" for user ( for ex. demo).</para>
        <para>Execute the following command:</para>
<programlisting><![CDATA[
curl -i -d "INSERT IN GRAPH <http://mygraph.com>
{ <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this>
  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
  <http://rdfs.org/sioc/ns#User> .
  <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this>
  <http://www.w3.org/2000/01/rdf-schema#label>
  <Kingsley Uyi Idehen> .
  <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this>
  <http://rdfs.org/sioc/ns#creator_of>

<http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1300>
  } " -u "demo:demo" -H "Content-Type: application/sparql-query" http://localhost:8890/DAV/home/demo/test/myrq
]]></programlisting>
        <para>As result the response will be:</para>
<programlisting><![CDATA[
HTTP/1.1 201 Created
Server: Virtuoso/05.00.3023 (Win32) i686-generic-win-32  VDB
Connection: Keep-Alive
Content-Type: text/html; charset=ISO-8859-1
Date: Thu, 20 Dec 2007 16:25:25 GMT
Accept-Ranges: bytes
MS-Author-Via: SPARQL
Content-Length: 0
]]></programlisting>
        <para>Now let's check the inserted triples. Go to the sparql endpoint, i.e. http://localhost:8890/sparql and:</para>
<itemizedlist>
<listitem>Enter for Default Graph URI:
<programlisting><![CDATA[
http://mygraph.com
]]></programlisting>
</listitem>
<listitem>Enter in the Query area:
<programlisting><![CDATA[
SELECT * WHERE {?s ?p ?o}
]]></programlisting>
</listitem>
<listitem>Click the button "Run Query"</listitem>
<listitem>As result will be shown the inserted triples:
<programlisting><![CDATA[
s  	                                                          p                                                 o
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this   http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://rdfs.org/sioc/ns#User
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this   http://www.w3.org/2000/01/rdf-schema#label 	    Kingsley
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this   http://rdfs.org/sioc/ns#creator_of                http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1300
]]></programlisting>
</listitem>
</itemizedlist>
</sect2>
     <sect2 id="rdfinsertmethodshttpput"><title>HTTP PUT using Content-Type: application/rdf+xml</title>
	<para>The URI in a PUT request identifies the entity enclosed with the request. Therefore using HTTP PUT is a more useful and meaningful command than using POST (which is more about submitting data to a script).</para>
        <para><emphasis>Example:</emphasis></para>
        <para>Suppose there is myfoaf.rdf file with the following content:</para>
<programlisting><![CDATA[
<rdf:RDF xmlns="http://www.example/jose/foaf.rdf#"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:log="http://www.w3.org/2000/10/swap/log#"
    xmlns:myfoaf="http://www.example/jose/foaf.rdf#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

    <foaf:Person rdf:about="http://www.example/jose/foaf.rdf#jose">
        <foaf:homepage rdf:resource="http://www.example/jose/"/>
        <foaf:knows rdf:resource="http://www.example/jose/foaf.rdf#juan"/>
        <foaf:name>Jose Jimen~ez</foaf:name>
        <foaf:nick>Jo</foaf:nick>
        <foaf:workplaceHomepage rdf:resource="http://www.corp.example/"/>
    </foaf:Person>

    <foaf:Person rdf:about="http://www.example/jose/foaf.rdf#juan">
        <foaf:mbox rdf:resource="mailto:juan@mail.example"/>
    </foaf:Person>

    <foaf:Person rdf:about="http://www.example/jose/foaf.rdf#julia">
        <foaf:mbox rdf:resource="mailto:julia@mail.example"/>
    </foaf:Person>

    <rdf:Description rdf:about="http://www.example/jose/foaf.rdf#kendall">
        <foaf:knows rdf:resource="http://www.example/jose/foaf.rdf#edd"/>
    </rdf:Description>
</rdf:RDF>
]]></programlisting>
        <para>Now let's upload the myfoaf.rdf file to destination server demo.openlinksw.com for user demo:</para>
<programlisting><![CDATA[
curl -T myfoaf.rdf http://demo.openlinksw.com/DAV/home/demo/rdf_sink/myfoaf.rdf -u demo:demo
]]></programlisting>
        <para>As result the response should be:</para>
<programlisting><![CDATA[
<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<HTML>
<HEAD>
  <TITLE>201 Created</TITLE>
</HEAD>
<BODY>
  <H1>Created</H1>
  Resource /DAV/home/demo/rdf_sink/ myfoaf.rdf has been created.
</BODY>
</HTML>
]]></programlisting>
    </sect2>
     <sect2 id="rdfinsertmethodsload"><title>SPARQL Insert using LOAD</title>
        <para>SPARQL INSERT operation can be done using the LOAD feature.</para>
        <para><emphasis>Example:</emphasis></para>
        <para>Execute from ISQL:</para>
<programlisting><![CDATA[
sparql insert in graph <http://mygraph.com>
{
  <http://myopenlink.net/dataspace/Kingsley#this>
  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
  <http://rdfs.org/sioc/ns#User> .

  <http://myopenlink.net/dataspace/Kingsley#this>
  <http://rdfs.org/sioc/ns#id>
  <Kingsley> .

  <http://myopenlink.net/dataspace/Caroline#this>
  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
  <http://rdfs.org/sioc/ns#User> .

 <http://myopenlink.net/dataspace/Caroline#this>
  <http://rdfs.org/sioc/ns#id>
  <Caroline> .

   <http://myopenlink.net/dataspace/Matt#this>
  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
  <http://rdfs.org/sioc/ns#User> .

  <http://myopenlink.net/dataspace/Matt#this>
  <http://rdfs.org/sioc/ns#id>
  <Matt> .

   <http://myopenlink.net/dataspace/demo#this>
  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>
  <http://rdfs.org/sioc/ns#User> .

  <http://myopenlink.net/dataspace/demo#this>
  <http://rdfs.org/sioc/ns#id>
  <demo> .};
]]></programlisting>
        <para>Create DAV collection which is visible to public, for ex: http://localhost:8890/DAV/tmp</para>
        <para>Upload to the DAV collection the following file for ex. with name listall.rq and with the following content:</para>
<programlisting><![CDATA[
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
PREFIX sioc: <http://rdfs.org/sioc/ns#>
SELECT ?x ?p ?o
FROM <http://mygraph.com>
WHERE
  {
    ?x rdf:type sioc:User .
    ?x ?p ?o.
    ?x sioc:id ?id .
    FILTER REGEX(str(?id), "^King")
  }
ORDER BY ?x
]]></programlisting>
        <para>Now from ISQL execute the following command:</para>
<programlisting><![CDATA[
sparql
load bif:concat ("http://", bif:registry_get("URIQADefaultHost"), "/DAV/tmp/listall.rq") into graph <http://myNewGraph.com>;
]]></programlisting>
        <para>As result should be shown:</para>
<programlisting><![CDATA[
callret-0
VARCHAR
_______________________________________________________________________________

Load <http://localhost:8890/DAV/tmp/listall.rq> into graph <http://myNewGraph.com> -- done

1 Rows. -- 321 msec.
]]></programlisting>
    </sect2>
     <sect2 id="rdfindertmethodsparqlendpoint"><title>SPARQL Insert via /sparql endpoint</title>
        <para>SPARQL INSERT operation can be sent to a web service endpoint as a single statement and executed in sequence.</para>
        <para><emphasis>Example:</emphasis></para>
        <para>Using the Virtuoso ISQL tool or using the /sparql UI at http://host:port/sparql, execute the following:</para>
<itemizedlist>
<listitem>Insert into graph http://example/bookStore 3 triples:
<programlisting><![CDATA[
sparql insert in graph <http://BookStore.com>
{ <http://www.dajobe.org/foaf.rdf#i> <http://purl.org/dc/elements/1.1/date> <1999-04-01T00:00:00> .
  <http://www.w3.org/People/Berners-Lee/card#i> <http://purl.org/dc/elements/1.1/date> <1998-05-03T00:00:00> .
  <http://www.w3.org/People/Connolly/#me> <http://purl.org/dc/elements/1.1/date> <2001-02-08T00:00:00> };
]]></programlisting>
</listitem>
<listitem>As result will be shown the message:
<programlisting><![CDATA[
Insert into <http://BookStore.com>, 3 triples -- done
]]></programlisting>
</listitem>
<listitem>Next we will select all triples from the graph http://example/bookStore:
<programlisting><![CDATA[
sparql select * from <http://BookStore.com> where {?s ?p ?o};
]]></programlisting>
</listitem>
<listitem>As result will be shown:
<programlisting><![CDATA[
s                                              p                                       o
VARCHAR                                        VARCHAR                                 VARCHAR
_______________________________________________________________________________

http://www.w3.org/People/Berners-Lee/card#i    http://purl.org/dc/elements/1.1/date    1998-05-03T00:00:00
http://www.w3.org/People/Connolly/#me          http://purl.org/dc/elements/1.1/date    2001-02-08T00:00:00
http://www.dajobe.org/foaf.rdf#i               http://purl.org/dc/elements/1.1/date    1999-04-01T00:00:00

3 Rows. -- 0 msec.

]]></programlisting>
</listitem>
<listitem>Now let's insert into graph another http://NewBookStore.com graph's values:
<programlisting><![CDATA[
sparql
PREFIX dc:  <http://purl.org/dc/elements/1.1/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
INSERT INTO GRAPH <http://NewBookStore.com> { ?book ?p ?v }
WHERE
  { GRAPH  <http://BookStore.com>
   { ?book dc:date ?date
     FILTER ( xsd:dateTime(?date) < xsd:dateTime("2000-01-01T00:00:00")).
     ?book ?p ?v.
   }
  };
]]></programlisting>
</listitem>
<listitem>As result will be shown:
<programlisting><![CDATA[
callret-0
VARCHAR
_______________________________________________________________________________

Insert into <http://NewBookStore.com>, 2 triples -- done
]]></programlisting>
</listitem>
<listitem>Finally we will check the triples from the graph NewBookStore.com:
<programlisting><![CDATA[
SQL> sparql select * from <http://NewBookStore.com> where {?s ?p ?o};
]]></programlisting>
</listitem>
<listitem>As result will be shown:
<programlisting><![CDATA[
s                                             p                                      o
VARCHAR                                                 VARCHAR    VARCHAR
_______________________________________________________________________________

http://www.w3.org/People/Berners-Lee/card#i   http://purl.org/dc/elements/1.1/date   1998-05-03T00:00:00
http://www.dajobe.org/foaf.rdf#i              http://purl.org/dc/elements/1.1/date   1999-04-01T00:00:00

2 Rows. -- 10 msec.
]]></programlisting>
</listitem>
</itemizedlist>
</sect2>
     <sect2 id="rdfinsertmethodsparqlqueryandodswiki"><title>SPARQL Insert via HTTP Post using Content-Type: application/sparql-query and ODS wiki</title>
        <para>With HTTP Post and ODS wiki can be written an rdf document and respectively to be performed over it INSERT/UPDATE action.</para>
        <para>You can write to a file using SIOC terms for ODS-Wiki</para>
        <para>You can check with sparql the inserted / updated triples in the Quad Store.</para>
        <para><emphasis>Example:</emphasis></para>
        <para>Suppose there is ODS user test3 with ODS password 1, which has testWiki wiki instance.</para>
        <para>Execute the following:</para>
<programlisting><![CDATA[
curl -i -d "INSERT {<http://localhost:8890/dataspace/test3/wiki/testWiki> <http://atomowl.org/ontologies/atomrdf#contains> <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> . <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://rdfs.org/sioc/ns#has_container> <http://localhost:8890/dataspace/test3/wiki/testWiki> . <http://localhost:8890/dataspace/test3/wiki/testWiki> <http://atomowl.org/ontologies/atomrdf#entry> <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> . <http://localhost:8890/dataspace/test3/wiki/testWiki> <http://rdfs.org/sioc/ns#container_of> <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> . <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://rdfs.org/sioc/ns#topic>  <http://localhost:8890/dataspace/test3/wiki/testWiki> . <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://atomowl.org/ontologies/atomrdf#source> <http://localhost:8890/dataspace/test3/wiki/testWiki> . <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://rdfs.org/sioc/types#Comment> . <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://atomowl.org/ontologies/atomrdf#Entry> . <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://www.w3.org/2000/01/rdf-schema#label> 'MyTest' . <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://atomowl.org/ontologies/atomrdf#Link> . <http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://rdfs.org/sioc/ns#content> <test>}" -u "test3:1" -H "Content-Type: application/sparql-query" http://localhost:8890/DAV/home/test3/wiki/testWiki/MyTest
]]></programlisting>
        <para>As result we should have 2 files created:</para>
<itemizedlist>
<listitem>In the user DAV folder "DAV/home/test3/wiki/testWiki/" will be created a file "MyTest" with type "application/sparql-query". You can view the content of this file from from the Conductor UI or from the user's Briefcase UI, path "DAV/home/test3/wiki/testWiki". Its content will be:
<programlisting><![CDATA[
<?xml version="1.0" encoding="utf-8" ?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#">
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki"><ns0pred:entry xmlns:ns0pred="http://atomowl.org/ontologies/atomrdf#" rdf:resource="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"/></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"><ns0pred:label xmlns:ns0pred="http://www.w3.org/2000/01/rdf-schema#">MyTest</ns0pred:label></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"><ns0pred:type xmlns:ns0pred="http://www.w3.org/1999/02/22-rdf-syntax-ns#" rdf:resource="http://atomowl.org/ontologies/atomrdf#Link"/></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"><ns0pred:type xmlns:ns0pred="http://www.w3.org/1999/02/22-rdf-syntax-ns#" rdf:resource="http://rdfs.org/sioc/types#Comment"/></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"><ns0pred:type xmlns:ns0pred="http://www.w3.org/1999/02/22-rdf-syntax-ns#" rdf:resource="http://atomowl.org/ontologies/atomrdf#Entry"/></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"><ns0pred:has_container xmlns:ns0pred="http://rdfs.org/sioc/ns#" rdf:resource="http://localhost:8890/dataspace/test3/wiki/testWiki"/></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki"><ns0pred:container_of xmlns:ns0pred="http://rdfs.org/sioc/ns#" rdf:resource="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"/></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki"><ns0pred:contains xmlns:ns0pred="http://atomowl.org/ontologies/atomrdf#" rdf:resource="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"/></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"><ns0pred:content xmlns:ns0pred="http://rdfs.org/sioc/ns#">test</ns0pred:content></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"><ns0pred:topic xmlns:ns0pred="http://rdfs.org/sioc/ns#" rdf:resource="http://localhost:8890/dataspace/test3/wiki/testWiki"/></rdf:Description>
<rdf:Description rdf:about="http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest"><ns0pred:source xmlns:ns0pred="http://atomowl.org/ontologies/atomrdf#" rdf:resource="http://localhost:8890/dataspace/test3/wiki/testWiki"/></rdf:Description>
</rdf:RDF>
]]></programlisting>
</listitem>
<listitem>To the user's wiki instance will be added a new WikiWord "MyTest" with content the value of the SIOC term attribute "content":
<programlisting><![CDATA[
<http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest> <http://rdfs.org/sioc/ns#content> <test>
i.e. the content will be "test".
]]></programlisting>
</listitem>
</itemizedlist>
        <para>Now let's check what data was inserted in the Quad Store:</para>
<itemizedlist>
<listitem>Go to the sparql endpoint, i.e. for ex. to http://localhost:8890/sparql</listitem>
<listitem>Enter for Default Graph URI:
<programlisting><![CDATA[
http://localhost:8890/DAV/home/test3/wiki/testWiki/MyTest
]]></programlisting>
</listitem>
<listitem>Enter for Query text:
<programlisting><![CDATA[
SELECT * WHERE {?s ?p ?o}
]]></programlisting>
</listitem>
<listitem>Click the "Run Query" button.</listitem>
<listitem>As result will be shown the inserted triples:
<programlisting><![CDATA[
s                                 p                                           o
http://localhost:8890/dataspace/test3/wiki/testWiki 	    http://rdfs.org/sioc/ns#container_of 	     http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest
http://localhost:8890/dataspace/test3/wiki/testWiki 	    http://atomowl.org/ontologies/atomrdf#entry      http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest
http://localhost:8890/dataspace/test3/wiki/testWiki 	    http://atomowl.org/ontologies/atomrdf#contains   http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest
http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest  http://www.w3.org/1999/02/22-rdf-syntax-ns#type  http://rdfs.org/sioc/types#Comment
http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest  http://www.w3.org/1999/02/22-rdf-syntax-ns#type  http://atomowl.org/ontologies/atomrdf#Entry
http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest  http://www.w3.org/1999/02/22-rdf-syntax-ns#type  http://atomowl.org/ontologies/atomrdf#Link
http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest  http://www.w3.org/2000/01/rdf-schema#label 	     MyTest
http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest  http://rdfs.org/sioc/ns#has_container 	     http://localhost:8890/dataspace/test3/wiki/testWiki
http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest  http://rdfs.org/sioc/ns#content 	             test
http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest  http://rdfs.org/sioc/ns#topic 	             http://localhost:8890/dataspace/test3/wiki/testWiki
http://localhost:8890/dataspace/test3/wiki/testWiki/MyTest  http://atomowl.org/ontologies/atomrdf#source     http://localhost:8890/dataspace/test3/wiki/testWiki
]]></programlisting>
</listitem>
</itemizedlist>
    </sect2>
    <sect2 id="rdfinsertmethodwebdav">
      <title>Using WebDAV</title>
      <para> Example using WebDAV (mount folder to DAV and dump; if this is the rdf_sink
the Quad Store is updated automatically, or you can load from DAV manually to quad store)</para>
      <para><emphasis>Example:</emphasis></para>
      <para><emphasis>Example 1: Using ODS Briefcase</emphasis></para>
      <para>Go to your ods location, for ex. http://localhost:8890/ods</para>
      <para>Register user, for ex. user test1</para>
      <para>Login if not already in ods</para>
      <para>Go to ODS ->Briefcase</para>
      <para>Create new instance</para>
      <para>Go to the briefcase instance by clicking its name link</para>
      <para>Upload file in a new created folder mytest or in the rdf_sink folder with:</para>
      <itemizedlist>
        <listitem>checked option "RDF Store"</listitem>
        <listitem>set RDF Graph Name, for ex. http://localhost:8890/DAV/home/test1/ of
the current folder>/ for ex. http://localhost:8890/DAV/home/test1/mytest/ or
http://localhost:8890/DAV/home/test1/rdf_sink/</listitem>
        <listitem>For ex. upload the following file with name jose.rdf.</listitem>
<programlisting><![CDATA[
<rdf:RDF xmlns="http://www.example/jose/foaf.rdf#"
    xmlns:foaf="http://xmlns.com/foaf/0.1/"
    xmlns:log="http://www.w3.org/2000/10/swap/log#"
    xmlns:myfoaf="http://www.example/jose/foaf.rdf#"
    xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">

    <foaf:Person rdf:about="http://www.example/jose/foaf.rdf#jose">
        <foaf:homepage rdf:resource="http://www.example/jose/"/>
        <foaf:knows rdf:resource="http://www.example/jose/foaf.rdf#juan"/>
        <foaf:name>Jose Jimen~ez</foaf:name>
        <foaf:nick>Jo</foaf:nick>
        <foaf:workplaceHomepage rdf:resource="http://www.corp.example/"/>
    </foaf:Person>

    <foaf:Person rdf:about="http://www.example/jose/foaf.rdf#juan">
        <foaf:mbox rdf:resource="mailto:juan@mail.example"/>
    </foaf:Person>

    <foaf:Person rdf:about="http://www.example/jose/foaf.rdf#julia">
        <foaf:mbox rdf:resource="mailto:julia@mail.example"/>
    </foaf:Person>

    <rdf:Description rdf:about="http://www.example/jose/foaf.rdf#kendall">

        <foaf:knows rdf:resource="http://www.example/jose/foaf.rdf#edd"/>
    </rdf:Description>
</rdf:RDF>
]]></programlisting>
      </itemizedlist>
      <para>Execute the following query:</para>
<programlisting><![CDATA[
select * from <>
where {?s ?p ?o}
]]></programlisting>
      <para>As result should be shown:</para>
<programlisting><![CDATA[
s  	                                  p  	                                            o
http://www.example/jose/foaf.rdf#jose 	  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://xmlns.com/foaf/0.1/Person
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/nick 	            Jo
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/name 	            Jose Jimen~ez
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/knows 	            http://www.example/jose/foaf.rdf#juan
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/homepage 	            http://www.example/jose/
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/workplaceHomepage 	    http://www.corp.example/
http://www.example/jose/foaf.rdf#kendall  http://xmlns.com/foaf/0.1/knows 	            http://www.example/jose/foaf.rdf#edd
http://www.example/jose/foaf.rdf#julia 	  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://xmlns.com/foaf/0.1/Person
http://www.example/jose/foaf.rdf#julia 	  http://xmlns.com/foaf/0.1/mbox 	            mailto:julia@mail.example
http://www.example/jose/foaf.rdf#juan 	  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://xmlns.com/foaf/0.1/Person
http://www.example/jose/foaf.rdf#juan 	  http://xmlns.com/foaf/0.1/mbox 	            mailto:juan@mail.example
]]></programlisting>
      <para><emphasis>Example 2: Using Conductor UI</emphasis></para>
      <para>Go to Conductor UI, for ex. at http://localhost:8890/conductor</para>
      <para>Login as dba user</para>
      <para>Go to Web Application Server and create new folder, for ex. test</para>
      <para>Upload in the folder test a file, for ex. the file jose.rdf from above with options:</para>
      <itemizedlist>
        <listitem>Destination: RDF Store</listitem>
        <listitem>Set the RDF IRI</listitem>
      </itemizedlist>
    </sect2>
    <sect2 id="rdfinsertmethodvirtuosocrawler">
      <title>Using Virtuoso Crawler</title>
      <para>Using Virtuoso Crawler (which includes the Sponger options so you crawl
non-RDF but get RDF and this can go to the Quad Store)</para>
      <para><emphasis>Example:</emphasis></para>
      <para>Go to Conductor UI. For ex. at http://localhost:8890/conductor</para>
      <para>Login as dba user</para>
      <para>Go to tab Web Application Server</para>
      <para>Go to tab Content Imports</para>
      <para>Click the "New Target" button</para>
      <para>In the shown form:</para>
      <itemizedlist>
        <listitem>Enter for "Target description": Tim Berners-Lee's electronic Business Card</listitem>
        <listitem>Enter for "Target URL": http://www.w3.org/People/Berners-Lee</listitem>
        <listitem>Enter for "Copy to local DAV collection" for ex.: /DAV/home/demo/rdf_sink/</listitem>
        <listitem>Choose from the list "Local resources owner": demo</listitem>
        <listitem>Check the check.box with label "Store metadata".</listitem>
        <listitem>Check all the check-boxes shown below the check-box "Store metadata".</listitem>
        <listitem>Click the button "Create".
          <figure id="rdfinsertwebdav1" float="1">
	    <title>Using Virtuoso Crawler</title>
	    <graphic fileref="ui/rdfinsert1.png"/>
          </figure>
        </listitem>
      </itemizedlist>
      <para>Click the button "Import Queues".</para>
      <para>For "Robot target" with label "Tim Berners-Lee's electronic Business Card"
click the start link.</para>
      <para>As result should be shown t he number of the pages retrieved.</para>
      <figure id="rdfinsertwebdav2" float="1">
        <title>Using Virtuoso Crawler</title>
	<graphic fileref="ui/rdfinsert2.png"/>
      </figure>
      <para>Now using the sparql endpoint with sponger option "Use only local data"
enter for Default Graph URI: http://www.w3.org/People/Berners-Lee and execute the following query: </para>
<programlisting><![CDATA[
select *
where {?s ?p ?o}
]]></programlisting>
      <para>As result should be shown the following triples:</para>
<programlisting><![CDATA[
s                                                      p                                                 o
http://www.w3.org/People/Berners-Lee 	http://www.w3.org/1999/02/22-rdf-syntax-ns#type     http://xmlns.com/foaf/0.1/Document
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Answers for young people - Tim Berners-Lee
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Berners-Lee: Weaving the Web
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Declaration by Tim BL 28 Feb 1996 w.r.t. CDA challenge
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Errata - Berners-Lee: Weaving the Web
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Frequently asked questions by the Press - Tim BL
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Glossary - Weaving the Web - Berners-Lee
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Longer Bio for Tim Berners-Lee
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Michael Dertouzos has left us
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            The Future of the Web and Europe
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            The World Wide Web: Past, Present and Future
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            The World Wide Web: A very short personal history
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Tim Berners-Lee
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Tim Berners-Lee - 3Com Founders chair
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Tim Berners-Lee: Disclosures
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Tim Berners-Lee: WWW and UU and I
http://www.w3.org/People/Berners-Lee 	http://purl.org/dc/elements/1.1/title 	            Tim Berners-Lee: WorldWideWeb, the first Web client
]]></programlisting>
    </sect2>
    <sect2 id="rdfinsertmethodsparqlqueryandsponger">
      <title>Using SPARQL Query and Sponger (i.e. we Sponge the Resources in the FROM Clause or values for the graph-uri parameter in SPARQL protocol URLs)</title>
      <para><emphasis>Example:</emphasis></para>
      <para>Execute the following query: </para>
<programlisting><![CDATA[
sparql
SELECT ?id
FROM NAMED <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/sioc.ttl>
 OPTION (get:soft "soft", get:method "GET")
WHERE { GRAPH ?g { ?id a ?o } }
limit 10;
]]></programlisting>
      <para>As result will be shown the retrieved triples:</para>
<programlisting><![CDATA[
id
VARCHAR
_______________________________________________________________________________

http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D
http://www.openlinksw.com/dataspace/person/kidehen@openlinksw.com#this
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/612
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/612
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/610
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/610
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/856
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/856

10 Rows. -- 20 msec.
]]></programlisting>
    </sect2>
    <sect2 id="rdfinsertmethodplapis">
      <title>Using Virtuoso PL APIs</title>
      <para><emphasis>Example:</emphasis></para>
      <para>In the example script we implement a basic mapper which maps a text/plain mime type
to an imaginary ontology, which extends the class Document from FOAF with properties 'txt:UniqueWords'
and 'txt:Chars', where the prefix 'txt:' we specify as 'urn:txt:v0.0:'.</para>
<programlisting><![CDATA[
use DB;

create procedure DB.DBA.RDF_LOAD_TXT_META
	(
	 in graph_iri varchar,
	 in new_origin_uri varchar,
	 in dest varchar,
         inout ret_body any,
	 inout aq any,
	 inout ps any,
	 inout ser_key any
	 )
{
  declare words, chars int;
  declare vtb, arr, subj, ses, str any;
  declare ses any;
  -- if any error we just say nothing can be done
  declare exit handler for sqlstate '*'
 {
      return 0;
 };
  subj := coalesce (dest, new_origin_uri);
  vtb := vt_batch ();
  chars := length (ret_body);
  -- using the text index procedures we get a list of words
  vt_batch_feed (vtb, ret_body, 1);
  arr := vt_batch_strings_array (vtb);
  -- the list has 'word' and positions array , so we must divide by 2
  words := length (arr) / 2;
  ses := string_output ();
  -- we compose a N3 literal
  http (sprintf ('<%s> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Document> .\n', subj), ses);
  http (sprintf ('<%s> <urn:txt:v0.0:UniqueWords> "%d" .\n', subj, words), ses);
  http (sprintf ('<%s> <urn:txt:v0.0:Chars> "%d" .\n', subj, chars), ses);
  str := string_output_string (ses);
  -- we push the N3 text into the local store
  DB.DBA.TTLP (str, new_origin_uri, subj);
  return 1;
}
;

--
delete from DB.DBA.SYS_RDF_MAPPERS where RM_HOOK = 'DB.DBA.RDF_LOAD_TXT_META';

insert soft DB.DBA.SYS_RDF_MAPPERS (RM_PATTERN, RM_TYPE, RM_HOOK, RM_KEY, RM_DESCRIPTION)
    values ('(text/plain)', 'MIME', 'DB.DBA.RDF_LOAD_TXT_META', null, 'Text Files (demo)');

-- here we set order to some large number so don't break existing mappers
update DB.DBA.SYS_RDF_MAPPERS set RM_ID = 2000 where RM_HOOK = 'DB.DBA.RDF_LOAD_TXT_META';
]]></programlisting>
      <para>To test the mapper we just use /sparql endpoint with option 'Retrieve remote
RDF data for all missing source graphs' to execute:</para>
<programlisting><![CDATA[
select * from <http://demo.openlinksw.com:8890/tutorial/hosting/ho_s_30/WebCalendar/tools/summary.txt>
where { ?s ?p ?o }
]]></programlisting>
      <para>To check the results:</para>
      <itemizedlist>
        <listitem>Make sure the initial state of tutorial <ulink url="http://demo.openlinksw.com/tutorial/rdf/rd_s_1/rd_s_1.vsp">RD_S_1</ulink> is set.</listitem>
        <listitem>Go to http://demo.openlinksw.com/sparql</listitem>
        <listitem>Enter for Default Graph URI this value:</listitem>
<programlisting><![CDATA[
http://localhost:80/tutorial/hosting/ho_s_30/WebCalendar/tools/summary.txt
]]></programlisting>
        <listitem>Enter for Query text:</listitem>
<programlisting><![CDATA[
select *
where {?s ?p ?o}
]]></programlisting>
        <listitem>Click the "Run Query" button.</listitem>
        <listitem>As result should be shown the following triples:</listitem>
<programlisting><![CDATA[
s  	                                                                    p  	                                            o
http://localhost:80/tutorial/hosting/ho_s_30/WebCalendar/tools/summary.txt  http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://xmlns.com/foaf/0.1/Document
http://localhost:80/tutorial/hosting/ho_s_30/WebCalendar/tools/summary.txt  urn:txt:v0.0:UniqueWords 	                    47
http://localhost:80/tutorial/hosting/ho_s_30/WebCalendar/tools/summary.txt  urn:txt:v0.0:Chars 	                            625
]]></programlisting>
      </itemizedlist>
      <para><emphasis>Important: Setting Sponger Permissions</emphasis></para>
      <para>In order to allow the Sponger to update the local RDF quad store with triples
constituting the sponged structured data, the role "SPARQL_SPONGE" must be granted to the
account "SPARQL", i.e., to the owner account of /sparql web service endpoint.
This should normally be the case. If not, you must manually grant this
permission. As with most Virtuoso DBA tasks, the Conductor provides the simplest means of
doing this.</para>
    </sect2>
    <sect2 id="rdfinsertmethodsimilerdfbankapi">
      <title>Using SIMILE RDF Bank API</title>
      <para>Virtuoso implements the HTTP-based Semantic Bank API that enables client
applications to post to its RDF Triple Store. This method offers an alternative to
using Virtuoso/PL functions or WebDAV uploads as the triples-insertion mechanism.</para>
      <para><emphasis>Example:</emphasis></para>
      <para>From your machine go to Firefox->Tools->PiggyBank->My Semantic Bank Accounts</para>
      <para>Add in the shown form:</para>
      <itemizedlist>
        <listitem>For bank: address: http://demo.openlinksw.com/bank</listitem>
        <listitem>For account id: demo</listitem>
        <listitem>For password: demo</listitem>
      </itemizedlist>
      <para>Go to http://demo.openlinksw.com/ods</para>
      <para>Log in as user demo, password: demo</para>
      <para>Go to the Weblog tab from the main ODS Navigation</para>
      <para>Click on weblog instance name, for ex. "demo's Weblog".</para>
      <para>When the weblog home page is loaded, click Alt + P.</para>
      <para>As result is shown the "My PiggyBank" page with all the collected information
presented in items.</para>
      <para>For several of the items add Tags from the form "Tag" shown for each of them.</para>
      <para>As result should be shown the message "Last updated: [here goes the date value].</para>
      <para>You can also click "Save" and "Publish" for these items.</para>
      <para>Go to http://demo.openlinksw.com/sparql</para>
      <para>Enter for the "Default Graph URI" field: http://simile.org/piggybank/demo</para>
      <para>Enter for the "Query text" text-area:</para>
<programlisting><![CDATA[
prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
prefix sioc: <http://rdfs.org/sioc/ns#>
select *
from <http://simile.org/piggybank/demo>
where {?s ?p  ?o}
]]></programlisting>
      <para>Click "Run Query".</para>
      <para>As results are shown the found results.</para>
    </sect2>
    <sect2 id="rdfinsertmethodrdfnet">
      <title>Using RDF NET</title>
      <para><emphasis>Example:</emphasis></para>
      <para>Execute the following query:</para>
<programlisting><![CDATA[
SQL> select DB.DBA.HTTP_RDF_NET ('sparql load
"http://www.openlinksw.com/dataspace/person/kidehen@openlinksw.com"
into graph <http://www.openlinksw.com/>');
]]></programlisting>
      <para>As result should be shown:</para>
<programlisting><![CDATA[
callret
VARCHAR
_______________________________________________________

<?xml version="1.0" ?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#"
xmlns="http://example.org/book/" xmlns:dc="http://purl.org/dc/elements/1.1/"
xmlns:ns="http://example.org/ns#">
<rdf:Description>
<callret-0>Load <http://www.openlinksw.com/dataspace/person/kidehen@openlinksw.com>
into graph <http://www.openlinksw.com/> -- done</callret-0>
</rdf:Description>
</rdf:RDF>

1 Rows. -- 1982 msec.
]]></programlisting>
    </sect2>
</sect1>
<sect1 id="virtuososponger"><title>Virtuoso Sponger</title>
<para>The Virtuoso Sponger is a middleware component of Virtuoso that generates RDF Linked Data from
a variety of data sources. The sponger is transparently integrated into the Virtuoso SPARQL Query
Processor, where it serves as part of the URI/IRI dereferencing functionality. It is also optionally
used by the Virtuoso Content Crawler.
</para>
<para>
A majority of the worlds data naturally resides in non RDF form at the current time. The Sponger
delivers middleware that accelerates the bootstrap of the Semantic Data Web by generating RDF
from non RDF data sources, unobtrusively.
</para>
<para>
When an RDF aware client requests data from a network accessible resource via the Sponger
the following events occur:
</para>
<itemizedlist>
<listitem>A requests in made for data in RDF form, and if RDF is returned nothing further happens</listitem>
<listitem>If RDF isn't returned, then the Sponger passes the data through a Metadata Extraction
Pipeline process (using Metadata Extractors)</listitem>
<listitem>The extracted data is transformed to RDF via a Mapping Pipeline process (RDF is extracted by way
of Ontology matching and mapping) that results in RDF Entities (instance data) generation</listitem>
<listitem>RDF Entities are returned to the client</listitem>
</itemizedlist>
<para>The imported data forms a local cache and its invalidation rules conform to those of traditional
HTTP clients (Web Browsers). Thus, expiration time is determined based on subsequent data fetches of
the same resource (note: the first data load will record the 'expires' header) with current time
compared to expiration time stored in the local cache. If HTTP 'expires' header data isn't returned
by the source data server, then the "Sponger" will derive it's own invalidation time frame by
evaluating the 'date' header and 'last-modified' HTTP headers. Irrespective of path taken,
local cache invalidation is driven by an assessment of current time relative to recorded expiration time.
</para>
<para>Designed with a pluggable architecture, the Sponger's core functionality is provided by Catridges.
Each catridge includes Data Extractors which extract data from one or more data sources, and Ontology
Mappers which map the extracted data to one or more ontologies/schemas, and route to producing RDF
Linked Data.
</para>
<para>The Schema Mappers are typically XSLT (e.g. GRDDL and other OpenLink Mapping Schemes) or
Virtuoso PL based. The Metadata Extractors may be developed in Virtuoso PL, C/C++, Java, or any
other language that can be integrated into the Virtuoso via it's server extensions APIs.
</para>
<para>
The Sponger also includes a pluggable name resolution mechanism that enables the development of
Custom Resolvers for naming schemes (e.g. URNs) associated with protocols beyond HTTP.
Examples of custom resolvers include:
</para>
<itemizedlist>
<listitem>LSID</listitem>
<listitem>DOI</listitem>
</itemizedlist>
<sect2 id="virtuosospongercartridges"><title>Virtuoso Cartridge-Supported Data Sources</title>
<itemizedlist>
<listitem>RDF that doesn't contain Linked Data URIs</listitem>
<listitem>Non-RDF Data Sources
  <itemizedlist>
    <listitem>Data Portability formats
      <itemizedlist>
        <listitem>RSS</listitem>
        <listitem>ATOM</listitem>
        <listitem>OPML</listitem>
        <listitem>XBEL</listitem>
        <listitem>XBRL</listitem>
        <listitem>APML</listitem>
        <listitem>XR-XML</listitem>
        <listitem>hCalendar</listitem>
        <listitem>iCalendar</listitem>
        <listitem>hCard</listitem>
        <listitem>vCard</listitem>
        <listitem>XFN</listitem>
        <listitem>OAI</listitem>
        <listitem>GRDDL</listitem>
        <listitem>RDFa</listitem>
        <listitem>DOI</listitem>
      </itemizedlist>
    </listitem>
    <listitem>Vendor specific XML Web Services
      <itemizedlist>
        <listitem>Facebook</listitem>
        <listitem>Amazon</listitem>
        <listitem>eBay</listitem>
        <listitem>Musicbrainz</listitem>
        <listitem>Freebase</listitem>
        <listitem>Calais</listitem>
        <listitem>Yahoo! Finance</listitem>
        <listitem>Flickr</listitem>
        <listitem>Del.icio.us</listitem>
        <listitem>Bugzilla</listitem>
        <listitem>others</listitem>
      </itemizedlist>
    </listitem>
  </itemizedlist>
</listitem>
</itemizedlist>
</sect2>
<sect2 id="virtuosospongercartridgesextractor"><title>Virtuoso Sponger Cartridge RDF Extractor</title>
<para>
Used to extract RDF from a Web Data Source it consumes services from: Virtuoso PL, C/C++, Java
based RDF Extractors</para>
<para>The RDF mappers provide a way to extract metadata from non-RDF documents such as HTML pages,
images Office documents etc. and pass to SPARQL sponger (crawler which retrieve missing
source graphs). For brevity further in this article the "RDF mapper" we simply will call "mapper".
</para>
<para>The mappers consist of PL procedure (hook) and extractor, where extractor itself can be built
using PL, C or any external language supported by Virtuoso server.</para>
<para>Once the mapper is developed it must be plugged into the SPARQL engine by adding a record
in the table DB.DBA.SYS_RDF_MAPPERS.</para>
<para>If a SPARQL query instructs the SPARQL processor to retrieve target graph into local storage,
then the SPARQL sponger will be invoked. If the target graph IRI represents a deferencable URL
then content will be retrieved using content negotiation. The next step is the content type
to be detected:</para>
<itemizedlist>
<listitem>If RDF and no further transformation such as GRDDL is needed, then the process would stop.</listitem>
<listitem>If such as 'text/plain' and is not known to have metadata, then the SPARQL sponger will
look in the DB.DBA.SYS_RDF_MAPPERS table by order of RM_ID and for every matching URL or MIME
type pattern (depends on column RM_TYPE) will call the mapper hook.
  <itemizedlist>
    <listitem>If hook returns zero the next mapper will be tried;</listitem>
    <listitem>If result is negative the process would stop instructing the SPARQL nothing was retrieved;</listitem>
    <listitem>If result is positive the process would stop instructing the SPARQL that metadata was retrieved.</listitem>
  </itemizedlist>
</listitem>
</itemizedlist>
<sect3 id="virtuosospongercartridgesextractorpl"><title>Virtuoso Sponger Cartridge RDF Extractor PL Requirements</title>
<para><emphasis>PL hook requirements:</emphasis></para>
<para>Every PL function used to plug a mapper into SPARQL engine must have following parameters in
the same order:</para>
<itemizedlist>
<listitem>in graph_iri varchar: the graph IRI which is currently retrieved</listitem>
<listitem>in new_origin_uri varchar: the URL of the document retrieved</listitem>
<listitem>in destination varchar: the destination graph IRI</listitem>
<listitem>inout content any: the content of the document retrieved by SPARQL sponger </listitem>
<listitem>inout async_queue any: an asynchronous queue, can be used to push something to execute
on background if needed.</listitem>
<listitem>inout ping_service any: the value of [SPARQL] - PingService INI parameter, could be used
to configure a service notification such as pingthesemanticweb.com</listitem>
<listitem>inout api_key any: a plain text id single key value or serialized vector of key structure,
basically the value of RM_KEY column of the DB.DBA.SYS_RDF_MAPPERS table.</listitem>
</itemizedlist>
<para>Note: the names of the parameters are not important, but their order and presence are!</para>
<para><emphasis>Example Implementation:</emphasis></para>
<para>In the example script bellow we implement a basic mapper, which maps a text/plain mime type to an
imaginary ontology, which extends the class Document from FOAF with properties 'txt:UniqueWords'
and 'txt:Chars', where the prefix 'txt:' we specify as 'urn:txt:v0.0:'.</para>
<programlisting><![CDATA[
use DB;

create procedure DB.DBA.RDF_LOAD_TXT_META
 (
  in graph_iri varchar,
  in new_origin_uri varchar,
  in dest varchar,
  inout ret_body any,
  inout aq any,
  inout ps any,
  inout ser_key any
  )
{
  declare words, chars int;
  declare vtb, arr, subj, ses, str any;
  declare ses any;
  -- if any error we just say nothing can be done
  declare exit handler for sqlstate '*'
    {
      return 0;
    };
  subj := coalesce (dest, new_origin_uri);
  vtb := vt_batch ();
  chars := length (ret_body);

  -- using the text index procedures we get a list of words
  vt_batch_feed (vtb, ret_body, 1);
  arr := vt_batch_strings_array (vtb);

  -- the list has 'word' and positions array, so we must divide by 2
  words := length (arr) / 2;
  ses := string_output ();

  -- we compose a N3 literal
  http (sprintf ('<%s> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://xmlns.com/foaf/0.1/Document> .\n', subj), ses);
  http (sprintf ('<%s> <urn:txt:v0.0:UniqueWords> "%d" .\n', subj, words), ses);
  http (sprintf ('<%s> <urn:txt:v0.0:Chars> "%d" .\n', subj, chars), ses);
  str := string_output_string (ses);

  -- we push the N3 text into the local store
  DB.DBA.TTLP (str, new_origin_uri, subj);
  return 1;
};

delete from DB.DBA.SYS_RDF_MAPPERS where RM_HOOK = 'DB.DBA.RDF_LOAD_TXT_META';

insert soft DB.DBA.SYS_RDF_MAPPERS (RM_PATTERN, RM_TYPE, RM_HOOK, RM_KEY, RM_DESCRIPTION)
values ('(text/plain)', 'MIME', 'DB.DBA.RDF_LOAD_TXT_META', null, 'Text Files (demo)');

-- here we set order to some large number so don't break existing mappers
update DB.DBA.SYS_RDF_MAPPERS set RM_ID = 2000 where RM_HOOK = 'DB.DBA.RDF_LOAD_TXT_META';
]]></programlisting>
<para>To test the mapper we just use /sparql endpoint with option 'Retrieve remote RDF data
for all missing source graphs' to execute:</para>
<programlisting><![CDATA[
select * from <URL-of-a-txt-file> where { ?s ?p ?o }
]]></programlisting>
<para>It is important that the SPARQL_UPDATE role to be granted to "SPARQL" account in order
to allow local repository update via sponge feature.</para>
<para><emphasis>Authentication in Sponger</emphasis></para>
<para>To enable usage of user defined authentication, there are added more parameters to the
/proxy/rdf and /sparql endpoints. So to use it, the RDF browser and iSPARQL should send following
url parameters:</para>
<itemizedlist>
<listitem>for /proxy/rdf endpoint:
<programlisting><![CDATA[
'login=<account name>'
]]></programlisting>
</listitem>
<listitem>for /sparql endpoint:
<programlisting><![CDATA[
get-login=<account name>
]]></programlisting>
</listitem>
</itemizedlist>
</sect3>
<sect3 id="virtuosospongercartridgesextractorusecases"><title>RDF Cartridges Use Cases</title>
<para>This section contains examples of Web resources which can be transformed by RDF Cartridges.
It also states where additional setup for given cartrides is needed i.e. keys account names etc.
</para>
<para><emphasis>Service based:</emphasis></para>
<itemizedlist>
<listitem>amazon
<programlisting><![CDATA[
needs: api key
example: http://www.amazon.com/gp/product/0553383043
]]></programlisting>
</listitem>
<listitem>ebay
<programlisting><![CDATA[
needs: account, api-key
example: http://cgi.ebay.com/RARE-DAY-IN-FAIRY-LAND-ELEPHANT-FOLIO-20-FULL-COLOR_W0QQitemZ140209597189QQihZ004QQcategoryZ29223QQssPageNameZWDVWQQrdZ1QQcmdZViewItem
]]></programlisting>
</listitem>
<listitem>flickr
needs: api-key
example: http://farm1.static.flickr.com/212/496684670_7122c831ed.jpg
<programlisting><![CDATA[
]]></programlisting>
</listitem>
<listitem>mbz
<programlisting><![CDATA[
example: http://musicbrainz.org/release/37e955d4-a53c-45aa-a812-1b23b88dbc13.html
]]></programlisting>
</listitem>
<listitem>mql (freebase)
<programlisting><![CDATA[
example: http://www.freebase.com/view/en/beta_ursae_majoris
]]></programlisting>
</listitem>
<listitem>facebook
<programlisting><![CDATA[
needs: api-key, secret, persistent-session-id
example: http://www.facebook.com/profile.php?id=841100003
]]></programlisting>
</listitem>
<listitem>yahoo-stock
<programlisting><![CDATA[
example: http://finance.yahoo.com/q?s=AAPL
]]></programlisting>
</listitem>
<listitem>yahoo-traffic
<programlisting><![CDATA[
example: http://local.yahooapis.com/MapsService/V1/trafficData?appid=YahooDemo&street=701+First+Street&city=Sunnyvale&state=CA
]]></programlisting>
</listitem>
<listitem>Bugzilla
<programlisting><![CDATA[
example: https://bugzilla.mozilla.org/show_bug.cgi?id=251714
]]></programlisting>
</listitem>
<listitem>SVG</listitem>
<listitem>OO document
<programlisting><![CDATA[
needs: unzip plugin
]]></programlisting>
</listitem>
<listitem>Wikipedia
<programlisting><![CDATA[
needs: php plugin & dbpedia extractor
example: http://wikipedia.org/wiki/London
]]></programlisting>
</listitem>
<listitem>Opencalais</listitem>
<listitem>iCalendar</listitem>
</itemizedlist>
<para><emphasis>GRDDL</emphasis></para>
<itemizedlist>
<listitem>Google Base (google)
<programlisting><![CDATA[
example: http://www.google.com/base/feeds/snippets/17891817243016304554
]]></programlisting>
</listitem>
<listitem>eRDF</listitem>
<listitem>RDFa</listitem>
<listitem>hCard</listitem>
<listitem>hCalendar</listitem>
<listitem>hReview</listitem>
<listitem>relLicense</listitem>
<listitem>XBRL</listitem>
<listitem>HR-XML</listitem>
<listitem>DC</listitem>
<listitem>geoURL</listitem>
<listitem>Ning</listitem>
<listitem>XFN</listitem>
<listitem>xFolk</listitem>
</itemizedlist>
<para><emphasis>URN handlers</emphasis></para>
<itemizedlist>
<listitem>LSID
<programlisting><![CDATA[
example: urn:lsid:ubio.org:namebank:12292
]]></programlisting>
</listitem>
<listitem>DOI
<programlisting><![CDATA[
needs: hslookup plugin, relevant html, pdf, xml etc. mappers enabled
example: doi:10.1038/35057062
]]></programlisting>
</listitem>
<listitem>OAI
<programlisting><![CDATA[
example: oai:dcmi.ischool.washington.edu:article/8
]]></programlisting>
</listitem>
</itemizedlist>
</sect3>
</sect2>
<sect2 id="virtuosospongerrdfmappers"><title>Extending SPARQL IRI Dereferencing with RDF Mappers</title>
<para>The Virtuoso SPARQL engine (called for brevity just SPARQL bellow) supports IRI Dereferencing,
however it understands only RDF data, that is it can retrieve only files containing RDF/XML, turtle
or N3 serialized RDF data, if format is unknown it will try mapping with built-in WebDAV metadata
extractor. In order to extend this feature with dereferencing web or file resources which naturally
don't have RDF data (like PDF, JPEG files for example) is provided a special mechanism in SPARQL
engine. This mechanism is called RDF mappers for translation of non-RDF data files to RDF.</para>
<para>In order to instruct the SPARQL to call a RDF mapper it needs to be registered and it will
be called for a given URL or MIME type pattern. In other words, when unknown for SPARQL format is
received during URL dereferencing process, it will look into a special registry (a table) to match
either the MIME type or IRI using a regular expression, if match is found the mapper function will
be called.</para>
<sect3 id="virtuosospongerrdfmappersregistry"><title>Regsitry</title>
<para>The table DB.DBA.SYS_RDF_MAPPERS is used as registry for registering RDF mappers.</para>
<programlisting><![CDATA[
create table DB.DBA.SYS_RDF_MAPPERS (
    RM_ID integer identity,         -- mapper ID, designate order of execution
    RM_PATTERN varchar,             -- a REGEX pattern to match URL or MIME type
    RM_TYPE varchar default 'MIME', -- what property of the current resource to match: MIME or URL are supported at present
    RM_HOOK varchar,                -- fully qualified PL function name e.q. DB.DBA.MY_MAPPER_FUNCTION
    RM_KEY  long varchar,           -- API specific key to use
    RM_DESCRIPTION long varchar,    -- Mapper description, free text
    RM_ENABLED integer default 1,   -- a flag 0 or 1 integer to include or exclude the given mapper from processing chain
    primary key (RM_TYPE, RM_PATTERN))
;
]]></programlisting>
<para>The current way to register/update/unregister a mapper is just a DML statement e.g.
NSERT/UPDATE/DELETE.</para>
</sect3>
<sect3 id="virtuosospongerrdfmappersexec"><title>Execution order and processing</title>
<para>When SPARQL retrieves a resource with unknown content it will look in the mappers registry
and will loop over every record having RM_ENABLED flag true. The sequence of look-up is based on
ordering by RM_ID column. For every record it will either try matching the MIME type or URL against
RM_PATTERN value and if there is match the function specified in RM_HOOK column will be called.
If the function doesn't exists or signal an error the SPARQL will look at next record.</para>
<para>When it stops looking? It will stop if value returned by mapper function is positive or
negative number, if the return is negative processing stops with meaning no RDF was supplied,
if return is positive the meaning is that RDF data was extracted, if zero integer is returned
then SPARQL will look for next mapper. The mapper function also can return zero if it is expected
next mapper in the chain to get more RDF data.</para>
<para>If none of the mappers matches the signature (MIME type nor URL) the built-in WebDAV
metadata extractor will be called.</para>
</sect3>
<sect3 id="virtuosospongerrdfmappersextfunc"><title>Extension function</title>
<para>The mapper function is a PL stored procedure with following signature:</para>
<programlisting><![CDATA[
THE_MAPPER_FUNCTION_NAME (
        in graph_iri varchar,
        in origin_uri varchar,
        in destination_uri varchar,
        inout content varchar,
        inout async_notification_queue any,
        inout ping_service any,
        inout keys any
        )
{
   -- do processing here
   -- return -1, 0 or 1 (as explained above in Execution order and processing section)
}
;
]]></programlisting>
<para><emphasis>Parameters</emphasis></para>
<itemizedlist>
<listitem>graph_iri - the target graph IRI</listitem>
<listitem>origin_uri - the current URI of processing</listitem>
<listitem>destination_uri - get:destination value</listitem>
<listitem>content - the resource content</listitem>
<listitem>async_notification_queue - if INI parameter PingService is specified in SPARQL
section in the INI file, this is a pre-allocated asynchronous queue to be used to call
ping service</listitem>
<listitem>ping_service - the URL of the ping service configured in SPARQL section in the
INI in PingService parameter</listitem>
<listitem>keys - a string value contained in the RM_KEY column for given mapper, can be
single string or serialized array, generally can be used as mapper specific data.</listitem>
</itemizedlist>
<para><emphasis>Return value</emphasis></para>
<itemizedlist>
<listitem>0 - no data was retrieved or some next matching mapper must extract more data</listitem>
<listitem>1 - data is retrieved, stop looking for other mappers</listitem>
<listitem>-1 - no data is retrieved, stop looking for more data</listitem>
</itemizedlist>
</sect3>
<sect3 id="virtuosospongerrdfmapperspackage"><title>RDF Mappers package content</title>
<para>The Virtuoso supply as a rdf_mappers_dav VAD package a cartridge for extracting RDF data
from certain popular Web resources and file types. It can be installed (if not already) using
VAD_INSTALL function, see the VAD chapter in documentation on how to do that.</para>
<para><emphasis>HTTP-in-RDF</emphasis></para>
<para>Maps the HTTP request response to HTTP Vocabulary in RDF, see http://www.w3.org/2006/http#.</para>
<para>This mapper is disabled by default. If it's enabled , it must be first in order of execution.</para>
<para>Also it always will return 0, which means any other mapper should push more data.</para>
<para><emphasis>HTML</emphasis></para>
<para>This mapper is composite, it looking for metadata which can specified in a HTML pages as
follows:</para>
<itemizedlist>
<listitem>Embedded/linked RDF
  <itemizedlist>
    <listitem>scan for meta in RDF
<programlisting><![CDATA[
<link rel="meta" type="application/rdf+xml"
]]></programlisting></listitem>
    <listitem>RDF embedded in xHTML (as markup or inside XML comments)</listitem>
  </itemizedlist>
</listitem>
<listitem>Micro-formats
      <itemizedlist>
    <listitem>GRDDL - GRDDL Data Views: RDF expressed in XHTML and XML: http://www.w3.org/2003/g/data-view#</listitem>
    <listitem>eRDF - http://purl.org/NET/erdf/profile</listitem>
    <listitem>RDFa</listitem>
    <listitem>hCard - http://www.w3.org/2006/03/hcard</listitem>
    <listitem>hCalendar - http://dannyayers.com/microformats/hcalendar-profile</listitem>
    <listitem>hReview - http://dannyayers.com/micromodels/profiles/hreview</listitem>
    <listitem>relLicense - CC license: http://web.resource.org/cc/schema.rdf</listitem>
    <listitem>Dublin Core (DCMI) - http://purl.org/dc/elements/1.1/</listitem>
    <listitem>geoURL - http://www.w3.org/2003/01/geo/wgs84_pos#</listitem>
    <listitem>Google Base - OpenLink Virtuoso specific mapping</listitem>
    <listitem>Ning Metadata </listitem>
      </itemizedlist>
</listitem>
<listitem>Feeds extraction
      <itemizedlist>
    <listitem>RSS/RDF - SIOC &amp; AtomOWL</listitem>
    <listitem>RSS 1.0 - RSS/RDF, SIOC &amp; AtomOWL</listitem>
    <listitem>Atom 1.0 - RSS/RDF, SIOC &amp; AtomOWL</listitem>
      </itemizedlist>
</listitem>
<listitem>xHTML metadata transformation using FOAF (foaf:Document) and Dublin Core
properties (dc:title, dc:subject etc.)</listitem>
</itemizedlist>
<para>The HTML page mapper will look for RDF data in order as listed above, it will try to extract
metadata on each step and will return positive flag if any of the above step give a RDF data. In
case where page URL matches some of other RDF mappers listed in registry it will return 0 so
next mapper to extract more data. In order to function properly, this mapper must be executed
before any other specific mappers.</para>
<para><emphasis>Flickr URLs</emphasis></para>
<para>This mapper extracts metadata of the Flickr images, using Flickr REST API. To function
properly it must have configured key. The Flickr mapper extracts metadata using: CC license,
Dublin Core, Dublin Core Metadata Terms, GeoURL, FOAF, EXIF: http://www.w3.org/2003/12/exif/ns/ ontology.
</para>
<para><emphasis>Amazon URLs</emphasis></para>
<para>This mapper extracts metadata for Amazon articles, using Amazon REST API. It needs a Amazon
API key in order to be functional.</para>
<para><emphasis>eBay URLs</emphasis></para>
<para>Implements eBay REST API for extracting metadata of eBay articles, it needs a key and user
name to be configured in order to work.</para>
<para><emphasis>Open Office (OO) documents</emphasis></para>
<para>The OO documents contains metadata which can be extracted using UNZIP, so this extractor
needs Virtuoso unzip plugin to be configured on the server.</para>
<para><emphasis>Yahoo traffic data URLs</emphasis></para>
<para>Implements transformation of the result of Yahoo traffic data to RDF.
</para>
<para><emphasis>iCal files</emphasis></para>
<para>Transform iCal files to RDF as per http://www.w3.org/2002/12/cal/ical# .</para>
<para><emphasis>Binary content, PDF, PowerPoint</emphasis></para>
<para>The unknown binary content, PDF and MS PowerPoint files can be transformed to RDF using
Aperture framework (http://aperture.sourceforge.net/). This mapper needs Virtuoso with Java hosting
support, Aperture framework and MetaExtractor.class installed on the host system in order to work.</para>
<para>The Aperture framework &amp; MetaExtractor.class must be installed on the system before to
install the RDF mappers package. If the package is already installed, then to activate this mapper
you can just re-install the VAD.</para>
<para><emphasis>Setting-up Virtuoso with Java hosting to run Aperture framework</emphasis></para>
<itemizedlist>
<listitem>Install Virtuoso with Java hosting</listitem>
<listitem>Download the Aperture framework from http://aperture.sourceforge.net</listitem>
<listitem>unpack in the Virtuoso working directory e.q. where database file is, make a symbolic link 'lib' to it</listitem>
<listitem>configure in the INI Parametres section:
<programlisting><![CDATA[
JavaClasspath = lib/sesame-2.0-alpha-3.jar:lib/openrdf-util-crazy-debug.jar:lib/htmlparser-1.6.jar:lib/activation-1.0.2-upd2.jar:lib/bcmail-jdk14-132.jar:lib/poi-scratchpad-3.0-alpha2-20060616.jar:lib/openrdf-model-2.0-alpha-3.jar:lib/jacob-1.10-pre4.jar:lib/bcprov-jdk14-132.jar:lib/demork-2.0.jar:lib/commons-codec.jar:lib/fontbox-0.1.0-dev.jar:lib/pdfbox-0.7.3.jar:lib/applewrapper-0.1.jar:lib/junit-3.8.1.jar:lib/winlaf-0.5.1.jar:lib/aperture-test-2006.1-alpha-3.jar:lib/openrdf-util-fixed-locking.jar:lib/commons-logging-1.1.jar:lib/mail-1.4.jar:lib/aperture-2006.1-alpha-3.jar:lib/poi-3.0-alpha2-20060616.jar:lib/ical4j-cvs20061019.jar:lib/openrdf-util-2.0-alpha-3.jar:lib/rio-2.0-alpha-3.jar:lib/poi-contrib-3.0-alpha2-20060616.jar:lib/aperture-examples-2006.1-alpha-3.jar:.
]]></programlisting>
</listitem>
<listitem>Make sure MetaExtractor.class is in the Virtuoso working directory</listitem>
<listitem>Start the Virtuoso server with java hosting support</listitem>
<listitem>connect with ISQL tool and check if installation is complete:
<programlisting><![CDATA[
SQL> DB.DBA.import_jar (NULL, 'MetaExtractor', 1);

Done. -- 466 msec.
SQL> select "MetaExtractor"().getMetaFromFile ('some_pdf_in_server_working_dir.pdf', 5);
... some RDF must be returned ...
]]></programlisting>
</listitem>
</itemizedlist>
<para>Important: the above is verified to work with aperture-2006.1-alpha-3 on Linux system.
For different version of Aperture of operation system this may need some adjustments e.g. to
re-build MetaExtractor.class &amp; changes to CLASSPATH etc.</para>
<para><emphasis>Examples &amp; tutorials</emphasis></para>
<para>How to write own RDF mapper? Look at Virtuoso tutorial on this subject
http://demo.openlinksw.com/tutorial/rdf/rd_s_1/rd_s_1.vsp .
</para>
</sect3>
<sect3 id="virtuosospongerproxy"><title>Sponger Proxy service</title>
<para>Sponger functionality is also exposed via Virtuoso's "/proxy/rdf/" endpoint, as an in-built
REST style Web service available in any Virtuoso standard installation. This web service takes
a target URL and either returns the content "as is" or tries to transform (by sponging) to RDF.
Thus, the proxy service can be used as a 'pipe' for RDF browsers to browse non-RDF sources.
</para>
<para>For more information see <link linkend="rdfproxyservice">RDF Sponger Proxy service</link></para>
</sect3>
</sect2>
</sect1>
<sect1 id="rdfiridereferencing"><title>Dereferencable IRIs and RDF Linked Data</title>
<para>There are many cases when RDF data should be retrieved from remote sources only when really needed.
E.g., a scheduling application may read personal calendars from personal sites of its users.
Calendar data expire quickly, so there's no reason to frequently re-load them in hope that they are queried before expired.
</para>
<para>Virtuoso extends SPARQL so it is possible to download RDF resource from a given IRI, parse them and store the resulting triples in a graph, all three operations will be performed during the SPARQL query execution.
The IRI of graph to store triples is usually equal to the IRI where the resource is download from, so the feature is named &quot;IRI dereferencing&quot;
There are two different use cases for this feature.
In simple case, a SPARQL query contains <emphasis>from</emphasis> clauses that enumerate graphs to process, but there are no triples in <emphasis>DB.DBA.RDF_QUAD</emphasis> that correspond to some of these graphs.
The query execution starts with dereferencing of these graphs and the rest runs as usual.
In more sophisticated case, the query is executed many times in a loop.
Every execution produces a partial result.
SPARQL processor checks for IRIs in the result such that resources with that IRIs may contain relevant data but not yet loaded into the <emphasis>DB.DBA.RDF_QUAD</emphasis>.
After some iteration, the partial result is identical to the result of the previous iteration, because there's no more data to retrieve.
As the last step, SPARQL processor builds the final result set.
</para>
<sect2 id="rdfinputgrab"><title>IRI Dereferencing For FROM Clauses, &quot;define get:...&quot; Pragmas</title>
<para>Virtuoso extends SPARQL syntax of <emphasis>from</emphasis> and <emphasis>from named</emphasis> clauses.
It allows additional list of options at end of clause: <emphasis>option ( param1 value1, param2 value2, ... )</emphasis>
where parameter names are QNames that start with <emphasis>get:</emphasis> prefix and values are &quot;precode&quot; expressions, i.e. expressions that does not contain variables other than external parameters.
Names of allowed parameters are listed below.
</para>
<itemizedlist>
  <listitem><emphasis>get:soft</emphasis> is the retrieval mode, supported values are &quot;soft&quot; and &quot;replacing&quot;.
If the value is &quot;soft&quot; then the SPARQL processor will not even try to retrieve triples if the destination graph is non-empty.
Other <emphasis>get:...</emphasis> parameters are useless without this one.</listitem>
  <listitem><emphasis>get:uri</emphasis> is the IRI to retrieve if it is not equal to the IRI of the <emphasis>from</emphasis> clause.
These can be used if data should be retrieved from a mirror, not from original resource location or in any other case when the destination graph IRI differs from the location of the resource.</listitem>
<programlisting><![CDATA[
SQL>sparql
define get:uri "http://myopenlink.net/dataspace/person/kidehen"
select ?id
from named <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/sioc.ttl>
where { graph ?g { ?id a ?o } }
limit 10;

id
VARCHAR
_______________________________________________________________________________

http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1231
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1231
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1243
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1243
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D

10 Rows. -- 10 msec.

]]></programlisting>
  <listitem><emphasis>get:method</emphasis> is the HTTP method that should be used to retrieve the resource, supported methods are &quot;GET&quot; for plain HTTP and &quot;MGET&quot; for URIQA web service endpoint.
By default, &quot;MGET&quot; is used for IRIs that end with &quot;/&quot; and &quot;GET&quot; for everything else.</listitem>
  <listitem><emphasis>get:refresh</emphasis> is the maximum allowed age of the cached resource, no matter what is specified by the server where the resource resides.
The value is an positive integer (number of seconds). Virtuoso reads HTTP headers and uses &quot;Date&quot;, &quot;ETag&quot;, &quot;Expires&quot;, &quot;Last-Modified&quot;, &quot;Cache-Control&quot; and &quot;Pragma: no-cache&quot; fields to calculate when the resource should be reloaded, this value can become smaller due to <emphasis>get:refresh</emphasis> but can not be incremented.</listitem>
<programlisting><![CDATA[
sparql
define get:refresh "3600"
select ?id
from named <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/sioc.ttl>
where { graph ?g { ?id a ?o } }
limit 10;

id
VARCHAR
_______________________________________________________________________________

http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1231
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1231
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1243
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1243
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D

10 Rows. -- 10 msec.

]]></programlisting>
  <listitem><emphasis>get:proxy</emphasis> address of the proxy server, as &quot;host:port&quot; string, if direct download is impossible; the default is to not use proxy.</listitem>
<programlisting><![CDATA[
sparql
define get:proxy "www.openlinksw.com:80"
define get:method "GET"
define get:soft "soft"
select ?id
from named <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/sioc.ttl>
where { graph ?g { ?id a ?o } }
limit 10;

id
VARCHAR
_______________________________________________________________________________

http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1231
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1231
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1243
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1243
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1261
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com#this
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D
http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D

10 Rows. -- 10 msec.
SQL> limit 10;
]]></programlisting>

<!--
  <listitem><emphasis>get:login</emphasis></listitem>
  <listitem><emphasis>get:password</emphasis></listitem>
  <listitem><emphasis>get:query</emphasis></listitem> -->
<para>If a value of some <emphasis>get:...</emphasis> parameter repeats for every <emphasis>from</emphasis> clause then it can be written as a global
pragma like <emphasis>define get:soft "soft"</emphasis>.
The following two queries will work identically:
</para>
<programlisting><![CDATA[
sparql
select ?id
from named <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/sioc.ttl>
  option (get:soft "soft", get:method "GET")
from named <http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/sioc.ttl>
  option (get:soft "soft", get:method "GET")
where { graph ?g { ?id a ?o } }
limit 10;

id
VARCHAR
_______________________________________________________________________________

http://www.openlinksw.com/dataspace/person/oerling#this
http://www.openlinksw.com/mt-tb
http://www.openlinksw.com/RPC2
http://www.openlinksw.com/dataspace/oerling#this
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/958
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/958
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/949
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/949

10 Rows. -- 862 msec.
]]></programlisting>
<programlisting><![CDATA[
sparql
define get:method "GET"
define get:soft "soft"
select ?id
from named <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/sioc.ttl>
from named <http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/sioc.ttl>
where { graph ?g { ?id a ?o } }
limit 10;

id
VARCHAR
_______________________________________________________________________________

http://www.openlinksw.com/dataspace/person/oerling#this
http://www.openlinksw.com/mt-tb
http://www.openlinksw.com/RPC2
http://www.openlinksw.com/dataspace/oerling#this
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/958
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/958
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/949
http://www.openlinksw.com/dataspace/oerling/weblog/Orri%20Erling%27s%20Blog/949

10 Rows. -- 10 msec.
]]></programlisting>
<para>
It can make text shorter and it is especially useful when the query text comes from client but the parameter should have a fixed value due to security reasons:
the values set by <emphasis>define get:...</emphasis> can not be redefined inside the query and the application may prevent the text with desired pragmas before the execution.
</para>
<para>
Note that the user should have <emphasis>SPARQL_UPDATE</emphasis> role in order to execute such a query.
By default SPARQL web service endpoint is owned by <emphasis>SPARQL</emphasis> user that have <emphasis>SPARQL_SELECT</emphasis> but not
<emphasis>SPARQL_UPDATE</emphasis>.
It is possible in principle to grant <emphasis>SPARQL_UPDATE</emphasis> to <emphasis>SPARQL</emphasis> but this breaches the whole security of the RDF storage.
</para>
<listitem><emphasis>FROM CLAUSE with options</emphasis>: options in OPTION() list should be delimited with commas.
grab options are not allowed as they are global for the query. Only specific 'get:xxx' options are useful here.</listitem>
<programlisting><![CDATA[
SQL>sparql
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT DISTINCT ?friend
FROM NAMED  <http://myopenlink.net/dataspace/person/kidehen>
OPTION (get:soft "soft", get:method "GET")
WHERE
  {
      <http://myopenlink.net/dataspace/person/kidehen#this> foaf:knows
?friend .
  };
friend
VARCHAR
_______________________________________________________________________________

http://www.dajobe.org/foaf.rdf#i
http://www.w3.org/People/Berners-Lee/card#i
http://www.w3.org/People/Connolly/#me
http://my.opera.com/chaals/xml/foaf#me
http://www.w3.org/People/Berners-Lee/card#amy
http://www.w3.org/People/EM/contact#me
http://myopenlink.net/dataspace/person/ghard#this
http://myopenlink.net/dataspace/person/omfaluyi#this
http://myopenlink.net/dataspace/person/alanr#this
http://myopenlink.net/dataspace/person/bblfish#this
http://myopenlink.net/dataspace/person/danja#this
http://myopenlink.net/dataspace/person/tthibodeau#this
...
36 Rows. -- 1693 msec.
]]></programlisting>
</itemizedlist>
</sect2>
<sect2 id="rdfinputgrab"><title>IRI Dereferencing For Variables, &quot;define input:grab-...&quot; Pragmas</title>
<para>
Consider a set of personal data such that one resource can list many persons and point to resources where that persons are described in more details.
E.g. resource about <emphasis>user1</emphasis> describes the user and also contain statements that <emphasis>user2</emphasis> and <emphasis>user3</emphasis> are persons and more data can be found in <emphasis>user2.ttl</emphasis> and <emphasis>user3.ttl</emphasis>,
<emphasis>user3.ttl</emphasis> can contain statements that <emphasis>user4</emphasis> is also person and more data can be found in <emphasis>user4.ttl</emphasis> and so on.
The query should find as many users as it is possible and return their names and e-mails.
</para>
<para>
If all data about all users were loaded into the database, the query could be quite simple:
</para>
<programlisting><![CDATA[
SQL>sparql
prefix foaf: <http://xmlns.com/foaf/0.1/>
prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
select ?id ?firstname ?nick
where
  {
    graph ?g
      {
        ?id rdf:type foaf:Person.
        ?id foaf:firstName ?firstname.
        ?id foaf:knows ?fn .
        ?fn foaf:nick ?nick.
      }
   }
limit 10;

id                                                      firstname  nick
VARCHAR                                                 VARCHAR    VARCHAR
_______________________________________________________________________________

http://myopenlink.net/dataspace/person/pmitchell#this   LaRenda    sdmonroe
http://myopenlink.net/dataspace/person/pmitchell#this   LaRenda    kidehen{at}openlinksw.com
http://myopenlink.net/dataspace/person/pmitchell#this   LaRenda    alexmidd
http://myopenlink.net/dataspace/person/abm#this         Alan       kidehen{at}openlinksw.com
http://myopenlink.net/dataspace/person/igods#this       Cameron    kidehen{at}openlinksw.com
http://myopenlink.net/dataspace/person/goern#this       Christoph  captsolo
http://myopenlink.net/dataspace/person/dangrig#this     Dan        rickbruner
http://myopenlink.net/dataspace/person/dangrig#this     Dan        sdmonroe
http://myopenlink.net/dataspace/person/dangrig#this     Dan        lszczepa
http://myopenlink.net/dataspace/person/dangrig#this     Dan        kidehen

10 Rows. -- 80 msec.
]]></programlisting>
<para>
It is possible to enable IRI dereferencing in such a way that all appropriate resources are loaded during the query execution even if names of some of them are not known a priori.
</para>
<programlisting><![CDATA[
SQL>sparql
  define input:grab-var "?more"
  define input:grab-depth 10
  define input:grab-limit 100
  define input:grab-base "http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1300"
  prefix foaf: <http://xmlns.com/foaf/0.1/>
  prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
  prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>
select ?id ?firstname ?nick
where {
    graph ?g {
               ?id rdf:type foaf:Person.
               ?id foaf:firstName ?firstname.
               ?id foaf:knows ?fn .
               ?fn foaf:nick ?nick.
               optional { ?id rdfs:SeeAlso ?more }
            }
}
limit 10;

id                                                         firstname  nick
VARCHAR                                                    VARCHAR    VARCHAR
_______________________________________________________________________________

http://myopenlink.net/dataspace/person/ghard#this          Yrj+?n+?   kidehen
http://inamidst.com/sbp/foaf#Sean                          Sean       d8uv
http://myopenlink.net/dataspace/person/dangrig#this        Dan        rickbruner
http://myopenlink.net/dataspace/person/dangrig#this        Dan        sdmonroe
http://myopenlink.net/dataspace/person/dangrig#this        Dan        lszczepa
http://myopenlink.net/dataspace/person/dangrig#this        Dan        kidehen
http://captsolo.net/semweb/foaf-captsolo.rdf#Uldis_Bojars  Uldis      mortenf
http://captsolo.net/semweb/foaf-captsolo.rdf#Uldis_Bojars  Uldis      danja
http://captsolo.net/semweb/foaf-captsolo.rdf#Uldis_Bojars  Uldis      zool
http://myopenlink.net/dataspace/person/rickbruner#this     Rick       dangrig

10 Rows. -- 530 msec.

]]></programlisting>
<para>
The IRI dereferencing is controlled by the following pragmas:
</para>
<itemizedlist>
  <listitem><emphasis>input:grab-var</emphasis> specifies a name of variable whose values should be used as IRIs of resources that should be downloaded.
It is not an error if the variable is sometimes unbound or gets values that can not be converted to IRIs (e.g., integers) -- bad values are silently ignored.
It is also not an error if the IRI can not be retrieved, this makes IRI retrieval somewhat similar to &quot;best effort union&quot; in SQL.
This pragma can be used more than once to specify many variable names.
It is not an error if values of different variables result in same IRI or a variable gets same value many times -- no one IRI is retrieved more than once.</listitem>
  <listitem><emphasis>input:grab-iri</emphasis> specifies an IRI that should be retrieved before executing the rest of the query, if it is not in the <emphasis>DB.DBA.RDF_QUAD</emphasis> already.
This pragma can be used more than once to specify many IRIs.
The typical use of this pragma is querying a set of related resources when only one &quot;root&quot; resource IRI is known but even that resource is not loaded.</listitem>
<programlisting><![CDATA[
SQL>sparql
  define input:storage ""
  define input:grab-iri <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/sioc.ttl>
  define input:grab-var "id"
  define input:grab-depth 10
  define input:grab-limit 100
  define input:grab-base "http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1300"
select ?id
where { graph ?g { ?id a ?o } }
limit 10;

id
VARCHAR
_______________________________________________________________________________

http://www.openlinksw.com/virtrdf-data-formats#default-iid
http://www.openlinksw.com/virtrdf-data-formats#default-iid-nullable
http://www.openlinksw.com/virtrdf-data-formats#default-iid-nonblank
http://www.openlinksw.com/virtrdf-data-formats#default-iid-nonblank-nullable
http://www.openlinksw.com/virtrdf-data-formats#default
http://www.openlinksw.com/virtrdf-data-formats#default-nullable
http://www.openlinksw.com/virtrdf-data-formats#sql-varchar
http://www.openlinksw.com/virtrdf-data-formats#sql-varchar-nullable
http://www.openlinksw.com/virtrdf-data-formats#sql-longvarchar
http://www.openlinksw.com/virtrdf-data-formats#sql-longvarchar-nullable

10 Rows. -- 530 msec.

]]></programlisting>
  <listitem><emphasis>input:grab-all</emphasis> is the simplest possible way to enable the feature but the resulting performance can be very bad.
It turns all variables and IRI constants in all graph, subject and object fields of all triple patterns of the query into values for
<emphasis>input:grab-var</emphasis> and <emphasis>input:grab-iri</emphasis>,
so the SPARQL processor will dereference everything what might be related to the text of the query.</listitem>
<programlisting><![CDATA[
SQL>sparql
  define input:grab-all "yes"
  define input:grab-depth 10
  define input:grab-limit 100
  define input:grab-base "http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1300"
  prefix foaf: <http://xmlns.com/foaf/0.1/>
  prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
select ?id ?firstname ?nick
where
  {
    graph ?g
     {
       ?id rdf:type foaf:Person.
       ?id foaf:firstName ?firstname.
       ?id foaf:knows ?fn .
       ?fn foaf:nick ?nick.
     }
  }
limit 10;

id                                                      firstname   nick
VARCHAR                                                 VARCHAR     VARCHAR
____________________________________________________________________

http://myopenlink.net/dataspace/person/pmitchell#this   LaRenda     sdmonroe
http://myopenlink.net/dataspace/person/pmitchell#this   LaRenda     kidehen{at}openlinksw.com
http://myopenlink.net/dataspace/person/pmitchell#this   LaRenda     alexmidd
http://myopenlink.net/dataspace/person/abm#this         Alan        kidehen{at}openlinksw.com
http://myopenlink.net/dataspace/person/igods#this       Cameron     kidehen{at}openlinksw.com
http://myopenlink.net/dataspace/person/goern#this       Christoph   captsolo
http://myopenlink.net/dataspace/person/dangrig#this     Dan         rickbruner
http://myopenlink.net/dataspace/person/dangrig#this     Dan         sdmonroe
http://myopenlink.net/dataspace/person/dangrig#this     Dan         lszczepa
http://myopenlink.net/dataspace/person/dangrig#this     Dan         kidehen

10 Rows. -- 660 msec.

]]></programlisting>
  <listitem><emphasis>input:grab-seealso</emphasis> (and synonym <emphasis>input:grab-follow-predicate</emphasis>) specifies an IRI of an predicate similar to foaf:seeAlso.
Predicates of that sort suggest location of resources that contain more data about predicate subject.
The IRI dereferencing routine may use these predicates to find additional IRIs for loading resources.
This is especially useful when the text of the query comes from remote client and may lack triple patterns like
<emphasis><![CDATA[optional { ?id <SeeAlso> ?more }]]></emphasis> from the previous example.
The use of <emphasis>input:grab-seealso</emphasis> makes the SPARQL query nondeterministic, because the order and the number of retrieved documents will
depend on execution plan and they may change from run to run.
This pragma can be used more than once to specify many IRIs, but this feature is costly.
Every additional predicate may result in significant number of lookups in the RDF storage, affecting total execution time.</listitem>
<programlisting><![CDATA[
SQL>sparql
  define input:grab-iri <http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/sioc.ttl>
  define input:grab-var "id"
  define input:grab-depth 10
  define input:grab-limit 100
  define input:grab-base "http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1300"
  define input:grab-seealso <foaf:maker>
    prefix foaf: <http://xmlns.com/foaf/0.1/>
select ?id
where
  {
    graph ?g
      {
        ?id a foaf:Person .
      }
  }
limit 10;

id
VARCHAR
_______________________________________________________________________________

mailto:somebody@example.domain
http://localhost:8895/dataspace/person/dav#this
http://localhost:8895/dataspace/person/dba#this
mailto:2@F.D
http://localhost:8895/dataspace/person/test1#this
http://www.openlinksw.com/blog/~kidehen/gems/rss.xml#Kingsley%20Uyi%20Idehen
http://art.weblogsinc.com/rss.xml#
http://digitalmusic.weblogsinc.com/rss.xml#
http://partners.userland.com/nytrss/books.xml#
http://partners.userland.com/nytrss/arts.xml#

10 Rows. -- 105 msec.

]]></programlisting>
  <listitem><emphasis>input:grab-limit</emphasis> should be an integer that is a maximum allowed number of resource retrievals.
The default value is pretty big (few millions of documents) so it is strongly recommended to set smaller value.
Set it even if you're absolutely sure that the set of resources is small, because program errors are always possible.
All resource downloads are counted, both successful and failed, both forced by <emphasis>input:grab-iri</emphasis> and forced by <emphasis>input:grab-var</emphasis>.
Nevertheless, all constant IRIs specified by <emphasis>input:grab-iri</emphasis> (or <emphasis>input:grab-all</emphasis>) are downloaded before the first check of the <emphasis>input:grab-limit</emphasis> counter,
so this limit will never prevent from downloading &quot;root&quot; resources.
</listitem>
  <listitem><emphasis>input:grab-depth</emphasis> should be an integer that is a maximum allowed number of query iterations.
Every iteration may find new IRIs to retrieve, because resources loaded on previous iteration may add these IRIs to <emphasis>DB.DBA.RDF_QUAD</emphasis> and make result set longer.
The default value is 1, so the SPARQL processor will retrieve only resources explicitly named in &quot;root&quot; resources or in quad that are in the database before the query execution.
</listitem>
  <listitem><emphasis>input:grab-base</emphasis> specifies a base IRI used to convert relative IRIs into absolute. The default is an empty string.</listitem>
<programlisting><![CDATA[
SQL>sparql
  define input:grab-depth 10
  define input:grab-limit 100
  define input:grab-var "more"
  define input:grab-base "http://www.openlinksw.com/dataspace/kidehen@openlinksw.com/weblog/kidehen@openlinksw.com%27s%20BLOG%20%5B127%5D/1300"
  prefix foaf: <http://xmlns.com/foaf/0.1/>
select ?id
where
  {
    graph ?g
     {
       ?id a foaf:Person .
       optional { ?id foaf:maker ?more }
     }
  }
limit 10;

id
VARCHAR
_______________________________________________________________________________

mailto:somebody@example.domain
http://localhost:8895/dataspace/person/dav#this
http://localhost:8895/dataspace/person/dba#this
mailto:2@F.D
http://localhost:8895/dataspace/person/test1#this
http://www.openlinksw.com/blog/~kidehen/gems/rss.xml#Kingsley%20Uyi%20Idehen
http://art.weblogsinc.com/rss.xml#
http://digitalmusic.weblogsinc.com/rss.xml#
http://partners.userland.com/nytrss/books.xml#
http://partners.userland.com/nytrss/arts.xml#

10 Rows. -- 115 msec.

]]></programlisting>
  <listitem><emphasis>input:grab-resolver</emphasis> is a name of procedure that resolve IRIs and determines the HTTP method of retrieval.
The default is name of <emphasis>DB.DBA.RDF_GRAB_RESOLVER_DEFAULT()</emphasis> procedure that is described below.
If other procedure is specified, the signature should match to the default one.</listitem>
  <listitem><emphasis>input:grab-destination</emphasis> is to override the default behaviour of the IRI dereferencing and store all retrieved triples in a single graph.
This is convenient when there's no logical difference where any given triple comes from, and changes in remote resources will only add triples but not make cached triples obsolete.
A SPARQL query is usually faster when all graph IRIs are fixed and there are no graph group patterns with an unbound graph variable, so storing everything in one single graph is worth considering.
</listitem>
  <listitem><emphasis>input:grab-loader</emphasis> is a name of procedure that retrieve the resource via HTTP, parse it and store it.
The default is name of <emphasis>DB.DBA.RDF_SPONGE_UP()</emphasis> procedure; this procedure also used by IRI dereferencing for FROM clauses.
You will probably never need to write your own procedure of this sort but some Virtuoso plugins will provide ready-to-use functions that will retrieve non-RDF resources and extract their metadata as triples or
will implement protocols other than HTTP.
</listitem>
</itemizedlist>
<para>Default resolver procedure is <emphasis>DB.DBA.RDF_GRAB_RESOLVER_DEFAULT()</emphasis>. Note that the function produce two absolute URIs,
<emphasis>abs_uri</emphasis> and <emphasis>dest_uri</emphasis>. Default procedure returns two equal strings, but other may return different values,
e.g., return primary and permanent location of the resource as <emphasis>dest_uri</emphasis> and the fastest known mirror location as
<emphasis>abs_uri</emphasis> thus saving HTTP retrieval time. It can even signal an error to block the downloading of some unwanted resource.</para>
<programlisting><![CDATA[
DB.DBA.RDF_GRAB_RESOLVER_DEFAULT (
  in base varchar,         -- base IRI as specified by input:grab-base pragma
  in rel_uri varchar,      -- IRI of the resource as it is specified by input:grab-iri or a value of a variable
  out abs_uri varchar,     -- the absolute IRI that should be downloaded
  out dest_uri varchar,    -- the graph IRI where triples should be stored after download
  out get_method varchar ) -- the HTTP method to use, should be "GET" or "MGET".
]]></programlisting>
</sect2>
<sect2 id="rdfiridereferencingexamples"><title>Examples of other Web Resolvers</title>
<para>Example of <emphasis>LSIDs</emphasis>: A scientific name from UBio</para>
<programlisting><![CDATA[
SQL>sparql
define get:soft "soft"
select *
from <urn:lsid:ubio.org:namebank:11815>
where { ?s ?p ?o }
limit 5;

s                                 p                                           o
VARCHAR                           VARCHAR                                     VARCHAR
_______________________________________________________________________________

urn:lsid:ubio.org:namebank:11815  http://purl.org/dc/elements/1.1/title       Pternistis leucoscepus
urn:lsid:ubio.org:namebank:11815  http://purl.org/dc/elements/1.1/subject     Pternistis leucoscepus (Gray, GR) 1867
urn:lsid:ubio.org:namebank:11815  http://purl.org/dc/elements/1.1/identifier  urn:lsid:ubio.org:namebank:11815
urn:lsid:ubio.org:namebank:11815  http://purl.org/dc/elements/1.1/creator     http://www.ubio.org
urn:lsid:ubio.org:namebank:11815  http://purl.org/dc/elements/1.1/type        Scientific Name

5 Rows. -- 741 msec.
]]></programlisting>
<para>Example of <emphasis>LSIDs</emphasis>: A segment of the human genome from GDB</para>
<programlisting><![CDATA[
SQL>sparql
define get:soft "soft"
select *
from <urn:lsid:gdb.org:GenomicSegment:GDB132938>
where { ?s ?p ?o }
limit 5;

s                                          p                                                     o
VARCHAR                                    VARCHAR                                               VARCHAR
_______________________________________________________________________________

urn:lsid:gdb.org:GenomicSegment:GDB132938  urn:lsid:gdb.org:DBObject-predicates:accessionID      GDB:132938
urn:lsid:gdb.org:GenomicSegment:GDB132938  http://www.ibm.com/LSID/2004/RDF/#lsidLink            urn:lsid:gdb.org:DBObject:GDB132938
urn:lsid:gdb.org:GenomicSegment:GDB132938  urn:lsid:gdb.org:DBObject-predicates:objectClass      DBObject
urn:lsid:gdb.org:GenomicSegment:GDB132938  urn:lsid:gdb.org:DBObject-predicates:displayName      D20S95
urn:lsid:gdb.org:GenomicSegment:GDB132938  urn:lsid:gdb.org:GenomicSegment-predicates:variantsQ  nodeID://1000027961

5 Rows. -- 822 msec.
]]></programlisting>
<para>Example of <emphasis>OAI</emphasis>: an institutional / departmental repository.</para>
<programlisting><![CDATA[
SQL>sparql
define get:soft "soft"
select *
from <oai:etheses.bham.ac.uk:23>
where { ?s ?p ?o }
limit 5;

s                           p                                           o
VARCHAR                     VARCHAR                                     VARCHAR
_____________________________________________________________________________

oai:etheses.bham.ac.uk:23   http://purl.org/dc/elements/1.1/title       A study of the role of ATM mutations in the pathogenesis of B-cell chronic lymphocytic leukaemia
oai:etheses.bham.ac.uk:23   http://purl.org/dc/elements/1.1/date        2007-07
oai:etheses.bham.ac.uk:23   http://purl.org/dc/elements/1.1/subject     RC0254 Neoplasms. Tumors. Oncology (including Cancer)
oai:etheses.bham.ac.uk:23   http://purl.org/dc/elements/1.1/identifier  Austen, Belinda (2007) A study of the role of ATM mutations in the pathogenesis of B-cell chronic lymphocytic leukaemia. Ph.D. thesis, University of Birmingham.
oai:etheses.bham.ac.uk:23   http://purl.org/dc/elements/1.1/identifier  http://etheses.bham.ac.uk/23/1/Austen07PhD.pdf

5 Rows. -- 461 msec.
]]></programlisting>
<para>Example of <emphasis>DOI</emphasis></para>
<para>In order to execute correctly queries with doi resolver you need to have:</para>
<itemizedlist>
<listitem>the handle.dll file accessible from your system. For ex. you can put it in the Virtuoso bin folder where the rest of the server components are.</listitem>
<listitem>in your Virtuoso database ini file in section Plugins added the hslookup.dll file, which location should be in the plugins folder under your Virtuoso server installation. For ex:
<programlisting><![CDATA[
[Plugins]
LoadPath = ./plugin
...
Load6    = plain,hslookup
]]></programlisting>
</listitem>
</itemizedlist>
<programlisting><![CDATA[
SQL>sparql
define get:soft "soft"
select *
from <doi:10.1045/march99-bunker>
where { ?s ?p ?o } ;

s                                                      p                                                 o
VARCHAR                                                VARCHAR                                           VARCHAR
_______________________________________________________________________________

http://www.dlib.org/dlib/march99/bunker/03bunker.html  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   http://www.openlinksw.com/schemas/XHTML#
http://www.dlib.org/dlib/march99/bunker/03bunker.html  http://www.openlinksw.com/schemas/XHTML#title     Collaboration as a Key to Digital Library Development: High Performance Image Management at the University of Washington

2 Rows. -- 12388 msec.
]]></programlisting>
<para>Other examples</para>
<programlisting><![CDATA[
SQL>sparql
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX doap: <http://usefulinc.com/ns/doap#>
SELECT DISTINCT ?name ?mbox ?projectName
WHERE {
 <http://dig.csail.mit.edu/2005/ajar/ajaw/data#Tabulator>
doap:developer ?dev .
 ?dev foaf:name ?name .
 OPTIONAL { ?dev foaf:mbox ?mbox }
 OPTIONAL { ?dev doap:project ?proj .
            ?proj foaf:name ?projectName }
};

name          mbox              projectName
VARCHAR       VARCHAR           VARCHAR
_______________________________________________________________________________

Adam Lerer    NULL              NULL
Dan Connolly  NULL              NULL
David Li      NULL              NULL
David Sheets  NULL              NULL
James Hollenbach  NULL          NULL
Joe Presbrey  NULL              NULL
Kenny Lu      NULL              NULL
Lydia Chilton NULL              NULL
Ruth Dhanaraj NULL              NULL
Sonia Nijhawan    NULL          NULL
Tim Berners-Lee   NULL          NULL
Timothy Berners-Lee   NULL      NULL
Yuhsin Joyce Chen         NULL NULL

13 Rows. -- 491 msec.
]]></programlisting>
<programlisting><![CDATA[
SQL>sparql
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
SELECT DISTINCT ?friendsname ?friendshomepage ?foafsname ?foafshomepage
WHERE
 {
  <http://myopenlink.net/dataspace/person/kidehen#this> foaf:knows ?friend .
  ?friend foaf:mbox_sha1sum ?mbox .
  ?friendsURI foaf:mbox_sha1sum ?mbox .
  ?friendsURI foaf:name ?friendsname .
  ?friendsURI foaf:homepage ?friendshomepage .
  OPTIONAL { ?friendsURI foaf:knows ?foaf .
              ?foaf foaf:name ?foafsname .
              ?foaf foaf:homepage ?foafshomepage .
           }
 };

friendsname   friendshomepage  foafsname   foafshomepage
VARCHAR       VARCHAR          VARCHAR VARCHAR
_______________________________________________________________________________

Tim Berners-Lee   http://www.w3.org/People/Berners-Lee/                 Dan Connolly    http://www.w3.org/People/Connolly/
Tim Berners-Lee   http://www.w3.org/People/Berners-Lee/                 Eric Miller     http://purl.org/net/eric/
Dave Beckett      http://www.dajobe.org/                                NULL            NULL
Richard Cyganiak  http://richard.cyganiak.de/                           Dan Connolly    http://www.w3.org/People/Connolly/

...
73 Rows. -- 1452 msec.
SQL>

]]></programlisting>
</sect2>
</sect1>
<sect1 id="rdfviews"><title>RDF Views -- Mapping Relational Data to RDF</title>
<para>
RDF Views map relational data into RDF and allow customizing RDF representation of locally stored RDF data.
To let SPARQL clients access relational data as well as physical RDF graphs in a single query, we introduce a declarative Meta Schema Language for mapping SQL Data to RDF Ontologies.
As a result, all types of clients can efficiently access all data stored on the server.
The mapping functionality dynamically generates RDF Data Sets for popular ontologies such as SIOC, SKOS, FOAF, and ATOM/OWL without disruption to the existing database infrastructure of Web 1.0 or Web 2.0 solutions.
RDF views are also suitable for declaring custom representation for RDF triples, e.g. property tables, where one row holds many single-valued properties.
</para>
<sect2 id="rdfviewsintro"><title>Introduction</title>
<para>
The Virtuoso RDF Views meta schema is a built-in feature of Virtuoso's SPARQL to SQL translator.
It recognizes triple patterns that refer to graphs for which an alternate representation is declared and translates these into SQL accordingly.
The main purpose of this is evaluating SPARQL queries against existing relational databases.
There exists previous work from many parties for rendering relational data as RDF and opening it to SPARQL access.
We can mention D2RQ, SPASQL, Squirrel RDF, DBLP and others.
The Virtuoso effort differs from these mainly in the following:
</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>Integration with a triple store.
Virtuoso can process a query for which some triple patterns will go to local or remote relational data and some to local physical RDF triples.
</listitem><listitem>SPARQL query can be used in any place where SQL can.
Database connectivity protocols are neutral to the syntax of queries they transmit, thus any SQL client, e.g. JDBC, ODBC or XMLA application, can send SPARQL queries and fetch result sets.
Moreover, a SQL query may contain SPARQL subqueries and SPARQL expressions may use SQL built-in functions and stored procedures.
</listitem><listitem>Integration with SQL.
Since SPARQL and SQL share the same run time and query optimizer, the query compilation decisions are always made with the best knowledge of the data and its location.
This is especially important when mixing triples and relational data or when dealing with relational data distributed across many outside databases.
</listitem><listitem>No limits on SPARQL.
It remains possible to make queries with unspecified graph or predicate against mapped relational data, even though these may sometimes be inefficient.
</listitem><listitem>Coverage of the whole relational model.
Multi-part keys etc. are supported in all places.
</listitem>
</itemizedlist>
</sect2>
<sect2 id="rdfviewrationale"><title>Rationale</title>
<para>
Since most of the data that is of likely use for the emerging semantic web is stored in relational databases, the argument for exposing this to SPARQL access is clear.
We note that historically, SQL access to relational data has essentially never been given to the public outside of the organization.
If programmatic access to corporate IS has been available to partners or the public, it has been through dynamic web pages or more recently web services.
There are reasons of performance, security, maintainability and so forth for this.
</para><para>
The culture of the emerging semantic web is however taking a different turn.
Since RDF and OWL offer a mergeable and queryable model for heterogeneous data, it is more meaningful and maintainable to expose selected data for outside query than it would be with SQL.
Advances in hardware make this also less of a performance issue than it would have been in the client-server database era.
</para><para>
In the context of Virtuoso, since Virtuoso is originally a virtual/federated database, incorporating SPARQL to relational mapping is an evident extension of the product's mission as a multi-protocol, multi-platform connector between information systems.
</para>
</sect2>
<sect2 id="rdfviewquadmapatternsvalueandiriclasses"><title>Quad Map Patterns, Value and IRI Classes</title>
<para>
In the simplest sense, any relational schema can be rendered into RDF by converting all primary keys and foreign keys into IRI's, assigning a predicate IRI to each column, and an rdf:type predicate for each row linking it to a RDF class IRI corresponding to the table.
Then a triple with the primary key IRI as subject, the column IRI as predicate and the column's value as object is considered to exist for each column that is neither part of a primary or foreign key.
</para><para>
Strictly equating a subject value to a row and each column to a predicate is often good but is too restrictive for the general case.
</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>Multiple triples with the same subject and predicate can exist.
</listitem><listitem>A single subject can get single-valued properties from multiple tables or in some cases stored procedures.
</listitem><listitem>An IRI value of a subject or other field of a triple can be composed from more than one SQL value, these values may reside in different columns, maybe in different joined tables.
</listitem><listitem>Some table rows should be excluded from mapping.
</listitem></itemizedlist>
<para>
Thus in the most common case the RDF meta schema should consist of independent transformations; the domain of each transformation is a result-set of some SQL <emphasis>SELECT</emphasis> statement and range is a set of triples.
The <emphasis>SELECT</emphasis> that produce the domain is quite simple: it does not use aggregate functions, joins and sorting, only inner joins and <emphasis>WHERE</emphasis> conditions.
There is no need to support outer joins in the RDF meta schema because NULLs are usually bad inputs for functions that produce IRIs.
In the rare cases when NULLs are OK for functions, outer joins can be encapsulated in SQL views.
The range of mapping can be described by a SPARQL triple pattern: a pattern field is a variable if it depends on table columns, otherwise it is a constant.
Values of variables in the pattern may have additional restrictions on datatypes, when datatypes of columns are known.
</para><para>
This common case of an RDF meta schema is implemented in Virtuoso, with one adjustment.
Virtuoso stores quads, not triples, using the graph field (G) to indicate that a triple belongs to some particular application or resource.
A SPARQL query may use quads from different graphs without large difference between G and the other three fields of a quad.
E.g., variable <emphasis>?g</emphasis> in expression <emphasis>GRAPH ?g {...}</emphasis> can be unbound.
SPARQL has special syntax for &quot;graph group patterns&quot; that is convenient for sets of triple patterns with a common graph, but it also has shorthands for common subject and predicate, so the difference is no more than in syntax.
There is only one feature that is specific for graphs but not for other fields: the SPARQL compiler can create restrictions on graphs according to <emphasis>FROM</emphasis> and <emphasis>FROM NAMED</emphasis> clauses.
</para><para>
Virtuoso RDF Views should offer the same flexibility with the graphs as SPARQL addressing physical triples.
A transformation cannot always be identified by the graph used for ranges because graph may be composed from SQL data. The key element of the meta schema is a &quot;<emphasis>quad map pattern</emphasis>&quot;.
A simple quad map pattern fully defines one particular transformation from one set of relational columns into triples that match one SPARQL graph pattern.
The main part of quad map pattern is four declarations of &quot;<emphasis>quad map values</emphasis>&quot;, each declaration specifies how to calculate the value of the corresponding triple field from the SQL data.
The pattern also lists boolean SQL expressions that should be used to filter out unwanted rows of source data (and to join multiple tables if source columns belong to different tables).
There are also quad map patterns that group together similar quad patterns but do not specify any real transformation or even prevent unwanted transformations from being used, they are described in &quot;Grouping Map Patterns&quot; below.
</para><para>
Quad map values refer to schema elements of two further types: &quot;IRI classes&quot; and &quot;literal classes&quot;.
</para>
<sect3 id="rdfviewiriclasses"><title>IRI Classes</title>
<para>
An IRI class declares that a column or set of columns gets converted into a IRI in a certain way.
The conversion of this sort can be declared revertible (bijection) so an IRI can be parsed into original SQL values; this is useful when some equality of an IRI constant and a calculated IRI can be replaced with an equality of a parse result of a constant and an SQL column that is index criteria or simply faster.
In addition, the SPARQL optimizer will eliminate redundant conversions if one IRI class is explicitly declared as a subclass of another.
The most flexible declaration for conversion consists of specifying functions that assemble and disassemble from IRI into its constituent parts.
This is overkill for typical conversions so it is possible to specify only one sprintf-style format string such that <emphasis>sprintf()</emphasis> SQL function will print an IRI using this format and <emphasis>sprintf_inverse()</emphasis> will be able to parse it back.
</para><para>The use of <emphasis>sprintf_inverse()</emphasis> assumes that the format does not contain fragments like <emphasis>'%s%s'</emphasis> that make it impossible to separate parts of IRI from each other.
</para><para>
In the following, we shall map the Virtuoso users and user roles system tables into the SIOC ontology.
</para>
<programlisting><![CDATA[
create iri class oplsioc:user_iri "http://myhost/sys/user?id=%d"
  (in uid integer not null) .
create iri class oplsioc:group_iri "http://myhost/sys/group?id=%d"
  (in gid integer not null) .
create iri class oplsioc:membership_iri
  "http://myhost/sys/membership?super=%d&sub=%d"
  (in super integer not null, in sub integer not null) .
create iri class oplsioc:dav_iri "http://myhost%s"
  (in path varchar) .
]]></programlisting>
<para>
These IRI classes are used for mapping data from the <emphasis>DB.DBA.SYS_USERS</emphasis> and <emphasis>DB.DBA.SYS_ROLE_GRANTS</emphasis> system tables that are defined in Virtuoso as follows:
</para>
<programlisting><![CDATA[
create table DB.DBA.SYS_USERS (
  U_ID                integer not null unique,
  U_NAME              char (128) not null primary key,
  U_IS_ROLE           integer default 0,
  U_FULL_NAME         char (128),
  U_E_MAIL            char (128) default &quot;,
  U_ACCOUNT_DISABLED  integer default 1,
  U_DAV_ENABLE        integer default 0,
  U_SQL_ENABLE        integer default 1,
  U_HOME              varchar (128),
. . .
 );
]]></programlisting>
<para>
Single record in <emphasis>DB.DBA.SYS_USERS</emphasis> corresponds to a plain user or a group (role).
Users and roles are collectively named &quot;grantees&quot;. Thus a role may be granted to another role or to a user account.
A role grant may be direct (explicit) or assigned by recursion.
</para>
<programlisting><![CDATA[
create table SYS_ROLE_GRANTS (
  GI_SUPER   integer,
  GI_SUB     integer,
  GI_DIRECT  integer default 1,
. . .
  primary key (GI_SUPER, GI_SUB, GI_DIRECT));
]]></programlisting>
<para>One IRI class usually corresponds to one ontology class, because similar things are usually called similarly.
One may wish to use identifiers of ontology classes as identifiers of related IRI classes, to not remember double number of names, e.g. <emphasis>create IRI class mybank:XpressXfer</emphasis> for subjects that will have <emphasis>rdf:type</emphasis> property <emphasis>mybank:XpressXfer</emphasis> made by mapping. That is technically possible but proven to become inconvenient and misleading as application evolves. While RDF types tend to persist, IRI classes may change over time or same subject may get more than one name via more than one IRI class, say, for exports to different systems. It is found to be more convenient to compose names of IRI classes by adding some common prefixes or suffixes to RDF classes (or to table names), say, write <emphasis>create IRI class mybank:XpressXfer_iri</emphasis>.</para>
</sect3>
<sect3 id="rdfviewliteralclasses"><title>Literal Classes</title>
<para>
A &quot;literal class&quot; declares that a column or set of columns gets converted into a literal instead of an IRI.
More precisely, the result of conversion can be <emphasis>IRI_ID</emphasis> so it represents an IRI, but in current version of Virtuoso this is supported only for some internal built-in literal classes, not for classes declared by the user.
So for user-defined literal class the result of the conversion is an RDF literal even if it is a string representation of a valid IRI.
</para><para>
In any case, a literal class can be used only in quad map values of O fields, because Virtuoso does not support literal values as subjects.
</para><para>
A special case of literal class is the identity class that converts a value from <emphasis>varchar</emphasis> column into an untyped literal and value from column of any other SQL datatype into a typed literal with type from XMLSchema set, i.e. <emphasis>xsd:integer</emphasis>, <emphasis>xsd:dateTime</emphasis> and so on.
Columns of types <emphasis>ANY</emphasis> and <emphasis>IRI_ID</emphasis> are not supported.
</para><para>
The SPARQL optimizer knows that RDF literal types are pairwise disjoint so literal classes that produce literals of different types are known to be pairwise disjoint.
The optimizer will replace a join on two disjoint literal classes with an empty statement, to simplify the resulting query.
</para>
</sect3>
<sect3 id="rdfviewsimplequadmappatterns"><title>Simple Quad Map Patterns</title>
<para>
The following declaration of quad map pattern is self-explanatory. The line for <emphasis>object</emphasis> uses identity literal class so there's no need to specify its name.
</para>
<programlisting><![CDATA[
graph      <http://myhost/sys>
subject    oplsioc:user_iri (DB.DBA.SYS_USERS.U_ID)
predicate  foaf:email
object     DB.DBA.SYS_USERS.U_E_MAIL
]]></programlisting>
<para>
The description language also supports SPARQL-style notation that contains less keywords and eliminates duplicate graphs, subjects and predicates.
The following add two patterns with constant graph IRI <emphasis>&lt;http://myhost/sys&gt;</emphasis> and subjects are made from column <emphasis>DB.DBA.SYS_USERS.U_ID</emphasis> by <emphasis>oplsioc:user_iri</emphasis>.
</para>
<programlisting><![CDATA[
graph <http://myhost/sys>
  {
    oplsioc:user_iri (DB.DBA.SYS_USERS.U_ID)
      a sioc:user ;
      oplsioc:name DB.DBA.SYS_USERS.U_FULL_NAME .
  }
]]></programlisting>
</sect3>
<sect3 id="rdfviewassigningnamestoquadmappatterns"><title>Assigning Names To Quad Map Patterns</title>
<para>
In real applications, quad map patterns should be named, for schema manipulation and keeping debug info readable.
Thus it is much better to rewrite the previous example as
</para>
<programlisting><![CDATA[
create virtrdf:SysUsers as graph <http://myhost/sys>
  {
    oplsioc:user_iri (DB.DBA.SYS_USERS.U_ID)
      a sioc:user
          as virtrdf:SysUserType-User;
      oplsioc:name DB.DBA.SYS_USERS.U_FULL_NAME
          as virtrdf:SysUsersFullName .
  }
]]></programlisting>
<para>
Using these names, one may later write, say, <emphasis>drop quad map virtrdf:SysUserType-User</emphasis>.
</para><para>
One name, <emphasis>virtrdf:DefaultQuadMap</emphasis> is reserved.
It is an internal quad map pattern used to access &quot;native-form&quot; quads from <emphasis>DB.DBA.RDF_QUAD</emphasis>:
</para>
<programlisting><![CDATA[
create virtrdf:DefaultQuadMap as
graph rdfdf:default-iid-nonblank (DB.DBA.RDF_QUAD.G)
subject rdfdf:default-iid (DB.DBA.RDF_QUAD.S)
predicate rdfdf:default-iid-nonblank (DB.DBA.RDF_QUAD.P)
object rdfdf:default (DB.DBA.RDF_QUAD.O)
]]></programlisting>
<para>
IRI classes from <emphasis>rdfdf:...</emphasis> namespace are also reserved.
</para>
</sect3>
<sect3 id="rdfviewgroupingmappatterns"><title>Grouping Map Patterns</title>
<para>
The previous example actually contains three map patterns, not two.
The name <emphasis>virtrdf:SysUsers</emphasis> refers to a &quot;<emphasis>group map pattern</emphasis>&quot; that does not define any real transformation of relational data into RDF but helps organize quad map patterns into a tree.
Group may contain both quad map patterns and other groups.
A group can be manipulated as a whole, e.g. <emphasis>drop quad map virtrdf:SysUsers</emphasis> will remove all three map patterns.
</para>
</sect3>
</sect2>
<sect2 id="rdfviewnorthwindexample1"><title>Simple Mapping Example -- Northwind RDF View</title>
<para>Here is example of the basic Northwind RDF Views deployment. The sequence of operations is very common for adding SPARQL access to existing application.</para>
<para>There exist few important questions to answer. Who should have access to data behind RDF View? Should someone have access to other sorts of RDF data but not to the new View? What are applications that should be interoperable with the new RDF data source? Are there any applications that produce similar data but that data sould be kept apart from data made by view? How to ensure that deployment the view will not cause problems for other applications?</para>
<para>First of all, we decide whether the default web service endpoint should have access to the data in question. If it should then we have to grant SELECT priviledges to the account "SPARQL" that is used for the default endpoint; if it should not but some custom edpoint should then grant to the owner account of that account. Granting access is less trivial that it is usual. On one hand, those who can make SQL SELECT statements on application's tables can also make SPARQL queries on RDF View over that tables, because it makes SQL inside. On the other hand, those who do not intend to query that data at all may get unexpected &quot;permission denied&quot; errors on queries that worked fine before adding an RDF View. If SPARQL compiler can not prove that the query can not access data from the view then it will generate SQL code that will access tables behind the view. In some cases permission problems should be resolved by creating RDF View in a separate <link linkend="rdfviewconfiguringrdfstorages">RDF storage</link>. In this example, data are public:</para>
<programlisting><![CDATA[
use DB;

GRANT SELECT ON "Demo"."demo"."Products" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Suppliers" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Shippers" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Categories" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Customers" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Employees" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Orders" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Order_Details" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Countries" TO "SPARQL";
GRANT SELECT ON "Demo"."demo"."Provinces" TO "SPARQL";
]]></programlisting>

<para>Interoperability is the next question. The example is not
interoperable with anything so in can provide data of any form, a real
application will probably use some ontology from external
source. Sometimes data should be converted from internal application's
representation to something different (such as metric to imperial or
ATT country code to two-character country id); sometimes composed IRIs
should follow special rules; <link
linkend="rdfviewiriusingfunction">function-based IRI classes</link>
may help in that cases. As this is the first example, only plain
format-string-based IRI classes are used.</para>

<para>We should also ensure that data generated by the new view will
not be accidentally mixed with other data of the database. For that
purpose the example will use a unique graph name that includes both
application name and host name. In addition, the script will drop
declarations that might remain  from a previous run of the same script. The
script is executed many times during the development so erasing
previous version is worth writing. It will report an error if there's
nothing to erase but it's better than unpredictable errors due to
writing new declarations over existing ones.</para>


<note><para>Making graph name unique for every host is not needed if
the application is supposed to be &quot;local&quot; and nobody will
access more than one installation of the application. If this is
the case, use some fixed graph IRI, not necessarily starting with
hostname at all; this is much more convenient for querying because you
don't have to calculate the graph name in each query. With fixed graph
in use, it is still possible to clone the RDF View to map to a unique
graph as soon as the application become &quot;public&quot; and requires
merging data from many installations.</para></note>

<programlisting><![CDATA[
SPARQL drop quad map graph iri("http://^{URIQADefaultHost}^/Northwind")
;
SPARQL drop quad map virtrdf:NorthwindDemo
;
]]></programlisting>
<note><para>The <emphasis>^{URIQADefaultHost}^</emphasis> macro is replaced with the value of <emphasis>DefaultHost</emphasis> parameter of <link linkend="ini_URIQA">[URIQA]</link> section of configuration file. The IRI is written as <emphasis>iri("http://^{URIQADefaultHost}^/Northwind")</emphasis>, not as <emphasis>&lt;http://^{URIQADefaultHost}^/Northwind&gt;</emphasis> because macro of this sort works only inside SPARQL string values.</para></note>

<para>Now it's safe to create IRI classes needed for the view. If
these classes are used only in the view we define then it is safe to
create all of them in a single statement. If some of them are used
across multiple declarations then errors may occur. it is impossible
to redefine an IRI class that is in use; the compiler will try to
avoid reporting errors by checking whether the new declaration is
identical to the existing one and by trying garbage collection in hope
that the IRI class is used only in garbage, but errors may occur
anyway. Thus it is better to declare &quot;shared&quot; IRI classes by
individual statements and group together only &quot;private&quot; IRI
classes of a view. If a &quot;class redefinition&quot; error occurs in
the middle of a group then &quot;undefined class&quot; errors may
occur after because the processing of the group was interrupted before
rest of group was not executed. When in trouble, try <link
linkend="fn_rdf_audit_metadata"><function>DB.DBA.RDF_AUDIT_METADATA</function></link>
procedure.</para>


<programlisting><![CDATA[
SPARQL
create iri class northwind:Category "http://^{URIQADefaultHost}^/Northwind/Category/%d#this" (in category_id integer not null) .
create iri class northwind:Shipper "http://^{URIQADefaultHost}^/Northwind/Shipper/%d#this" (in shipper_id integer not null) .
create iri class northwind:Supplier "http://^{URIQADefaultHost}^/Northwind/Supplier/%d#this" (in supplier_id integer not null) .
create iri class northwind:Product   "http://^{URIQADefaultHost}^/Northwind/Product/%d#this" (in product_id integer not null) .
create iri class northwind:Customer "http://^{URIQADefaultHost}^/Northwind/Customer/%U#this" (in customer_id varchar not null) .
create iri class northwind:Employee "http://^{URIQADefaultHost}^/Northwind/Employee/%U%U%d#this" (in employee_firstname varchar not null, in employee_lastname varchar not null, in employee_id integer not null) .
create iri class northwind:Order "http://^{URIQADefaultHost}^/Northwind/Order/%d#this" (in order_id integer not null) .
create iri class northwind:CustomerContact "http://^{URIQADefaultHost}^/Northwind/CustomerContact/%U#this" (in customer_id varchar not null) .
create iri class northwind:OrderLine "http://^{URIQADefaultHost}^/Northwind/OrderLine/%d/%d#this" (in order_id integer not null, in product_id integer not null) .
create iri class northwind:Province "http://^{URIQADefaultHost}^/Northwind/Province/%U/%U#this" (in country_name varchar not null, in province_name varchar not null) .
create iri class northwind:Country "http://^{URIQADefaultHost}^/Northwind/Country/%U#this" (in country_name varchar not null) .
create iri class northwind:Flag "http://^{URIQADefaultHost}^%U#this" (in flag_path varchar not null) .
create iri class northwind:dbpedia_iri "http://dbpedia.org/resource/%U" (in uname varchar not null) .
create iri class northwind:EmployeePhoto "http://^{URIQADefaultHost}^/DAV/VAD/demo/sql/EMP%d#this" (in emp_id varchar not null) .
create iri class northwind:CategoryPhoto "http://^{URIQADefaultHost}^/DAV/VAD/demo/sql/CAT%d#this" (in category_id varchar not null) .
;
]]></programlisting>
<para>One IRI class per subject type; format strings begin with same host but different directory names so this will let the compiler to guess the type of subject by the text of IRI. Most of declarations are <link linkend="rdfviewbijandreturns">bijections</link> and may get  <emphasis>option (bijection)</emphasis> hint but these format strings are so simple that the compiler may understant it by itself.
(<emphasis>northwind:Employee</emphasis> is not a bijection because <link linkend="fn_sprintf_inverse"><function>sprintf_inverse</function></link> will be unable to split the tail of IRI string and find the boundary between first and last name.)</para>
<para>The final operation is extending the default quad storage with new tree of quad map patterns.</para>
<programlisting><![CDATA[
SPARQL
prefix northwind: <http://demo.openlinksw.com/schemas/northwind#>
prefix oplsioc: <http://www.openlinksw.com/schemas/oplsioc#>
prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>
prefix sioc: <http://rdfs.org/sioc/ns#>
prefix foaf: <http://xmlns.com/foaf/0.1/>
prefix owl: <http://www.w3.org/2002/07/owl#>
prefix wgs: <http://www.w3.org/2003/01/geo/wgs84_pos#>

alter quad storage virtrdf:DefaultQuadStorage
from Demo.demo.Products as products
from Demo.demo.Suppliers as suppliers
from Demo.demo.Shippers as shippers
from Demo.demo.Categories as categories
from Demo.demo.Customers as customers
from Demo.demo.Employees as employees
from Demo.demo.Orders as orders
from Demo.demo.Order_Details as order_lines
from Demo.demo.Countries as countries
from Demo.demo.Provinces as provinces
where (^{suppliers.}^.Country = ^{countries.}^.Name)
where (^{customers.}^.Country = ^{countries.}^.Name)
where (^{employees.}^.Country = ^{countries.}^.Name)
where (^{orders.}^.ShipCountry = ^{countries.}^.Name)
{
        create virtrdf:NorthwindDemo as graph iri ("http://^{URIQADefaultHost}^/Northwind") option (exclusive)
        {
                northwind:CustomerContact (customers.CustomerID)
                        a foaf:Person
                                as virtrdf:CustomerContact-foaf_Person .

                northwind:CustomerContact (customers.CustomerID)
                        a northwind:CustomerContact
                                as virtrdf:CustomerContact-CustomerContact;
                        foaf:name customers.ContactName
                                as virtrdf:CustomerContact-contact_name ;
                        foaf:phone customers.Phone
                                as virtrdf:CustomerContact-foaf_phone ;
                        northwind:is_contact_at northwind:Customer (customers.CustomerID)
                                as virtrdf:CustomerContact-is_contact_at ;
                        northwind:country northwind:Country (customers.Country)
                                as virtrdf:CustomerContact-country ;
                        rdfs:isDefinedBy northwind:customercontact_iri (customers.CustomerID) ;
                        rdfs:isDefinedBy northwind:CustomerContact (customers.CustomerID) .

                northwind:Country (customers.Country)
                        northwind:is_country_of
                northwind:CustomerContact (customers.CustomerID) as virtrdf:CustomerContact-is_country_of .

                northwind:Product (products.ProductID)
                        a northwind:Product
                                as virtrdf:Product-ProductID ;
                        northwind:has_category northwind:Category (products.CategoryID)
                                as virtrdf:Product-product_has_category ;
                        northwind:has_supplier northwind:Supplier (products.SupplierID)
                                as virtrdf:Product-product_has_supplier ;
                        northwind:productName products.ProductName
                                as virtrdf:Product-name_of_product ;
                        northwind:quantityPerUnit products.QuantityPerUnit
                                as virtrdf:Product-quantity_per_unit ;
                        northwind:unitPrice products.UnitPrice
                                as virtrdf:Product-unit_price ;
                        northwind:unitsInStock products.UnitsInStock
                                as virtrdf:Product-units_in_stock ;
                        northwind:unitsOnOrder products.UnitsOnOrder
                                as virtrdf:Product-units_on_order ;
                        northwind:reorderLevel products.ReorderLevel
                                as virtrdf:Product-reorder_level ;
                        northwind:discontinued products.Discontinued
                                as virtrdf:Product-discontinued ;
                        rdfs:isDefinedBy northwind:product_iri (products.ProductID) ;
                        rdfs:isDefinedBy northwind:Product (products.ProductID) .

                northwind:Category (products.CategoryID)
                        northwind:category_of northwind:Product (products.ProductID) as virtrdf:Product-category_of .

                northwind:Supplier (products.SupplierID)
                        northwind:supplier_of northwind:Product (products.ProductID) as virtrdf:Product-supplier_of .

                northwind:Supplier (suppliers.SupplierID)
                        a northwind:Supplier
                                as virtrdf:Supplier-SupplierID ;
                        northwind:companyName suppliers.CompanyName
                                as virtrdf:Supplier-company_name ;
                        northwind:contactName suppliers.ContactName
                                as virtrdf:Supplier-contact_name ;
                        northwind:contactTitle suppliers.ContactTitle
                                as virtrdf:Supplier-contact_title ;
                        northwind:address suppliers.Address
                                as virtrdf:Supplier-address ;
                        northwind:city suppliers.City
                                as virtrdf:Supplier-city ;
                        northwind:dbpedia_city northwind:dbpedia_iri(suppliers.City)
                                as virtrdf:Supplier-dbpediacity ;
                        northwind:region suppliers.Region
                                as virtrdf:Supplier-region ;
                        northwind:postalCode suppliers.PostalCode
                                as virtrdf:Supplier-postal_code ;
                        northwind:country northwind:Country(suppliers.Country)
                                as virtrdf:Supplier-country ;
                        northwind:phone suppliers.Phone
                                as virtrdf:Supplier-phone ;
                        northwind:fax suppliers.Fax
                                as virtrdf:Supplier-fax ;
                        northwind:homePage suppliers.HomePage
                                as virtrdf:Supplier-home_page ;
                        rdfs:isDefinedBy northwind:supplier_iri (suppliers.SupplierID) ;
                        rdfs:isDefinedBy northwind:Supplier (suppliers.SupplierID) .

                northwind:Country (suppliers.Country)
                        northwind:is_country_of
                northwind:Supplier (suppliers.SupplierID) as virtrdf:Supplier-is_country_of .

                northwind:Category (categories.CategoryID)
                        a northwind:Category
                                as virtrdf:Category-CategoryID ;
                        northwind:categoryName categories.CategoryName
                                as virtrdf:Category-home_page ;
                        northwind:description categories.Description
                                as virtrdf:Category-description ;
                        foaf:img northwind:CategoryPhoto(categories.CategoryID)
                                as virtrdf:Category-categories.CategoryPhoto ;
                        rdfs:isDefinedBy northwind:category_iri (categories.CategoryID) ;
                        rdfs:isDefinedBy northwind:Category (categories.CategoryID) .

                northwind:CategoryPhoto(categories.CategoryID)
                        a northwind:CategoryPhoto
                                as virtrdf:Category-categories.CategoryPhotoID ;
                        rdfs:isDefinedBy northwind:categoryphoto_iri (categories.CategoryID) ;
                        rdfs:isDefinedBy northwind:CategoryPhoto(categories.CategoryID) .

                northwind:Shipper (shippers.ShipperID)
                        a northwind:Shipper
                                as virtrdf:Shipper-ShipperID ;
                        northwind:companyName shippers.CompanyName
                                as virtrdf:Shipper-company_name ;
                        northwind:phone shippers.Phone
                                as virtrdf:Shipper-phone ;
                        rdfs:isDefinedBy northwind:shipper_iri (shippers.ShipperID) ;
                        rdfs:isDefinedBy northwind:Shipper (shippers.ShipperID) .

                northwind:Customer (customers.CustomerID)
                        a  northwind:Customer
                                as virtrdf:Customer-CustomerID2 ;
                        a  foaf:Organization
                                as virtrdf:Customer-CustomerID ;
                        foaf:name customers.CompanyName
                                as virtrdf:Customer-foaf_name ;
                        northwind:companyName customers.CompanyName
                                as virtrdf:Customer-company_name ;
                        northwind:has_contact northwind:CustomerContact (customers.CustomerID)
                                as virtrdf:Customer-contact ;
                        northwind:country northwind:Country (customers.Country)
                                as virtrdf:Customer-country ;
                        northwind:contactName customers.ContactName
                                as virtrdf:Customer-contact_name ;
                        northwind:contactTitle customers.ContactTitle
                                as virtrdf:Customer-contact_title ;
                        northwind:address customers.Address
                                as virtrdf:Customer-address ;
                        northwind:city customers.City
                                as virtrdf:Customer-city ;
                        northwind:dbpedia_city northwind:dbpedia_iri(customers.City)
                                as virtrdf:Customer-dbpediacity ;
                        northwind:region customers.Region
                                as virtrdf:Customer-region ;
                        northwind:PostalCode customers.PostalCode
                                as virtrdf:Customer-postal_code ;
                        foaf:phone customers.Phone
                                as virtrdf:Customer-foaf_phone ;
                        northwind:phone customers.Phone
                                as virtrdf:Customer-phone ;
                        northwind:fax customers.Fax
                                as virtrdf:Customer-fax ;
                        rdfs:isDefinedBy northwind:customer_iri (customers.CustomerID) ;
                        rdfs:isDefinedBy northwind:Customer (customers.CustomerID) .

                northwind:Country (customers.Country)
                        northwind:is_country_of
                northwind:Customer (customers.CustomerID) as virtrdf:Customer-is_country_of .

                northwind:Employee (employees.FirstName, employees.LastName, employees.EmployeeID)
                        a northwind:Employee
                                as virtrdf:Employee-EmployeeID2 ;
                        a foaf:Person
                                as virtrdf:Employee-EmployeeID ;
                        foaf:surname employees.LastName
                                as virtrdf:Employee-foaf_last_name ;
                        northwind:lastName employees.LastName
                                as virtrdf:Employee-last_name ;
                        foaf:firstName employees.FirstName
                                as virtrdf:Employee-foaf_first_name ;
                        northwind:firstName employees.FirstName
                                as virtrdf:Employee-first_name ;
                        foaf:title employees.Title
                                as virtrdf:Employee-title ;
                        northwind:titleOfCourtesy employees.TitleOfCourtesy
                                as virtrdf:Employee-title_of_courtesy ;
                        foaf:birthday employees.BirthDate
                                as virtrdf:Employee-foaf_birth_date ;
                        northwind:birthday employees.BirthDate
                                as virtrdf:Employee-birth_date ;
                        northwind:hireDate employees.HireDate
                                as virtrdf:Employee-hire_date ;
                        northwind:address employees.Address
                                as virtrdf:Employee-address ;
                        northwind:city employees.City
                                as virtrdf:Employee-city ;
                        northwind:dbpedia_city northwind:dbpedia_iri(employees.City)
                                as virtrdf:Employee-dbpediacity ;
                        northwind:region employees.Region
                                as virtrdf:Employee-region ;
                        northwind:postalCode employees.PostalCode
                                as virtrdf:Employee-postal_code ;
                        northwind:country northwind:Country(employees.Country)
                                as virtrdf:Employee-country ;
                        foaf:phone employees.HomePhone
                                as virtrdf:Employee-home_phone ;
                        northwind:extension employees.Extension
                                as virtrdf:Employee-extension ;
                        northwind:notes employees.Notes
                                as virtrdf:Employee-notes ;
                        northwind:reportsTo northwind:Employee(employees.FirstName, employees.LastName, employees.ReportsTo) where (^{employees.}^.ReportsTo = ^{employees.}^.EmployeeID)
                                as virtrdf:Employee-reports_to ;
                        foaf:img northwind:EmployeePhoto(employees.EmployeeID)
                                as virtrdf:Employee-employees.EmployeePhoto ;
                        rdfs:isDefinedBy northwind:employee_iri (employees.EmployeeID) ;
                        rdfs:isDefinedBy northwind:Employee (employees.FirstName, employees.LastName, employees.EmployeeID) .

                northwind:EmployeePhoto(employees.EmployeeID)
                        a northwind:EmployeePhoto
                                as virtrdf:Employee-employees.EmployeePhotoId ;
                        rdfs:isDefinedBy northwind:employeephoto_iri (employees.EmployeeID) ;
                        rdfs:isDefinedBy northwind:EmployeePhoto (employees.EmployeeID) .

                northwind:Employee (employees.FirstName, employees.LastName, orders.EmployeeID)
                        northwind:is_salesrep_of
                northwind:Order (orders.OrderID) where (^{orders.}^.EmployeeID = ^{employees.}^.EmployeeID) as virtrdf:Order-is_salesrep_of .

                northwind:Country (employees.Country)
                        northwind:is_country_of
                northwind:Employee (employees.FirstName, employees.LastName, employees.EmployeeID) as virtrdf:Employee-is_country_of .

                northwind:Order (orders.OrderID)
                        a northwind:Order
                                as virtrdf:Order-Order ;
                        northwind:has_customer northwind:Customer (orders.CustomerID)
                                as virtrdf:Order-order_has_customer ;
                        northwind:has_salesrep northwind:Employee (employees.FirstName, employees.LastName, orders.EmployeeID) where (^{orders.}^.EmployeeID = ^{employees.}^.EmployeeID)
                                as virtrdf:Customer-has_salesrep ;
                        northwind:has_employee northwind:Employee (employees.FirstName, employees.LastName, orders.EmployeeID) where (^{orders.}^.EmployeeID = ^{employees.}^.EmployeeID)
                                as virtrdf:Order-order_has_employee ;
                        northwind:orderDate orders.OrderDate
                                as virtrdf:Order-order_date ;
                        northwind:requiredDate orders.RequiredDate
                                as virtrdf:Order-required_date ;
                        northwind:shippedDate orders.ShippedDate
                                as virtrdf:Order-shipped_date ;
                        northwind:order_ship_via northwind:Shipper (orders.ShipVia)
                                as virtrdf:Order-order_ship_via ;
                        northwind:freight orders.Freight
                                as virtrdf:Order-freight ;
                        northwind:shipName orders.ShipName
                                as virtrdf:Order-ship_name ;
                        northwind:shipAddress orders.ShipAddress
                                as virtrdf:Order-ship_address ;
                        northwind:shipCity orders.ShipCity
                                as virtrdf:Order-ship_city ;
                        northwind:dbpedia_shipCity northwind:dbpedia_iri(orders.ShipCity)
                                as virtrdf:Order-dbpediaship_city ;
                        northwind:shipRegion orders.ShipRegion
                                as virtrdf:Order-ship_region ;
                        northwind:shipPostal_code orders.ShipPostalCode
                                as virtrdf:Order-ship_postal_code ;
                        northwind:shipCountry northwind:Country(orders.ShipCountry)
                                as virtrdf:ship_country ;
                        rdfs:isDefinedBy northwind:order_iri (orders.OrderID) ;
                        rdfs:isDefinedBy northwind:Order (orders.OrderID) .

                northwind:Country (orders.ShipCountry)
                        northwind:is_ship_country_of
                northwind:Order (orders.OrderID) as virtrdf:Order-is_country_of .

                northwind:Customer (orders.CustomerID)
                        northwind:has_order northwind:Order (orders.OrderID) as virtrdf:Order-has_order .

                northwind:Shipper (orders.ShipVia)
                        northwind:ship_order northwind:Order (orders.OrderID) as virtrdf:Order-ship_order .

                northwind:OrderLine (order_lines.OrderID, order_lines.ProductID)
                        a northwind:OrderLine
                                as virtrdf:OrderLine-OrderLines ;
                        northwind:has_order_id northwind:Order (order_lines.OrderID)
                                as virtrdf:order_lines_has_order_id ;
                        northwind:has_product_id northwind:Product (order_lines.ProductID)
                                as virtrdf:order_lines_has_product_id ;
                        northwind:unitPrice order_lines.UnitPrice
                                as virtrdf:OrderLine-unit_price ;
                        northwind:quantity order_lines.Quantity
                                as virtrdf:OrderLine-quantity ;
                        northwind:discount order_lines.Discount
                                as virtrdf:OrderLine-discount ;
                        rdfs:isDefinedBy northwind:orderline_iri (order_lines.OrderID, order_lines.ProductID) ;
                        rdfs:isDefinedBy northwind:OrderLine (order_lines.OrderID, order_lines.ProductID) .

                northwind:Order (orders.OrderID)
                        northwind:is_order_of
                northwind:OrderLine (order_lines.OrderID, order_lines.ProductID) where (^{orders.}^.OrderID = ^{order_lines.}^.OrderID) as virtrdf:Order-is_order_of .

                northwind:Product (products.ProductID)
                        northwind:is_product_of
                northwind:OrderLine (order_lines.OrderID, order_lines.ProductID) where (^{products.}^.ProductID = ^{order_lines.}^.ProductID) as virtrdf:Product-is_product_of .

                northwind:Country (countries.Name)
                        a northwind:Country
                                as virtrdf:Country-Type2 ;
                        a wgs:SpatialThing
                                as virtrdf:Country-Type ;
                        owl:sameAs northwind:dbpedia_iri (countries.Name) ;
                        northwind:name countries.Name
                                as virtrdf:Country-Name ;
                        northwind:code countries.Code
                                as virtrdf:Country-Code ;
                        northwind:smallFlagDAVResourceName countries.SmallFlagDAVResourceName
                                as virtrdf:Country-SmallFlagDAVResourceName ;
                        northwind:largeFlagDAVResourceName countries.LargeFlagDAVResourceName
                                as virtrdf:Country-LargeFlagDAVResourceName ;
                        northwind:smallFlagDAVResourceURI northwind:Flag(countries.SmallFlagDAVResourceURI)
                                as virtrdf:Country-SmallFlagDAVResourceURI ;
                        northwind:largeFlagDAVResourceURI northwind:Flag(countries.LargeFlagDAVResourceURI)
                                as virtrdf:Country-LargeFlagDAVResourceURI ;
                        wgs:lat countries.Lat
                                as virtrdf:Country-Lat ;
                        wgs:long countries.Lng
                                as virtrdf:Country-Lng ;
                        rdfs:isDefinedBy northwind:country_iri (countries.Name) ;
                        rdfs:isDefinedBy northwind:Country (countries.Name) .

                northwind:Country (countries.Name)
                        northwind:has_province
                northwind:Province (provinces.CountryCode, provinces.Province) where (^{provinces.}^.CountryCode = ^{countries.}^.Code) as virtrdf:Country-has_province .

                northwind:Province (provinces.CountryCode, provinces.Province)
                        a northwind:Province
                                as virtrdf:Province-Provinces ;
                        northwind:has_country_code provinces.CountryCode
                                as virtrdf:has_country_code ;
                        northwind:provinceName provinces.Province
                                as virtrdf:Province-ProvinceName ;
                        rdfs:isDefinedBy northwind:province_iri (provinces.CountryCode, provinces.Province) ;
                        rdfs:isDefinedBy northwind:Province (provinces.CountryCode, provinces.Province) .

                northwind:Province (provinces.CountryCode, provinces.Province)
                        northwind:is_province_of
                northwind:Country (countries.Name) where  (^{countries.}^.Code = ^{provinces.}^.CountryCode) as virtrdf:Province-country_of .
        }.
}.
;
]]></programlisting>
<para>The created RDF View is sufficient for querying relational data via SPARQL but not for accessing data by dereferencing IRIs of subjects. Making IRIs dereferenceable requires configuring HTTP server; that is explained in <link linkend="rdfsparqlexnpointnorthwindexample">second part of the example</link>.</para>
</sect2>
<sect2 id="rdfviewconfiguringrdfstorages"><title>Configuring RDF Storages</title>
<para>
&quot;<emphasis>Quad Storage</emphasis>&quot; is a named set of quad map patterns.
The declaration <emphasis>define input:storage storage-name</emphasis> states that a SPARQL query will be executed using only quad patterns of the given quad storage.
Declarations of IRI classes, literal classes and quad patterns are shared between all quad storages of an RDF meta schema but every quad storage contains only a subset of all available quad patterns.
Two quad storages are always defined:
</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>A <emphasis>virtrdf:default</emphasis> one usually consists of everything (all user-relational mappings plus <emphasis>virtrdf:DefaultQuadMap</emphasis> for &quot;native-form&quot; quads from <emphasis>DB.DBA.RDF_QUAD</emphasis>)
</listitem><listitem>A <emphasis>virtrdf:empty</emphasis> storage refers solely to <emphasis>DB.DBA.RDF_QUAD</emphasis> and can not be altered.
</listitem></itemizedlist>
<para>
Three statements for manipulating storages are
</para>
<itemizedlist mark="number" spacing="compact">
<listitem><emphasis>create quad storage storage-name { quad-map-decls } .</emphasis>
</listitem><listitem><emphasis>alter quad storage storage-name { quad-map-decls-or-drops } .</emphasis>
</listitem><listitem><emphasis>drop quad storage storage-name . </emphasis>
</listitem></itemizedlist>
<para>
A map pattern can be created only as a part of <emphasis>create quad storage</emphasis> or <emphasis>alter quad storage</emphasis> statement, so initially it is used by exactly one storage.
It can be imported to some other storage using directive <emphasis>create map-id using storage source-storage</emphasis>. E.g., declarations of many storages create <emphasis>virtrdf:DefaultQuadMap</emphasis> using storage <emphasis>virtrdf:DefaultQuadStorage</emphasis>.
</para><para>
Only a &quot;top-level&quot; quad map pattern (standalone or a whole group with descendants) can be imported, member of a group can not.
The import directive also can not be a part of some group declaration.
</para><para>
The directive <emphasis>drop quad map map-name</emphasis> removes a map from one storage when it appears inside <emphasis>alter quad storage</emphasis> statement.
Otherwise it removes the map from all storages.
There exists garbage collection for quad map patterns, so any unused map is immediately deleted.
A group is deleted with all its descendants.
</para>
</sect2>
<sect2 id="rdfviewtranslationofpatterns"><title>Translation Of SPARQL Triple Patterns To Quad Map Patterns</title>
<para>
When a SPARQL query is compiled into SQL using a quad storage, every triple pattern should become a subquery that retrieves data from relational tables.
This subquery is an <emphasis>UNION ALL</emphasis> of joins generated from appropriate quad map patterns.
The complete SQL query is composed from these basic subqueries.
Thus the first operation of the SQL generation for a triple pattern is searching for quad map patterns that may in principle produce triples that match the triple pattern.
</para><para>
The more restrictions contained in the triple pattern the fewer quad map patterns will be used.
A triple pattern <emphasis>graph ?g { ?s ?p ?o }</emphasis> is common enough to invoke all data transformations of the storage.
A triple pattern <emphasis>graph &lt;g&gt; { ?s &lt;p&gt; &lt;o&gt; }</emphasis> will usually intersect with the range of only one quad map.
Sometimes it is possible to prove that the storage can not contain any data that matches the given triple pattern, hence zero number of members of <emphasis>UNION ALL</emphasis> will result in constantly empty result-set.
</para>
<para>The search for quad maps for a given pair of triple pattern and quad map storage is quite simple.
The storage is treated as a tree of map patterns where quad map patterns are leafs, grouping patterns are inner nodes and the whole storage is also treated as a grouping pattern that specify no fields and contains all top-level map patterns of the storage.
</para>
<para>
The tree is traversed from the root, left to right, non-leaf vertex are checked before their children.
The check of a vertex consists of up to four field checks, for G, S, P and O.
Every field check compares the field definition in the vertex and the corresponding field in the triple pattern, G and G, S and S and so on.
Note that a non-leaf vertex defines less than four of its fields, e.g., the root vertex does not define any of its fields and top-level <emphasis>graph map { ... }</emphasis> defines only graph.
Checks are performed only for defined fields and return one of three values: &quot;failed&quot;, &quot;passed&quot;, &quot;full match&quot;, according to the following rules:
</para>
<table><title>Matching Triple Field and Vertex Field</title>
<tgroup cols="3">
<thead><row>
<entry>Field of vertex</entry><entry>Field in triple pattern</entry><entry>Result</entry>
</row></thead>
<tbody>
<row><entry>constant</entry><entry>same constant</entry><entry>full match</entry></row>
<row><entry>constant</entry><entry>different constant</entry><entry>failed</entry></row>
<row><entry>constant</entry><entry>variable of same type</entry><entry>passed</entry></row>
<row><entry>constant</entry><entry>variable of different type</entry><entry>failed</entry></row>
<row><entry>quad map value</entry><entry>constant of same type</entry><entry>full match</entry></row>
<row><entry>quad map value</entry><entry>constant of different type</entry><entry>failed</entry></row>
<row><entry>quad map value of type X</entry><entry>variable, X or subtype of X</entry><entry>full match</entry></row>
<row><entry>quad map value of type X</entry><entry>variable, supertype of X</entry><entry>passed</entry></row>
<row><entry>quad map value of type X</entry><entry>variable, type does not intersect with X</entry><entry>failed</entry></row>
</tbody>
</tgroup>
</table>
<para>
If any of the checks fails, the vertex and all its children are excluded from the rest of processing.
Otherwise, if all four fields are defined for the quad map pattern, the map is added to the list of matching map patterns.
The difference between &quot;passed&quot; and &quot;full match&quot; is significant only if the map is declared with <emphasis>option (exclusive)</emphasis>
If all performed checks return &quot;full match&quot; and <emphasis>option (exclusive)</emphasis> is set then the traverse of the tree is stopped as soon as all children of the vertex are traversed.
The most typical use of this option is when the application developer is sure that all triples of a graph belong to his application and they come from his own quad map patterns, not from <emphasis>DB.DBA.RDF_QUAD</emphasis>.
This is to prevent the SPARQL compiler from generating redundant subqueries accessing <emphasis>DB.DBA.RDF_QUAD</emphasis>.
The declaration may look like
</para>
<programlisting><![CDATA[
create quad storage <mystorage>
  {
    graph <mygraph> option (exclusive) { . . . }
    create virtrdf:DefaultQuadMap
      using storage virtrdf:DefaultQuadStorage .
  }
]]></programlisting>
<para>
Exclusive patterns make the order of declarations important, because an exclusive declaration may &quot;throw a shadow&quot; on declarations after it.
Consider a database that have a special table RDF_TYPE that caches all RDF types of all subjects in all graphs.
Consider two declarations: all triples from graph <emphasis>&lt;http://myhost/sys&gt;</emphasis> and all triples with <emphasis>rdf:type</emphasis> predicate, both exclusive:
</para>
<programlisting><![CDATA[
graph <http://myhost/sys> option (exclusive)
  {
    . . . # mapping of DB.DBA.SYS_USERS as in previous examples.
  }
graph rdfdf:default-iid-nonblank (DB.DBA.RDF_TYPE.G)
subject rdfdf:default-iid (DB.DBA.RDF_TYPE.S)
predicate rdf:type
object rdfdf:default (DB.DBA.RDF_TYPE.O)
option (exclusive)
]]></programlisting>
<para>
The order of these declarations dictates that triple pattern
</para>
<programlisting><![CDATA[
graph <http://myhost/sys> {?s rdf:type ?o}
]]></programlisting>
<para>
is compiled using only quad map patterns of the graph declaration, ignoring second declaration (and of course ignoring default mapping rule, if any).
An explicit <emphasis>option (order N)</emphasis> at the end of quad map pattern will tweak the priority.
By default, order will grow from 1000 for the first declaration in the statement to 1999 for the last, explicit configuration is especially useful to make order persistent to <emphasis>alter storage</emphasis> statements.
</para>
<para>
The <emphasis>option (exclusive)</emphasis> trick is ugly, low-level and prone to cause compilation errors after altering storage declarations.
When misused, it is as bad as &quot;red cut&quot; in PROLOG, but one must use this trick to build scalable storages.
</para>
<para>The <emphasis>option (exclusive)</emphasis> helps the SPARQL compiler to prepare better SQL queries, but sometimes it is &quot;too exclusive&quot;. For instance, if a grouping quad map pattern specify only quad map value for graph and no other fields then making it exclusive prohibits the use of all declarations of the storage after that one. Sometimes it is better to notify compiler that quads made by the given quad map pattern are supposed to be different from all quads made by declarations listed after the given one.</para>

<para>Consider an application that exports users&apos; personal data
as graphs whose IRIs looks like
<emphasis>http://www.example.com/DAV/home/</emphasis>username<emphasis>/RDF/personal/</emphasis>;
the application makes a query and a triple pattern is proven to be
restrictive enough to filter out all quads that are not similar to
quads generated by the given quad map pattern (say, the graph is
constant
<emphasis>http://www.example.com/DAV/home/JohnSmith/RDF/personal/</emphasis>). The
application do not hope to find any quads that match the pattern but
made by other applications, because graphs named like in the pattern
are supposed to be solely for this single purpose; if, say,
DB.DBA.RDF_QUAD occasionally contains some quads with graph equal to
<emphasis>http://www.example.com/DAV/home/JohnSmith/RDF/personal/</emphasis>
then they can be ignored.</para>

<para>Under this circumstances, the quad map pattern may have <emphasis>option (soft exclusive)</emphasis>. That grants a permission to the compiler to ignore rest of storage as soon as it is proven that the triple pattern can not access quads that does not match the pattern. So if that is proven then the pattern is exclusive and it makes the query faster; when unsure, the complier work like there is no option at all.</para>


<note><para>The <emphasis>option (exclusive)</emphasis> can be used as
a security measure, <emphasis>option (soft exclusive)</emphasis> can
not. Say, if an financial application exports its data as a single
graph <emphasis>http://www.example.com/front-office/cash/</emphasis>
using <emphasis>exclusive</emphasis> then the query that explicitly
refers to that graph will never access any quads written by the
attacker into DB.DBA.RDF_QUAD using same graph IRI. The use of
<emphasis>soft exclusive</emphasis> gives no such protection. From the
compiler's perspective, the <emphasis>option (soft
exclusive)</emphasis> is a hint that may be ignored, not an
unambiguous order.</para></note>

<para>
There is one exception from the rules described above.
This exception is for <emphasis>virtrdf:DefaultQuadStorage</emphasis> only.
If a graph variable of a quad map pattern is not bound and no source graph specified by <emphasis>FROM</emphasis> clauses then quad maps for specific constant graphs are ignored.
In other words, if a default quad storage contains quad maps for specific graphs then the query in that storage should explicitly specify the graph in order to use a map for graph.
This rule will not work if the default quad map is removed from the <emphasis>virtrdf:DefaultQuadStorage</emphasis>.
This rule relates to the default storage itself, not to the containing patterns; copying some or all patterns into other storage will not reproduce there this special effect.
</para>
</sect2>
<sect2 id="rdfviewdescribingsourcerelationaltables"><title>Describing Source Relational Tables</title>
<para>Quad map patterns of an application usually share a common set of source tables and quad map values of one pattern usually share either a single table or very small number of joined tables.
Join and filtering conditions are also usually repeated in different patterns.
It is not necessary to type table descriptions multiple times, they are declare once in the beginning of storage declaration statement and shared between all quad map declarations inside the statement.
Names of aliases can be used instead of table names in quad map values.
</para>
<programlisting><![CDATA[
from DB.DBA.SYS_USERS as user where (^{user.}^.U_IS_ROLE = 0)
from DB.DBA.SYS_USERS as group where (^{group.}^.U_IS_ROLE = 1)
from DB.DBA.SYS_USERS as account
from user as active_user
  where (^{active_user.}^.U_ACCOUNT_DISABLED = 0)
from DB.DBA.SYS_ROLE_GRANTS as grant
  where (^{grant.}^.GI_SUPER = ^{account.}^.U_ID)
  where (^{grant.}^.GI_SUB = ^{group.}^.U_ID)
  where (^{grant.}^.GI_SUPER = ^{user.}^.U_ID)
]]></programlisting>
<para>
This declares five distinct aliases for two distinct tables, and six filtering conditions.
Every condition is an SQL expression with placeholders where a reference to the table should be printed.
The SPARQL compiler will not try to parse texts of these expressions (except dummy search for placeholders), so any logical expressions are acceptable.
When a quad map pattern declaration refers to some aliases, the <emphasis>WHERE</emphasis> clause of the generated SQL code will contain a conjunction of all distinct texts of &quot;relevant&quot; conditions.
A condition is relevant if every alias inside the condition is used in some quad map value of the map pattern, either directly or via clause like <emphasis>from user as active_user</emphasis>.
(<emphasis>user</emphasis> is a &quot;<emphasis>base alias</emphasis>&quot; for <emphasis>active_user</emphasis>).
</para><para>
Consider a group of four declarations.
</para>
<programlisting><![CDATA[
graph <http://myhost/sys>
  {
    oplsioc:user_iri (active_user.U_ID)
        a oplsioc:active-user .
    oplsioc:membership_iri (grant.GI_SUPER, grant.GI_SUB).
        oplsioc:is_direct
            grant.GI_DIRECT ;
        oplsioc:member-e-mail
            active_user.U_E_MAIL
               where (^{active_user.}^.U_E_MAIL like 'mailto:%').
    ldap:account-ref (account.U_NAME)
        ldap:belongs-to
            ldap:account-ref (group.U_NAME) option (using grant).
  }
]]></programlisting>
<para>
The first declaration will extend <emphasis>&lt;http://myhost/sys&gt;</emphasis> graph with one imaginary triples <emphasis>{ user a oplsioc:active-user }</emphasis> for every account record that is not a role and not disabled.
The second declaration deals with membership records.
A membership is a pair of a grantee (&quot;super&quot;) and a granted role (&quot;sub&quot;) stored as a row in <emphasis>DB.DBA.SYS_ROLE_GRANTS</emphasis>).
</para><para>
The second declaration states that every membership has <emphasis>oplsioc:is_direct</emphasis> property with value from <emphasis>GI_DIRECT</emphasis> column of that table (roles may be granted to other roles and users, so permissions are &quot;direct&quot; or &quot;recursive&quot;).
</para><para>
The third declaration declares <emphasis>oplsioc:member-e-mail</emphasis> property of memberships.
The value is a literal string from <emphasis>DB.DBA.SYS_USERS.U_E_MAIL</emphasis>, if the grantee is active (not disabled) and is not a role and its e-mail address starts with <emphasis>'mailto:'</emphasis>.
The join between <emphasis>DB.DBA.SYS_ROLE_GRANTS</emphasis> and <emphasis>DB.DBA.SYS_USERS</emphasis> is made by equality <emphasis>(GI_SUPER = U_ID)</emphasis> because the alias <emphasis>active_user</emphasis> in the declaration &quot;inherits&quot; all conditions specified for <emphasis>user</emphasis>.
In addition, the SPARQL compiler will add one more condition to check if the <emphasis>U_E_MAIL</emphasis> is not null because the NULL value is not a valid object and it knows that <emphasis>U_E_MAIL</emphasis> is not declared as <emphasis>NOT NULL</emphasis>.
</para><para>
The last declaration contains an <emphasis>option</emphasis> clause.
As usual, this indicates that the basic functionality is good for many tasks but not for all.
In this declaration, the <emphasis>ldap:belongs-to</emphasis> property establishes a relation between grantee (subject) and a granted role (object).
Both subject and object IRIs are based on account name, <emphasis>DB.DBA.SYS_USERS.U_NAME</emphasis>, so the quad map pattern contains two references to different aliases of <emphasis>DB.DBA.SYS_USERS</emphasis> but no alias for <emphasis>DB.DBA.SYS_ROLE_GRANTS</emphasis>.
Hence the declaration could produce a triple for every row of the Cartesian product of the <emphasis>DB.DBA.SYS_USERS</emphasis>.
To fix the problem, <emphasis>option (using alias-name)</emphasis> tells the compiler to process the alias-name as if it's used in some quad map value of the pattern.
</para><para>
It is an error to use an alias only in <emphasis>where</emphasis> clause of the quad map pattern but neither in values or in <emphasis>option (using alias-name)</emphasis>.
To detect more typos, an alias used in quad map values can not appear in <emphasis>option (using alias-name)</emphasis> clause.
</para>
</sect2>
<sect2 id="rdfviewiriusingfunction"><title>Function-Based IRI Classes</title>
<para>Most of IRI classes can be declared by a sprintf format string, but sophisticated cases may require calculations, not only printing the string. <emphasis>create IRI class using function</emphasis> allows the application transform relational values to IRIs by any custom routines.</para>
<para>
Let us extend the previous example about users and groups by a new class for grantees. Both users and groups are grantees and we have defined two IRI classes for them. Classes <emphasis>oplsioc:user_iri</emphasis> and <emphasis>oplsioc:group_iri</emphasis> work fine for quad maps of <emphasis>U_ID</emphasis> if and only if the value of <emphasis>U_IS_ROLE</emphasis> is accordingly restricted to FALSE or TRUE, otherwise one may occasionally generate, say, user IRI for a group.
To create and parse IRIs that correspond to any U_IDs, two functions should be created:
</para>
<programlisting><![CDATA[
create function DB.DBA.GRANTEE_URI (in id integer)
returns varchar
{
  declare isrole integer;
  isrole := coalesce ((select top 1 U_IS_ROLE
      from DB.DBA.SYS_USERS where U_ID = id ) );
  if (isrole is null)
    return NULL;
  else if (isrole)
    return sprintf ('http://%s/sys/group?id=%d', id);
  else
    return sprintf ('http://%s/sys/user?id=%d', id);
};
]]></programlisting>
<programlisting><![CDATA[
create function DB.DBA.GRANTEE_URI_INVERSE (in id_iri varchar)
returns integer
{
  declare parts any;
  parts := sprintf_inverse (id_iri,
      'http://myhost/sys/user?id=%d', 1 );
  if (parts is not null)
    {
      if (exists (select top 1 1 from DB.DBA.SYS_USERS
          where U_ID = parts[0] and not U_IS_ROLE ) )
        return parts[0];
    }
  parts := sprintf_inverse (id_iri,
      'http://myhost/sys/group?id=%d', 1 );
  if (parts is not null)
    {
      if (exists (select top 1 1 from DB.DBA.SYS_USERS
          where U_ID = parts[0] and U_IS_ROLE ) )
        return parts[0];
    }
  return NULL;
};
]]></programlisting>
<para>These functions may be more useful if the SPARQL web service endpoint is allowed to use them:</para>
<programlisting><![CDATA[
grant execute on DB.DBA.GRANTEE_URI to "SPARQL";
grant execute on DB.DBA.GRANTEE_URI_INVERSE to "SPARQL";
]]></programlisting>
<para>
The next declaration creates an IRI class based on these two functions:
</para>
<programlisting><![CDATA[
create iri class oplsioc:grantee_iri using
  function DB.DBA.GRANTEE_URI (in id integer)
    returns varchar,
  function DB.DBA.GRANTEE_URI_INVERSE (in id_iri varchar)
    returns integer .
]]></programlisting>
<para>
In common case, IRI class declaration contains an N-array function that composes IRIs and N inverse functions that gets an IRI as an argument and extracts the Nth SQL value.
IRI composing function should silently return NULL on incorrect arguments instead of error signal.
Inverse functions should return NULL if the argument has an incorrect type or value.
</para>
<para>
It is possible to specify only composing function without any of inverse functions. However <emphasis>option (bijection)</emphasis> can not be used in that case, obviously.
</para>
</sect2>
<sect2 id="rdfconnvarsiniriclasses"><title>Connection Variabes in IRI Classes</title>
<para>Writing function-based IRI class is overkill when the IRI can in principle be made by a <link linkend="fn_sprintf_iri"><function>sprintf_iri</function></link> but the format should contain some context-specific data, such as host name used for the <link linked="rdfdynamiclocal">dynamic renaming of local IRIs</link>.
Format strings offer a special syntax for that cases.
<emphasis>%{varname}U</emphasis> acts as <emphasis>%U</emphasis> but the function <link linkend="fn_sprintf"><function>sprintf</function></link> will take the value from client connection variable <emphasis>varname</emphasis>, not from list of arguments.
Similarly, <link linkend="fn_sprintf_inverse"><function>sprintf_inverse</function></link> will not return fragment that match to <emphasis>%{varname}U</emphasis> in the vector of other fragments; instead it will get the value from connection environment and ensure that it matches the fragment of input; mismatch between printed and actual value of variable will means that the whole string do not match the format.</para>
<para>SPARQL optimizer knows about this formatting feature and sometimes it makes more deductions from occurence of <emphasis>%{varname}U</emphasis> than from occurence of plain <emphasis>%U</emphasis>, so this notation may be used in <emphasis>option ( returns ...)</emphasis> when appropriate.
Of course, the optimizer has no access to the actual value of conection variabe because it may vary from run to run or may change between the compilation and the run, but the value is supposed to be persistent during any single query run so <emphasis>%{myvariable}U</emphasis> in one place is equal to <emphasis>%{myvariable}U</emphasis> in other.</para>
<para>Connection variables are set by <link linkend="fn_connection_set"><function>connection_set</function></link> and some of them have default values that are used if not overridden by application:</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem><emphasis>URIQADefaultHost</emphasis> is for default host as it is specified in Virtuoso configuration file.
Note, however, that it will be escaped when printed so if it contains colon and port number then the colon is escaped.
In addition, there are special variables that match dynamic renaming of local IRIs more accurately.</listitem>
<listitem><emphasis>WSHost</emphasis> is for host and port as it is used by current client connection for dynamic renaming.
The colon before port will be escaped.</listitem>
<listitem><emphasis>WSHostName</emphasis> is for host name only, without port, as it is used by current client connection for dynamic renaming.</listitem>
<listitem><emphasis>WSHostPort</emphasis> is for port part of host IRI. That is string, not integer. The only real use of the variable is in formats like <emphasis>http://%{WSHostName}U:%{WSHostPort}U/...</emphasis>.</listitem>
</itemizedlist>

<para>It is inconvenient to write different format strings for
different cases.  Two most common policies are different host names
for default HTTP port of a publicly available service and different
non-default ports for one or more host names of an intranet
instalation; these two approaches are almost never used in a mix. So
declaration of IRI classes may use shorthand
<emphasis>^{DynamicLocalFormat}^</emphasis> in format strings that is
expanded either to <emphasis>http://%{WSHost}U</emphasis> or to
<emphasis>http://%{WSHostName}U:%{WSHostPort}U/...</emphasis>,
depending on absence or presence of port number in the value of
<emphasis>DefaultHost</emphasis> parameter of
<emphasis>URIQA</emphasis> section of configuration file.</para>

<note><para><emphasis>^{DynamicLocalFormat}^</emphasis> is for IRI class declarations only and is not expanded in any other place, so it is useful sometimes to create an IRI class with empty argument list in order to get &quot;almost constant&quot; IRIs calculated without writing special procedures.</para></note>
</sect2>
<sect2 id="rdfviewbijandreturns"><title>Lookup Optimization -- BIJECTION and RETURNS Options</title>
<para>
There is one subtle problem with IRI class declarations.
To get benefit from a relational index, SPARQL optimizer should compose equality between table column and some known SQL value, not between return value of IRI class and a known composed IRI.
In addition, redundant calculations of IRIs takes time.
To enable this optimization, an IRI class declaration should end with <emphasis>option (bijection)</emphasis> clause. For some simple format strings the compiler may recognize the bijection automatically but an explicit declaration is always a good idea.
</para>
<note><title>Note:</title>
<para>
See also: <ulink url="http://en.wikipedia.org/wiki/One-to-one_correspondence">Wikipedia - Bijection</ulink>.
In mathematics, a bijection, or a bijective function is a function f from a set X to a set Y such that,
for every y in Y, there is exactly one x in X such that f(x) = y.
</para>
<para>
Alternatively, f is bijective if it is a one-to-one correspondence between those sets; i.e.,
both one-to-one (injective) and onto (surjective).
</para>
</note>
<para>
The SPARQL compiler may produce big amounts of SQL code when the query contains equality of two calculated IRIs and these IRIs may come from many different IRI classes.
It is possible to provide hints that will let the compiler check if two IRI classes form disjoint sets of possible IRI values. The more disjoint sets are found the less possible combinations remain so the resulting SQL query will contain fewer unions of joins.
The SPARQL compiler can prove some properties of sprintf format strings. E.g., it can prove that set of all strings printed by &quot;http://example.com/item%d&quot; and the set of strings printed by &quot;http://example.com/item%d/&quot; are disjoint.
It can prove some more complicated statements about unions and intersections of sets of strings.
The IRI or literal class declaration may contain <emphasis>option (returns ...)</emphasis> clause that will specify one or more sprintf patterns that cover the set of generated values.
Consider a better version of IRI class declaration listed above:
</para>
<programlisting><![CDATA[
create iri class oplsioc:grantee_iri using
  function DB.DBA.GRANTEE_URI (in id integer)
    returns varchar,
  function DB.DBA.GRANTEE_URI_INVERSE (in id_iri varchar)
    returns integer
  option ( bijection,
    returns "http://myhost/sys/group?id=%d"
    union   "http://myhost/sys/user?id=%d" ) .
]]></programlisting>
<para>
It is very important to keep IRI classes easily distinguishable by the text of IRI string and easy to parse.
</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>Format <emphasis>%U</emphasis> is better than <emphasis>%s</emphasis>, especially in the middle of IRI, because the <emphasis>%U</emphasis> fragment can not contain characters like &quot;/&quot; or &quot;=&quot;; one may prove that <emphasis>/%U/</emphasis> and <emphasis>/abra%d/cadabra/</emphasis> are disjoint but <emphasis>/%s/</emphasis> and <emphasis>/abra%d/cadabra/</emphasis> are not disjoint.
</listitem><listitem>It is better when the variable part like <emphasis>%U</emphasis> or <emphasis>%d</emphasis> is placed between characters that may not occur in the <emphasis>%U</emphasis> or <emphasis>%d</emphasis> output, i.e. <emphasis>%U</emphasis> is placed between &quot;/&quot;, &quot;&amp;&quot; or &quot;=&quot; and <emphasis>%d</emphasis> is placed between non-digits; <emphasis>order_line_%d</emphasis> is better than <emphasis>order-line-%d</emphasis> because minus may be part of <emphasis>%d</emphasis> output.
</listitem><listitem>End-of-line is treated as a special character, so placing <emphasis>%U</emphasis> or <emphasis>%d</emphasis> between &quot;/&quot; and end of line is as good as placing it between two &quot;/&quot;.
</listitem></itemizedlist>
<para>
In some cases <emphasis>option (returns ...)</emphasis> can be used for IRI classes that are declared using sprintf format, but actual data have more specific format.
Consider a literal class declaration that is used to output strings and the application knows that all these strings are ISBN numbers:
</para>
<programlisting><![CDATA[
create literal class example:isbn_ref "%s" (in isbn varchar not null)
  option ( bijection, returns "%u-%u-%u-%u" union "%u-%u-%u-X" )
]]></programlisting>
<para>
Sometimes interoperability restrictions will force you to violate these rules but please try to follow them as often as possible.
</para>
</sect2>
<sect2 id="rdfviewsubclasses"><title>Join Optimization -- Declaring IRI Subclasses</title>
<para>
Additional problem appears when the equality is between two IRIs of two different IRI classes.
Even if both of them are bijections, the compiler does not know if these IRI classes behave identically on the intersection of their domains.
To let the optimizer know this fact, one IRI class can be explicitly declared as a subclass of another:
</para>
<programlisting><![CDATA[
make oplsioc:user_iri subclass of oplsioc:grantee_iri .
make oplsioc:group_iri subclass of oplsioc:grantee_iri .
]]></programlisting>
<para>
The SPARQL compiler can not check the validity of a subclass declaration.
The developer should carefully test functions to ensure that transformations are really subclasses, as well as to ensure that functions of an IRI class declarations are really inverse to each other.
</para><para>
When declaring that a table's primary key is converted into a IRI according to one IRI class, one usually declares that all foreign keys referring to this class also get converted into an IRI as per this same class, or subclass of same class.
</para><para>
Subclasses can be declared for literal classes as well as for IRI classes, but this case is rare. The reason is that most of literals are made by identity literal classes that are disjoint to each other even if values may be equal in SQL sense, such as <emphasis>"2"</emphasis> of type <emphasis>xsd:integer</emphasis> and <emphasis>"2.0"</emphasis> of type <emphasis>xsd:double</emphasis>.
</para>
</sect2>
<sect2 id="rdfmetadatarecovery"><title>RDF Metadata Maintenance and Recovery</title>
   <para>
This section refers to checking and backing up RDF view and storage declarations only.  The checks and backup/restore do not affect physical quads, relational schema or tables or data therein.  For general backup and restore, see server administration.
     To detect and fix automatically most popular sorts of RDF metadata corruption use <link linkend="fn_rdf_audit_metadata"><function>DB.DBA.RDF_AUDIT_METADATA</function></link>.
It is also possible to backup RDF data by
    <link linkend="fn_rdf_backup_metadata"><function>DB.DBA.RDF_BACKUP_METADATA</function></link>
and restore the saved state later by using
    <link linkend="fn_rdf_restore_metadata"><function>DB.DBA.RDF_RESTORE_METADATA</function></link>.
It is convenient to make a backup before any modification of quad storages, quad map patterns or IRI classes, especially during debugging new RDF Views.
   </para>
<note><para>In SQL, adding a new view can not break anything. This is because SQL lacks the ability of querying &quot;everything&quot; so data sources are always specified. This is not true for SPARQL, so please treat <emphasis>any</emphasis> metadata manipulation as potentially destructive operation. If an RDF storage is supposed to be used by more than one application then these applications should be tested together, not one after other, and they should be installed/upgraded on live database in the very same order as they were installed/upgraded on istrumental machine during testing. Always remember that these applications share RDF tables so they may interfere.</para></note>
</sect2>
</sect1>
<sect1 id="rdfsparqlrule"><title>RDF Inference in Virtuoso</title>
<sect2 id="rdfsparqlruleintro"><title>Introduction</title>
<para>Virtuoso SPARQL can use an inference context for inferring triples that are not physically stored.
This functionality applies to physically stored quads and not to virtual triples generated from relational data with RDF views.
Such an inference context can be built from one or more graphs containing RDF Schema triples. The supported
RDF Schema or OWL constraints are imported from these graphs and are grouped together into rule bases.
A rule base is a persistent entity that can be referenced by a SPARQL query or end point. Queries running
with a given rule base work as if the triples asserted by this rule base were included in the graph or graphs accessed by the query.
</para>
<para>As of version 5.0, Virtuoso recognizes <emphasis>rdfs:subClassOf</emphasis> and <emphasis>rdfs:subPropertyOf</emphasis>.
owl:sameAs is considered for arbitrary subjects and objects if specially enabled by a pragme in the query.
As of 5.00.3031, owl:sameAs, owl:equivalentClass and owl:equivalentProperty are also considered when determining subclass or subproperty relations.  If two  classes are equivalent, they share all instances, subclasses and superclasses directly or indirectly stated in the data for either class.
Other RDF Schema or OWL information is not taken into account.
</para>
</sect2>
<sect2 id="rdfsparqlrulemake"><title>Making Rule Sets</title>
<para>Since RDF Schema and OWL schemas are RDF graphs, these can be loaded into the triple store. Thus, in order to use
such a schema as query context, one first loads the corresponding document into the triple store using <emphasis>ttlp</emphasis> or
<emphasis>rdf_load_rdfxml</emphasis> or related functions. After the schema document is loaded, one can add the assertions therein
into an inference context with the <emphasis>rdfs_rule_set</emphasis> function. This function specifies a logical name for the rule
set plus a graph URI. It is possible to combine multiple schema graphs into a single rule set. A single schema
graph may also independently participate in multiple rule sets.
</para>
<programlisting>
rdfs_rule_set (in name varchar, in uri varchar, in remove int := 0)
</programlisting>
<para>This function adds the applicable facts of the graph into a rule set. The graph URI must correspond
to the graph IRI of a graph stored in the triple store of the Virtuoso instance. If the remove argument
is true, the specified graph is removed from the rule set instead.
</para>
</sect2>
<sect2 id="rdfsparqlrulechange"><title>Changing Rule Sets</title>
<para>Changing a rule set affects queries made after the change. Some queries may have been previously
compiled and will not be changed as a result of modifying the rule set. When a rule set is changed, i.e.
when <emphasis>rdfs_rule_set</emphasis> is called with the first argument set to a pre-existing rule set's name, all the graphs
associated with this name are read and the relevant facts are added to a new empty rule set. Thus, if
triples are deleted from or added to the graphs comprising the rule set, calling <emphasis>rdfs_rule_set</emphasis> will refresh
the rule set to correspond to the state of the stored graphs.
</para>
</sect2>
<sect2 id="rdfsparqlrulesubclassandsubprop"><title>Subclasses and Subproperties</title>
<para>Virtuoso SPARQL supports RDF Schema subclasses and subproperties.
</para>
<para>The predicates <emphasis>rdfs:subClassOf</emphasis> and <emphasis>rdfs:subPropertyOf</emphasis> are
recognized when they appear in graphs included in a rule set. When such a rule set is specified as a context
for a SPARQL query, the following extra triples are generated as needed.
</para>
<para>For every <emphasis>?s rdf:type ?class</emphasis>, a triple <emphasis>?s rdf:type ?superclass</emphasis> is considered to exist,
such that <emphasis>?superclass</emphasis> is a direct or indirect superclass of <emphasis>?class</emphasis>. Direct superclasses are
declared with the <emphasis>rdfs:subClassOf</emphasis> predicate in the rule set graph. Transitivity of superclasses
is automatically taken into account, meaning that if a is a superclass of b and b a superclass of c,
then a is a superclass of c also. Cyclic superclass relations are not allowed. If such occur in the rule set data,
the behavior is undefined but will not involve unterminating recursion.
</para>
<para>For every <emphasis>?s ?subpredicate ?o</emphasis>, a triple <emphasis>?s ?superpredicate ?o</emphasis>
is considered to exist if the rule context declares <emphasis>?superpredicate</emphasis> to be a superpredicate
of <emphasis>?predicate</emphasis>. This is done by having the triple <emphasis>?subpredicate rdfs:subPropertyOf ?superpredicate</emphasis>
as part of the graphs making up the rule context. Transitivity is observed, thus if a is a subpredicate of b and b
a subpredicate of c, then a is also a subpredicate of c.
</para>
</sect2>
<sect2 id="rdfsameas"><title>OWL same-as Support</title>
<para>
Virtuoso has limited support for the OWL same-as predicate.
</para>
<para>
In the following, the abbreviation owl:sameAs is used for  the IRI http://www.w3.org/2002/07/owl#same-as .
</para>
<para>
If same-as traversal is enabled and a triple pattern with a given
subject or object is being matched, all the synonyms of the S and O
will be tried and results generated for all the tried bindings of S
and O.  The set of synonyms is generated at run time by following all
owl:sameAs triples where the IRI in question is either the subject or
the object.  These are followed recursively from object to subject and
subject to object until the complete transitive closure is generated.
All same-as triples from all the graphs applicable to instantiating
the triple pattern at hand are considered.
</para>
<para>
Thus, if we have
</para>
<programlisting><![CDATA[
<thing> <owl:sameAs> <gizmo> .
<thing> <label> "thingy" .
]]></programlisting>
<para>
and we instantiate <emphasis>?s &lt;label> "thingy"</emphasis>
we get <emphasis>?s</emphasis> bound to <emphasis>&lt;thing></emphasis>.
</para>
<para>
If we instantiate <emphasis>&lt;gizmo> &lt;label> ?l</emphasis>
we get <emphasis>?l</emphasis> bound to <emphasis>"thingy"</emphasis> because the subject was given and it was expanded to its synonyms.
</para>
<para>
If binding a variable in a pattern where the variable was free, we do not expand the value to the complete set of its synonyms.
</para>
<para>
Same-as expansion is enabled in a query by <emphasis>define input:same-as "yes"</emphasis> in the beginning of the SPARQL query.
This has a significant run time cost but is in some cases useful when joining data between sets which are mapped to each other with same-as.
</para>
<para>
We note that the number of same-as expansions will depend on the join order used for the SPARQL query.
The compiler does not know the number of synonyms and cannot set the join order accordingly.
Regardless of the join order we will however get at least one IRI of the each synonym set as answer.
Also when interactively navigating a graph with a browser, the same-as expansion will take all synonyms into account.
</para>
<para>
For getting the complete entailment of same-as, a forward
chaining approach should be used, effectively asserting all the
implied triples.
</para>
</sect2>
<sect2 id="rdfsparqlruleintro"><title>Implementation</title>
<para>Triples entailed by subclass or subproperty statements in an inference context are not physically stored.
Such triples are added to the result set by the query run time as needed. Also queries involving subclass or subproperty
rules are not rewritten into unions of all the possible triple patterns that might imply the pattern that is requested.
Instead, the SQL compiler adds special nodes that iterate over subclasses or subproperties at run time. The cost model
also takes subclasses and subproperties into account when determining the approximate cardinality of triple patterns.
</para>
<para>In essence, Virtuoso's support for subclasses and subproperties is backward chaining, i.e. it does not materialize
all implied triples but rather looks for the basic facts implying these triples at query evaluation time.
</para>
</sect2>
<sect2 id="rdfsparqlruleintro"><title>Enabling Inferencing</title>
<para>In a SPARQL query, the define input:inference clause is used to instruct the compiler to use the rules in the named rule set. For example:
</para>
<programlisting>
SQL> rdfs_rule_set ('sample', 'rule_graph');

SQL> sparql define input:inference "sample" select * from &lt;g&gt; where {?s ?p ?o};
</programlisting>
<para>will include all the implied triples in the result set, using the rules in the sample rule set.
</para>
<para>Inference can be enabled triple pattern by triple pattern. This is done with the option
(inference 'rule_set') clause after the triple pattern concerned. Specifying option (inference none)
will disable inference for the pattern concerned while the default inference context applies to the
rest of the patterns. Note that the keyword is input:inference in the query header and simply inference
in the option clause. See the examples section below for examples.
</para>
<para>In SQL, if RDF_QUAD occurs in a select from clause, inference can be added with the table option <emphasis>WITH</emphasis>, as follows:
</para>
<programlisting>
select * from rdf_quad table option (with 'sample') where g = iri_to_id ('xx', 0);
</programlisting>
<para>This is about the same as:
</para>
<programlisting>
define input:inference "sample" select * from &lt;xx&gt; where {?s ?p ?o}
</programlisting>
</sect2>
<sect2 id="rdfsparqlruleexamples"><title>Examples</title>
<programlisting><![CDATA[
ttlp ('
   <http://localhost:8890/dataspace>  	                        <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  <http://rdfs.org/sioc/ns#Space>.
   <http://localhost:8890/dataspace/test2/weblog/test2tWeblog>  <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  <http://rdfs.org/sioc/types#Weblog> .
   <http://localhost:8890/dataspace/discussion/oWiki-test1Wiki> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  <http://rdfs.org/sioc/types#MessageBoard>.
   <http://localhost:8890/dataspace>                            <http://rdfs.org/sioc/ns#link>                     <http://localhost:8890/ods> .
   <http://localhost:8890/dataspace/test2/weblog/test2tWeblog>  <http://rdfs.org/sioc/ns#link>                     <http://localhost:8890/dataspace/test2/weblog/test2tWeblog>.
   <http://localhost:8890/dataspace/discussion/oWiki-test1Wiki> <http://rdfs.org/sioc/ns#link>                     <http://localhost:8890/dataspace/discussion/oWiki-test1Wiki> .
   ', '', 'http://localhost:8890/test');
]]></programlisting>
<para>This loads data space instance data Triples into a Named Graph: &lt;http://localhost:8890/test&gt;
</para>
<programlisting><![CDATA[
ttlp (' @prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
  <http://rdfs.org/sioc/ns#Space> rdfs:subClassOf <http://www.w3.org/2000/01/rdf-schema#Resource> .
  <http://rdfs.org/sioc/ns#Container> rdfs:subClassOf <http://rdfs.org/sioc/ns#Space> .
  <http://rdfs.org/sioc/ns#Forum> rdfs:subClassOf <http://rdfs.org/sioc/ns#Container> .
  <http://rdfs.org/sioc/types#Weblog> rdfs:subClassOf <http://rdfs.org/sioc/ns#Forum> .
  <http://rdfs.org/sioc/types#MessageBoard> rdfs:subClassOf <http://rdfs.org/sioc/ns#Forum> .
  <http://rdfs.org/sioc/ns#link> rdfs:subPropertyOf <http://rdfs.org/sioc/ns> .
  ', '', 'http://localhost:8890/schema/test');
]]></programlisting>
<para>This loads a Triples into a Named Graph for schema/ontology data called:
&lt;http://localhost:8890/schema/test&gt; that expresses assertions
about subclasses and subproperties.
</para>
<programlisting>
rdfs_rule_set ('http://localhost:8890/schema/property_rules1', 'http://localhost:8890/schema/test');
</programlisting>
<para>This defines the rule context http://localhost:8890/schema/property_rules1 that is initialized from the contents of graph http://localhost:8890/schema/test.
</para>
<programlisting><![CDATA[
SQL>sparql define input:inference 'http://localhost:8890/schema/property_rules1' select ?s from <http://localhost:8890/test> where {?s <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  <http://rdfs.org/sioc/ns#Space> };
s
VARCHAR
_______________________________________________________________________________

http://localhost:8890/dataspace/test2/weblog/test2tWeblog
http://localhost:8890/dataspace/discussion/oWiki-test1Wiki
http://localhost:8890/dataspace

3 Rows. -- 0 msec.
]]></programlisting>
<para>This returns the instances of http://rdfs.org/sioc/ns#Space. Since http://rdfs.org/sioc/types#Weblog
and http://rdfs.org/sioc/types#MessageBoard are subclasses of http://rdfs.org/sioc/ns#Space,
instances of http://rdfs.org/sioc/ns#Space, http://rdfs.org/sioc/types#Weblog and
http://rdfs.org/sioc/types#MessageBoard are all returned. This results in the subjects
http://localhost:8890/dataspace, http://localhost:8890/dataspace/test2/weblog/test2tWeblog and
http://localhost:8890/dataspace/discussion/oWiki-test1Wiki.
</para>
<programlisting><![CDATA[
SQL>select id_to_iri (s)
from rdf_quad table option (with 'http://localhost:8890/schema/property_rules1')
where g = iri_to_id ('http://localhost:8890/test',0)
  and p = iri_to_id ('http://www.w3.org/1999/02/22-rdf-syntax-ns#type', 0)
  and o = iri_to_id ('http://rdfs.org/sioc/ns#Space', 0);
callret
VARCHAR
_______________________________________________________________________________

http://localhost:8890/dataspace/test2/weblog/test2tWeblog
http://localhost:8890/dataspace/discussion/oWiki-test1Wiki
http://localhost:8890/dataspace

3 Rows. -- 10 msec.
]]></programlisting>
<para>This is the corresponding SQL query, internally generated by the SPARQL query.</para>
<para>Below we first look for all instances of http://rdfs.org/sioc/ns#Space with some
property set to http://localhost:8890/dataspace/test2/weblog/test2tWeblog.
We get the subject http://localhost:8890/dataspace/test2/weblog/test2tWeblog and the
properties http://rdfs.org/sioc/ns#link and http://rdfs.org/sioc/ns.
The join involves both subclass and subproperty inference. Then we turn off the inference
for the second pattern and only get the property http://rdfs.org/sioc/ns#link. Then we do
the same but now specify that inference should apply only to the first triple pattern.
</para>
<programlisting><![CDATA[

SQL>sparql define input:inference  'http://localhost:8890/schema/property_rules1' select * from <http://localhost:8890/test>
where { ?s ?p <http://rdfs.org/sioc/ns#Space> . ?s ?p1 <http://localhost:8890/dataspace/test2/weblog/test2tWeblog> . };


s             p              p1
VARCHAR       VARCHAR       VARCHAR
_______________________________________________________________________________

http://localhost:8890/dataspace/test2/weblog/test2tWeblog  http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://rdfs.org/sioc/ns#link
http://localhost:8890/dataspace/test2/weblog/test2tWeblog  http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://rdfs.org/sioc/ns

2 Rows. -- 0 msec.


SQL>sparql  select * from <http://localhost:8890/test>
where { ?s ?p <http://rdfs.org/sioc/ns#Space> option (inference 'http://localhost:8890/schema/property_rules1') . ?s ?p1 <http://localhost:8890/dataspace/test2/weblog/test2tWeblog> . };

s             p              p1
VARCHAR       VARCHAR        VARCHAR
_______________________________________________________________________________

http://localhost:8890/dataspace/test2/weblog/test2tWeblog  http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://rdfs.org/sioc/ns#link

1 Rows. -- 10 msec.

]]></programlisting>
<emphasis>DBpedia example</emphasis>
<programlisting><![CDATA[
ttlp ('
 prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
 <http://dbpedia.org/property/birthcity> rdfs:subPropertyOf <http://dbpedia.org/property/birthPlace> .
 <http://dbpedia.org/property/birthcountry> rdfs:subPropertyOf  <http://dbpedia.org/property/birthPlace> .
 <http://dbpedia.org/property/cityofbirth> rdfs:subPropertyOf <http://dbpedia.org/property/birthPlace> .
 <http://dbpedia.org/property/countryofbirth> rdfs:subPropertyOf <http://dbpedia.org/property/birthPlace> .
 <http://dbpedia.org/property/countyofbirth> rdfs:subPropertyOf <http://dbpedia.org/property/birthPlace> .
 <http://dbpedia.org/property/cityofdeath> rdfs:subPropertyOf <http://dbpedia.org/property/deathPlace> .
 <http://dbpedia.org/property/countryofdeath> rdfs:subPropertyOf <http://dbpedia.org/property/deathPlace> . "",
 'http://dbpedia.org/inference/rules#') ;

rdfs_rule_set ('http://dbpedia.org/schema/property_rules1', 'http://dbpedia.org/inference/rules#');
]]></programlisting>
<programlisting><![CDATA[
sparql
define input:inference 'http://dbpedia.org/schema/property_rules1'
prefix p: <http://dbpedia.org/property/>
select ?s from <http://dbpedia.org> where {?s p:birthcity ?o }
limit 50
s
VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/Britt_Janyk
http://dbpedia.org/resource/Chiara_Costazza
http://dbpedia.org/resource/Christoph_Gruber
http://dbpedia.org/resource/Daron_Rahlves
http://dbpedia.org/resource/Finlay_Mickel
http://dbpedia.org/resource/Genevi%C3%A8ve_Simard
http://dbpedia.org/resource/Johann_Grugger
http://dbpedia.org/resource/Kalle_Palander
http://dbpedia.org/resource/Marc_Gini
http://dbpedia.org/resource/Mario_Scheiber
http://dbpedia.org/resource/Prince_Hubertus_of_Hohenlohe-Langenburg
http://dbpedia.org/resource/Resi_Stiegler
http://dbpedia.org/resource/Steven_Nyman
http://dbpedia.org/resource/Hannes_Reichelt
http://dbpedia.org/resource/Jeremy_Transue

15 Rows. -- 167 msec.

sparql
define input:inference 'http://dbpedia.org/schema/property_rules1'
prefix p: <http://dbpedia.org/property/>
select ?s from <http://dbpedia.org> where {?s p:countryofbirth ?o }
limit 50
s
VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/A._J._Wood
http://dbpedia.org/resource/A._J._Godbolt
http://dbpedia.org/resource/Ac%C3%A1cio_Casimiro
http://dbpedia.org/resource/Adam_Fry
http://dbpedia.org/resource/Adam_Gilchrist
http://dbpedia.org/resource/Adam_Griffin
http://dbpedia.org/resource/Adam_Gross
...

50 Rows. -- 324 msec.

sparql
define input:inference 'http://dbpedia.org/schema/property_rules1'
prefix p: <http://dbpedia.org/property/>
select ?s from <http://dbpedia.org> where {?s p:countyofbirth ?o }
limit 50

s
VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/Eddie_Colman

1 Rows. -- 163 msec.

sparql
define input:inference 'http://dbpedia.org/schema/property_rules1'
prefix p: <http://dbpedia.org/property/>
select ?s from <http://dbpedia.org> where {?s p:birthPlace ?o }

s
VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/Eddie_Colman
http://dbpedia.org/resource/Jeremy_Transue
http://dbpedia.org/resource/Finlay_Mickel
http://dbpedia.org/resource/Prince_Hubertus_of_Hohenlohe-Langenburg
http://dbpedia.org/resource/Hannes_Reichelt
http://dbpedia.org/resource/Johann_Grugger
http://dbpedia.org/resource/Chiara_Costazza
...
155287 Rows. -- 342179 msec.

]]></programlisting>
<emphasis>Loading script of the Yago Class hierarchy as inference rules example</emphasis>
<programlisting><![CDATA[
--- Load Class Hierarchy into a Named Graph
select ttlp_mt (file_to_string_output ('yago-class-hierarchy_en.nt'),
'', 'http://dbpedia.org/resource/classes/yago#');

-- Create an  Inference Rule that references the Yago Class Hierarchy
Named Graph

SQL>rdfs_rule_set ('http://dbpedia.org/resource/inference/rules/yago#',
'http://dbpedia.org/resource/classes/yago#');

-- Query for the "The Lord of the Rings" which is a "Book" as
-- explicitly claimed in the DBpedia data set (instance data)

SQL>SPARQL
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX dbpedia: <http://dbpedia.org/property/>
PREFIX yago: <http://dbpedia.org/class/yago/>

SELECT ?s
FROM <http://dbpedia.org>
WHERE {
?s a yago:Book106410904 .
?s dbpedia:name "The Lord of the Rings"@en .
};

s
VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/The_Lord_of_the_Rings

1 Rows. -- 321 msec.

-- Query aimed at Books via query scoped to the "Publication" class
-- of which it is a subclass in the Yago Hierarchy
SQL>SPARQL
define input:inference
'http://dbpedia.org/resource/inference/rules/yago#'
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX dbpedia: <http://dbpedia.org/property/>
PREFIX yago: <http://dbpedia.org/class/yago/>

SELECT ?s
FROM <http://dbpedia.org>
WHERE {
?s a yago:Publication106589574 .
?s dbpedia:name "The Lord of the Rings"@en .
};

s
VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/The_Lord_of_the_Rings


-- # Variant of query with Virtuoso's Full Text Index extension: bif:contains
SQL>SPARQL
define input:inference
'http://dbpedia.org/resource/inference/rules/yago#'
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX dbpedia: <http://dbpedia.org/property/>
PREFIX yago: <http://dbpedia.org/class/yago/>

SELECT ?s ?n
FROM <http://dbpedia.org>
WHERE {
?s a yago:Publication106589574 .
?s dbpedia:name ?n .
?n bif:contains 'Lord and Rings'
};

s                                                                                 n
VARCHAR                                                                           VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/The_Lord_of_the_Rings                                 The Lord of the Rings
http://dbpedia.org/resource/Journeys_of_Frodo                                     Journeys of Frodo: An Atlas of J. R. R. Tolkien's The Lord of the Ri
ngs
http://dbpedia.org/resource/The_Lord_of_the_Rings:_A_Reader%27s_Companion         The Lord of the Rings: A Reader's Companion
http://dbpedia.org/resource/Tolkien:_A_Look_Behind_%22The_Lord_of_the_Rings%22    A Look Behind "The Lord of the Rings""

4 Rows. -- 29573 msec.

-- Retrieve all individuals instances of the Book Class
SQL>SPARQL
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX dbpedia: <http://dbpedia.org/property/>
PREFIX yago: <http://dbpedia.org/class/yago/>

SELECT ?s ?n
FROM <http://dbpedia.org>
WHERE {
?s a yago:Book106410904 .
?s dbpedia:name ?n .
}
limit 10;
s                                                                                 n
VARCHAR                                                                           VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/.hack//AI_buster                                      .hack// AI buster
http://dbpedia.org/resource/1000_Love_and_Sex_Quiz                                1000 Love and Sex Quiz
http://dbpedia.org/resource/1491:_New_Revelations_of_the_Americas_Before_Columbus  1491: New Revelations of the Americas Before Columbus
http://dbpedia.org/resource/.hack//AI_buster_2                                    .hack// AI buster 2
http://dbpedia.org/resource/100_Books_by_August_Derleth                           100 Books by August Derleth
http://dbpedia.org/resource/1634:_The_Baltic_War                                  1634: The Baltic War
http://dbpedia.org/resource/003%C2%BD:_The_Adventures_of_James_Bond_Junior        003-+: The Adventures of James Bond Junior"
http://dbpedia.org/resource/1001_Movies_You_Must_See_Before_You_Die               1001 Movies You Must See Before You Die
http://dbpedia.org/resource/1066_and_All_That                                     1066 and All That
http://dbpedia.org/resource/14%2C000_Things_to_be_Happy_About                     14,000 Things to be Happy About

10 Rows. -- 1422 msec.

-- Retrieve all individuals instances of Publication Class which
-- should include all Books.
SQL>SPARQL
define input:inference
'http://dbpedia.org/resource/inference/rules/yago#'
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX dbpedia: <http://dbpedia.org/property/>
PREFIX yago: <http://dbpedia.org/class/yago/>

SELECT ?s ?n
FROM <http://dbpedia.org>
WHERE {
?s a yago:Publication106589574 .
?s dbpedia:name ?n .
};
s                                                                                 n
VARCHAR                                                                           VARCHAR
_______________________________________________________________________________

http://dbpedia.org/resource/461_Ocean_Boulevard                                   461 Ocean Boulevard
http://dbpedia.org/resource/A_Love_Supreme                                        A Love Supreme
http://dbpedia.org/resource/A_Night_at_Red_Rocks_with_the_Colorado_Symphony_Orchestra  A Night at Red Rocks with the Colorado Symphony Orchestra
http://dbpedia.org/resource/At_Fillmore_East                                      At Fillmore East
http://dbpedia.org/resource/August_and_Everything_After                           August and Everything After
...
]]></programlisting>
</sect2>
</sect1>
<sect1 id="rdfsparqlrulefulltext"><title>Using Full Text Search in SPARQL</title>
<para>Virtuoso's triple store supports optional full text indexing of RDF object values since version 5.0.
It is possible to declare that objects of triples with a given predicate or graph get indexed.
The graphs and triples may be enumerated or a wildcard may be used.
</para>
<para>The triples for which a full text index entry exists can be found using the <emphasis>bif:contains</emphasis>
or related filters and predicates.
</para>
<para>For example, the query:
</para>
<programlisting>
select *
  from &lt;people&gt;
 where {?s foaf:Name ?name . ?name bif:contains "rich*" .}
</programlisting>
<para>would match all subjects whose <emphasis>foaf:Name</emphasis> contained a word starting with Rich.
This would match Richard, Richie etc.
</para>
<para>If the <emphasis>bif:contains</emphasis> or related predicate is applied to an object that is not
a string or is not the object of an indexed triple, no match will be found.
</para>
<para>The syntax for text patterns is identical to the syntax for the SQL contains predicate.
</para>
<para>The SPARQL/SQL optimizer determines whether the text pattern will be used to drive the query or whether it
will filter results after other conditions are applied first. As opposed to <emphasis>bif:contains</emphasis>,
regexp matching never drives the query or makes us of an index, thus regexps are in practice checked after other conditions.
</para>
<sect2 id="rdfsparqlrulespecifywhatindex"><title>Specifying What to Index</title>
<para>Whether the object of a given triple is indexed in the text index depends on indexing rules. If at least one
indexing rule matches the triple, the object gets indexed if the object is a string. An indexing rule specifies
a graph and a predicate. Either may be an IRI or NULL, in which case it matches all IRI's.
</para>
<para>Rules also have a 'reason', which can be used to group rules into application-specific sets. A triple will stop
being indexed only after all rules mandating its indexing are removed. When an application requires indexing a
certain set of triples, rules are added to for the purpose. These rules are tagged with the name of the application
as their reason. When an application no longer requires indexing, the rules belonging to this application can be
removed. This will not turn off indexing if another application still needs certain triples to stay indexed.
</para>
<para>Indexing is enabled/disable for specific graph/predicate combinations with:
</para>
<programlisting>
create function DB.DBA.RDF_OBJ_FT_RULE_ADD
  (in rule_g varchar, in rule_p varchar, in reason varchar) returns integer
</programlisting>
<programlisting>
create function DB.DBA.RDF_OBJ_FT_RULE_DEL
  (in rule_g varchar, in rule_p varchar, in reason varchar) returns integer
</programlisting>
<para>The first function adds a rule. The two first arguments are the text representation of the IRI's for the graph
and predicate. If NULL is given then all graph's or predicates match. Specifying both as NULL means that all
string valued objects will be added to a text index.
</para>
<para>The second function reverses the effect of the first. Only a rule that actually has been added can be deleted.
Thus one cannot say that all except a certain enumerated set should be indexed.
</para>
<para>The reason argument is an arbitrary string identifying the application that needs this rule.
Two applications can add the same rule.
Removing one of them will still keep the rule in effect.
If an object is indexed due to more than one rule the index data remain free from duplicates, neither index size nor speed is affected.
</para>
<para>
If <emphasis>DB.DBA.RDF_OBJ_FT_RULE_ADD</emphasis> detects that the <emphasis>DB.DBA.RDF_QUAD</emphasis> contains quads whose graphs and/or predicates match to the new rule but not indexed before due to other rules then these quads are indexed automatically.
However the function <emphasis>DB.DBA.RDF_OBJ_FT_RULE_DEL</emphasis> does not remove indexing data about related objects.
Thus the presence of indexing data about an object does not imply that it is necessarily used in some quad that matches to some rule.
</para>
<para>Functions return one if the rule is added or deleted and zero if the call was redundant (the rule has been added before or there's no rule to delete).
</para>
<programlisting><![CDATA[

-- We load Tim Berners-Lee's FOAF file into a graph called people.

DB.DBA.RDF_LOAD_RDFXML (http_get ('http://www.w3.org/People/Berners-Lee/card#i'), 'no', 'people');

-- We check how many triples we got.

select count (*) from (sparql select * from <people> where {?s ?p ?o})f;

-- We specify that all string objects in the graph people should be text indexed.

DB.DBA.RDF_OBJ_FT_RULE_ADD ('people', null, 'people');

-- We update the text index. See below on how to keep the text index automatically updated.

DB.DBA.VT_INC_INDEX_DB_DBA_RDF_OBJ ();

-- We ask for the subjects and predicates of all triples in <people> where the object is a string which contains a word beginning with TIM.

sparql select * from <people> where { ?s ?p ?o . ?o bif:contains '"TIM*"' .};

s                p                o
VARCHAR          VARCHAR          VARCHAR
_______________________________________________________________________________

http://no        http://purl.org/dc/elements/1.1/title  Tim Berners-Lee's FOAF file
http://www.w3.org/People/Berners-Lee/card#i  http://xmlns.com/foaf/0.1/name  Timothy Berners-Lee
http://www.w3.org/People/Berners-Lee/card#i  http://www.w3.org/2000/01/rdf-schema#label  Tim Berners-Lee
http://www.w3.org/People/Berners-Lee/card#i  http://xmlns.com/foaf/0.1/givenname  Timothy
http://www.w3.org/People/Berners-Lee/card#i  http://xmlns.com/foaf/0.1/nick  TimBL
http://www.w3.org/People/Berners-Lee/card#i  http://xmlns.com/foaf/0.1/nick  timbl
http://dig.csail.mit.edu/breadcrumbs/blog/4  http://purl.org/dc/elements/1.1/title  timbl's blog

7 Rows. -- 2 msec.
]]></programlisting>
<para>
The below query is identical with the above but uses a different syntax.
The filter syntax is more flexible in that it allows passing extra options to the contains predicate. These may be useful in the future.
</para>
<programlisting><![CDATA[
sparql select * from <people> where { ?s ?p ?o . filter (bif:contains(?o,  '"TIM*"')) };
]]></programlisting>
<note><title>Note:</title><para>It is better to upgrade to the latest version of Virtuoso before adding free-text rules for the first time.
The upgrade is especially advised in case of big amounts of texts to be indexed.
The reason is that the free-text index on RDF may be changed in future versions and automatic upgrade of an existing index data into new format may take much more time than indexing from scratch.</para></note>
<para>The table <emphasis>DB.DBA.RDF_OBJ_FT_RULES</emphasis> stores list of free-text index configuration rules.
</para>
<programlisting><![CDATA[
create table DB.DBA.RDF_OBJ_FT_RULES (
  ROFR_G varchar not null,       -- specific graph IRI or NULL for "all graphs"
  ROFR_P varchar not null,       -- specific predicate IRI or NULL for "all predicates"
  ROFR_REASON varchar not null,  -- identification string of a creator, preferably human-readable
  primary key (ROFR_G, ROFR_P, ROFR_REASON) );
]]></programlisting>
<para>
Applications may read from this table but they should not write directly.
Numerous duplicates in rules does not affect speed of free-text index operations because the content of the table is cached in memory in a special way,
Unlike the use of configuration functions, direct write to the table will not update that cache.
</para>
<para>
The table is convenient to search for rules added by a given application.
If a unique identification string is used during installation of an application when rules are added then it's easy to remove that rules by an uninstall.
</para>
</sect2>
<sect2 id="rdfsparqlruletimeindexing"><title>Time of Indexing</title>
<para>The triple store's text index is by default in manual batch mode. This means that changes in triples are periodically
reflected in the text index but are not maintained in strict synchrony. This is much more efficient than keeping the
indices in constant synchrony. This setting may be altered with the <emphasis>db.dba.vt_batch_update</emphasis> stored procedure.
</para>
<para>To force synchronization of the RDF text index, use:
</para>
<programlisting>
DB.DBA.VT_INC_INDEX_DB_DBA_RDF_OBJ ();
</programlisting>
<para>To set the text index to follow the triples in real time, use:
</para>
<programlisting>
DB.DBA.VT_BATCH_UPDATE ('DB.DBA.RDF_OBJ', 'ON', null);
</programlisting>
<para>To set the text index to be updated every 10 minutes, use:
</para>
<programlisting>
DB.DBA.VT_BATCH_UPDATE ('DB.DBA.RDF_OBJ', 'ON', 10);
</programlisting>
<para>To make the update always manual, specify NULL as the last argument above.
</para>
<para>
Additional problem related to free-text index of <emphasis>DB.DBA.RDF_QUAD</emphasis> is that some applications (e.g. import of billions of triples) may set triggers off.
This will make free-text index data incomplete.
Call of procedure <emphasis>DB.DBA.RDF_OBJ_FT_RECOVER ()</emphasis> will insert all missing free-text index items by drop and re-insert every existing free-text index rule.
</para>
</sect2>
<sect2 id="rdfviewsandfreetext"><title>Free-Text Indexes on RDF Views</title>
<para>
If an <emphasis>O</emphasis> field of a quad map pattern gets its value from a database column that have a free text index then this index can be used in SPARQL for efficient text search.
As a variant, the free-text index of an additional table may be used.
</para>
<para>
If a statement of quad map pattern declaration starts with declaration of table aliases, declaration of table alias may include name of table column that should have a text index.
Consider possible use of free-text index on content of DAV resources stored in DAV system tables of Virtuoso:
</para>
<programlisting><![CDATA[
prefix mydav: <...>
create quad storage mydav:metadata
from WS.WS.SYS_DAV_RES as dav_resource text literal RES_CONTENT
...
  {
    ...
    mydav:resource-iri (dav_resource.RES_FULL_PATH)
        a mydav:resource ;
        mydav:resource-content dav_resource.RES_CONTENT ;
        mydav:resource-mime-type dav_resource.RESTYPE ;
    ...
  }
]]></programlisting>
<para>
The clause <emphasis>text literal RES_CONTENT</emphasis> grants the SPARQL compiler permission to use free-text index for objects that are literals composed from column <emphasis>dav_resource.RES_CONTENT</emphasis>; this clause also choose between <emphasis>text literal</emphasis> (supports <emphasis>contains()</emphasis> predicate only) and <emphasis>text xml literal</emphasis> (supports both <emphasis>contains()</emphasis> and <emphasis>xcontains()</emphasis>) text indexes.
It is important to understand that free-text index will produce results using raw relational data.
If a literal class transformation changes the text stored in the column then these changes are ignored by free-text search.
E.g., a transformation concatenates a word to the value of the column, but the free-text search will not find this word.
</para>
<para>
The free-text index may be used in a more sophisticated way. Consider a built-in table <emphasis>DB.DBA.RDF_QUAD</emphasis> that does not have a free-text index.
Moreover, the table does not contain full values of all objects; the <emphasis>O</emphasis> column contains &quot;short enough&quot; values inlined, but long and special values are represented by links to <emphasis>DB.DBA.RDF_OBJ</emphasis> table.
The RDF_OBJ table, however, has free-text index that can be used.
The full declaration of built-in default mapping for default storage could be written this way:
</para>
<programlisting><![CDATA[
-- Important! Do not try to execute on live system
-- without prior changing names of storage and quad map pattern!

sparql
create virtrdf:DefaultQuadMap as
graph rdfdf:default-iid-nonblank (DB.DBA.RDF_QUAD.G)
subject rdfdf:default-iid (DB.DBA.RDF_QUAD.S)
predicate rdfdf:default-iid-nonblank (DB.DBA.RDF_QUAD.P)
object rdfdf:default (DB.DBA.RDF_QUAD.O)

create quad storage virtrdf:DefaultQuadStorage
from DB.DBA.RDF_QUAD as physical_quad
from DB.DBA.RDF_OBJ as physical_obj text xml literal RO_DIGEST of (physical_quad.O)
where (^{physical_quad.}^.O = ^{physical_obj.}^.RO_DIGEST)
  {
    create virtrdf:DefaultQuadMap as
      graph rdfdf:default-iid-nonblank (physical_quad.G)
      subject rdfdf:default-iid (physical_quad.S)
      predicate rdfdf:default-iid-nonblank (physical_quad.P)
      object rdfdf:default (physical_quad.O) .
  }
;
]]></programlisting>
<para>
The reference to the free-text index is extended by clause <emphasis> of (physical_quad.O)</emphasis>.
This means that the free-text on <emphasis>DB.DBA.RDF_OBJ.RO_DIGEST</emphasis> will be used when the object value comes from <emphasis>physical_quad.O</emphasis> as if <emphasis>physical_quad.O</emphasis> were indexed itself.
If a SPARQL query invokes <emphasis>virtrdf:DefaultQuadMap</emphasis> but contains no free-text criteria then only <emphasis>DB.DBA.RDF_QUAD</emphasis> appears in the final SQL statement and no join with <emphasis>DB.DBA.RDF_OBJ</emphasis> is made.
Adding a free-text predicate will add <emphasis>DB.DBA.RDF_OBJ</emphasis> to the list of source tables and a join condition for <emphasis>DB.DBA.RDF_QUAD.O</emphasis> and <emphasis>DB.DBA.RDF_OBJ.RO_DIGEST</emphasis>; and it will add <emphasis>contains (RO_DIGEST, ...)</emphasis> predicate, not <emphasis>contains (O, ...)</emphasis>.
As a result, &quot;you pay only for what you use&quot;: adding free-text index to the declaration does not add tables to the query unless the index is actually used.
</para>
<para>
Boolean functions <function>bif:contains</function> and <function>bif:xcontains</function> are used for objects that come from RDF Views as well as for regular &quot;physical&quot; triples.
Every function gets two arguments and returns a boolean value.
The first argument is an local variable.
The argument variable should be used as an object field in the group pattern where the filter condition is placed.
Moreover, the occurrence of variable in object field should be placed <emphasis>before</emphasis> the filter.
If there are many occurrences of the variable in object fields then the free-text search is associated with the rightmost occurrence that is still to the left from the filter.
The triple pattern that contains the rightmost occurrence is called &quot;intake&quot; of free-text search.
When SPARQL compiler chooses appropriate quad map patterns that may generate data matching intake triple pattern it skips quad map patterns that have no declared free-text indexes, because nothing can be found by free-text search in data that have no free-text index.
Every quad map pattern that has free-text pattern will finally produce an invocation of SQL <link linkend="containspredicate">contains</link> or <link linkend="xcontainspredicate">xcontains</link> predicate, so the whole result of free-text search may be a union of free-text searches from different quad map patterns.
</para>
<para>
The described logic is important only in really complicated cases whereas simple queries are self-evident:
</para>
<programlisting><![CDATA[
select * from <my-dav-graph>
where {
    ?resource a mydav:resource ;
        mydav:resource-content ?text .
    filter (bif:contains (?text, "hello and world")) }
]]></programlisting>
<para>
or, more compact,
</para>
<programlisting><![CDATA[
select * from <my-dav-graph>
where {
    ?resource a mydav:resource ;
        mydav:resource-content ?text .
    ?text bif:contains "hello and world" . }
]]></programlisting>
</sect2>
</sect1>
<sect1 id="rdfsparqlprotocolendpoint"><title>Virtuoso SPARQL Query Service</title>
<sect2 id="rdfsparqlprotocolendpointintro"><title>Introduction</title>
<para>The Virtuoso SPARQL query service implements the <ulink url="http://www.w3.org/TR/rdf-sparql-protocol/">SPARQL Protocol for RDF</ulink>
(W3C Working Draft 25 January 2006) providing SPARQL query processing for RDF data available on the open internet.</para>
<para>The query processor extends the standard protocol to provide support for multiple output formats.
At present this uses additional query parameters.</para>
<para>Supported features include</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>GET and POST requests;</listitem>
<listitem>variety of transfer MIME-types, including RDF /XML and TURTLE;</listitem>
<listitem>default-graph-uri environment parameter;</listitem>
<listitem>all sorts of queries including CONSTRUCT and DESCRIBE.</listitem>
</itemizedlist>
</sect2>

<sect2 id="rdfsupportedprotocolendpointuri"><title>Service Endpoint </title>
<para>Virtuoso reserves the path '/sparql/' and a synonym path '/SPARQL/' for SPARQL service.</para>
<para>In the current implementation, Virtuoso defines virtual directories for HTTP requests that come to the port
specified as 'ServerPort' in the '[HTTPServer]' section of Virtuoso configuration file and refer to one of these
two path strings. So if the Virtuoso installation on host example.com listens for HTTP requests on
port 8080 then client applications should use the 'service endpoint' string equal to 'http://example.com:8080/sparql/'.</para>
<para>Both GET and POST requests are supported by both server and client. The server recognizes the 'Accept: ...' line of request header
to find MIME types preferred by the connected client and adjust the output mode of the response.</para>
<para>The client chooses between GET and POST automatically, using the length of query text as a criterion.</para>
<para>If the SPARQL endpoint is accessed without any URL and query entered, will be shown page with interactive query form.</para>
</sect2>

<sect2 id="rdfsupportedrequestmethodsofprotocol"><title>Request Methods</title>
<table colsep="1" frame="all" rowsep="0" shortentry="0" tocentry="1" tabstyle="decimalstyle" orient="land" pgwide="0">
    <title>Methods List</title>
    <tgroup align="char" charoff="50" char="." cols="3">
    <colspec align="left" colnum="1" colsep="0" colwidth="20pc"/>
    <thead>
      <row>
        <entry>Method</entry>
        <entry>Supported?</entry>
        <entry>Notes</entry>
      </row>
    </thead>
    <tbody>
      <row>
        <entry>GET</entry>
        <entry>Yes</entry>
        <entry>short queries are sent in GET mode</entry>
      </row>
      <row>
        <entry>POST</entry>
        <entry>Yes</entry>
        <entry>Queries longer than 1900 bytes are POST-ed.</entry>
      </row>
      <row>
        <entry>DELETE</entry>
        <entry>No</entry>
        <entry></entry>
      </row>
      <row>
        <entry>PUT</entry>
        <entry>No</entry>
        <entry></entry>
      </row>
    </tbody>
  </tgroup>
</table>
</sect2>

<sect2 id="rdfsparqlclientfunctions"><title>Functions</title>
<para>The SPARQL client can be invoked by three similar functions:</para>

<table colsep="1" frame="all" rowsep="0" shortentry="0" tocentry="1" tabstyle="decimalstyle" orient="land" pgwide="0">
    <title>Functions List</title>
    <tgroup align="char" charoff="50" char="." cols="3">
    <colspec align="left" colnum="1" colsep="0" colwidth="20pc"/>
    <thead>
      <row>
        <entry>Function</entry>
        <entry>Notes</entry>
      </row>
    </thead>
    <tbody>
      <row>
        <entry>DB.DBA.SPARQL_REXEC</entry>
        <entry>behaves like DBA.SPARQL_EVAL, but executes the query on the specified server. The procedure does not return anything. Instead, it creates a result set. </entry>
      </row>
      <row>
        <entry>DB.DBA.SPARQL_REXEC_TO_ARRAY</entry>
        <entry>behaves like DBA.SPARQL_EXEC_TO_ARRAY (), but executes the query on the specified server. The function return a vector of rows, where every row is represented by a vector of values of fields.</entry>
      </row>
      <row>
        <entry>DB.DBA.SPARQL_REXEC_WITH_META</entry>
        <entry>has no local 'SPARQL_EVAL' analog. It produces not only an array of result rows but also array of metadata about result set in a format used by the exec () function. This function can be used when the result should be passed later to exec_result_names () and exec_result () built-in functions. To process local query in similar style, an application can use plain SQL built-in function exec (): an SPARQL query (with 'SPARQL' keyword in front) can be passed to exec () instead of text of plain SQL SELECT statement.</entry>
      </row>
    </tbody>
  </tgroup>
</table>

<programlisting>
create procedure DB.DBA.SPARQL_REXEC (
    in service varchar, in query varchar, in dflt_graph varchar, in named_graphs any,
    in req_hdr any, in maxrows integer, in bnode_dict any );
</programlisting>

<programlisting>
create function DB.DBA.SPARQL_REXEC_TO_ARRAY (
    in service varchar, in query varchar, in dflt_graph varchar, in named_graphs any,
    in req_hdr any, in maxrows integer, in bnode_dict any )
    returns any;
</programlisting>

<programlisting>
create procedure DB.DBA.SPARQL_REXEC_WITH_META (
    in service varchar, in query varchar, in dflt_graph varchar, in named_graphs any,
    in req_hdr any, in maxrows integer, in bnode_dict any,
    out metadata any,  -- metadata like exec () returns.
    out resultset any) -- results as 'long valmode' values.
</programlisting>

</sect2>

<sect2 id="rdfrequestparamsofunctions"><title>Request Parameters</title>
<table colsep="1" frame="all" rowsep="0" shortentry="0" tocentry="1" tabstyle="decimalstyle" orient="land" pgwide="0">
    <title>Request Parameters List</title>
    <tgroup align="char" charoff="50" char="." cols="3">
    <colspec align="left" colnum="1" colsep="0" colwidth="20pc"/>
    <thead>
      <row>
        <entry>Parameter</entry>
        <entry>Notes</entry>
        <entry>Required?</entry>
        <entry>Occurrence</entry>
      </row>
    </thead>
    <tbody>
      <row>
        <entry>service</entry>
        <entry>service URI such as 'http://example.com/sparql/'</entry>
        <entry>Yes</entry>
        <entry></entry>
      </row>
      <row>
        <entry>query</entry>
        <entry>text of the query</entry>
        <entry>Yes</entry>
        <entry></entry>
      </row>
      <row>
        <entry>dflt_graph</entry>
        <entry>default graph URI (string or NULL)</entry>
        <entry>No</entry>
        <entry></entry>
      </row>
      <row>
        <entry>named_graphs</entry>
        <entry>vector of named graphs (or NULL to not override what's specified in the query</entry>
        <entry>Yes</entry>
        <entry></entry>
      </row>
      <row>
        <entry>req_hdr</entry>
        <entry>additional HTTP header lines that should be passed to the service; 'Host: ...' is most popular</entry>
        <entry>No</entry>
        <entry></entry>
      </row>
      <row>
        <entry>maxrows</entry>
        <entry>limit on numbers of rows that should be returned (actual size of result set may differ)</entry>
        <entry>No</entry>
        <entry></entry>
      </row>
      <row>
        <entry>bnode_dict</entry>
        <entry>dictionary of known blank node names, or NULL for usual loading</entry>
        <entry>No</entry>
        <entry></entry>
      </row>
    </tbody>
  </tgroup>
</table>
</sect2>

<sect2 id="rdfresponsecodeofprotocol"><title>Response Codes</title>
<para>If the query is a CONSTRUCT or DESCRIBE then the result set
consists of a single row and a single column, the value inside is a
dictionary of triples in 'long valmode'. Note that the dictionary
object can not be sent to SQL client, say, via ODBC. The client can
loose database connection trying to fetch a result set row that
contain a dictionary object. This disconnect will be safe for server,
so the client can re-connect to the server, but the disconnected
transaction will be rolled back.</para>
</sect2>

<sect2 id="rdfsupportedmimesofprotocol"><title>Response Format</title>
<para>All standard MIME types of SPARQL Protocol are supported:</para>
<table colsep="1" frame="all" rowsep="0" shortentry="0" tocentry="1" tabstyle="decimalstyle" orient="land" pgwide="0">
    <title>Supported MIME Types list</title>
    <tgroup align="char" charoff="50" char="." cols="3">
    <colspec align="left" colnum="1" colsep="0" colwidth="20pc"/>
    <thead>
      <row>
        <entry>Mimetype</entry>
        <entry>Supported for result of Query</entry>
      </row>
    </thead>
    <tbody>
      <row>
        <entry>'application/sparql-results+xml'</entry>
        <entry>SELECT</entry>
      </row>
      <row>
        <entry>'application/sparql-results+xml'</entry>
        <entry>ASK</entry>
      </row>
      <row>
        <entry>'application/rdf+xml'</entry>
        <entry>CONSTRUCT</entry>
      </row>
      <row>
        <entry>'application/rdf+xml'</entry>
        <entry>DESCRIBE</entry>
      </row>
      <row>
        <entry>'text/rdf+n3'</entry>
        <entry>CONSTRUCT</entry>
      </row>
      <row>
        <entry>'text/rdf+n3'</entry>
        <entry>DESCRIBE</entry>
      </row>
    </tbody>
  </tgroup>
</table>
<para>If the HTTP header returned by the remote server does not contain 'Content-Type' line, the MIME type may be guessed from the text of the returned body. </para>
<para>The current implementation does not support results of SELECT returned in RDF /XML, TURTLE or 'sparql-results-2'.</para>
<para>Error messages returned from the service are returned as XML documents, using the mime type application/xml. The documents consist of a single element containing an error message.</para>
</sect2>

<sect2 id="rdfsupportedmimesaddofprotocol"><title>Additional Response Formats -- SELECT</title>
<para>Use the format parameter to select one of the following alternate output formats:</para>

<table colsep="1" frame="all" rowsep="0" shortentry="0" tocentry="1" tabstyle="decimalstyle" orient="land" pgwide="0">
    <title>Additional Response formats list</title>
    <tgroup align="char" charoff="50" char="." cols="3">
    <colspec align="left" colnum="1" colsep="0" colwidth="20pc"/>
    <thead>
      <row>
        <entry>Format Value</entry>
        <entry>Description</entry>
        <entry>Mimetype</entry>
      </row>
    </thead>
    <tbody>
      <row>
        <entry>HTML</entry>
        <entry>HTML document containing query summary and tabular results</entry>
        <entry>text/html</entry>
      </row>
      <row>
        <entry>json</entry>
        <entry>JSON serialization of results. Conforms to the draft specification Serializing SPARQL Query Results in JSON</entry>
        <entry>application/sparql-results+json</entry>
      </row>
      <row>
        <entry>js</entry>
        <entry>Javascript serialization of results. Generates an HTML table with the CSS class sparql. The table contains a column indicating row number and additional columns for each query variable. Each query solution contributes one row of the table. Unbound variables are indicated with a non-breaking space in the appropriate table cells.</entry>
        <entry>application/javascript</entry>
      </row>
      <row>
        <entry>table</entry>
        <entry></entry>
        <entry>text/html</entry>
      </row>
      <row>
        <entry>XML</entry>
        <entry></entry>
        <entry>text/html</entry>
      </row>
      <row>
        <entry>TURTLE</entry>
        <entry></entry>
        <entry>text/html</entry>
      </row>
    </tbody>
  </tgroup>
</table>
</sect2>

<sect2 id="rdfsparqlendpointexamples"><title>Examples</title>
<para>Virtuoso's SPARQL Implementation Demo offers a live demonstration of Virtuoso's implementation of
<ulink url="http://www.w3.org/TR/rdf-dawg-uc/">DAWG's SPARQL test-suite</ulink>;
a collection of SPARQL query language use-cases that enable interactive and simplified testing of a Triple Store's implementation.
Can be found at 'http://example.com:8080/sparql_demo/' depending on the local Virtuoso Server Configuration, or live at <ulink url="http://demo.openlinksw.com/sparql_demo">Virtuoso Demo Server</ulink>.
</para>
<para><emphasis>Simple example with CONSTRUCT:</emphasis></para>
<para>Go to the sparql endpoint UI: i.e. go to http://host:port/sparql</para>
<para>Enter for Default Graph URI: http://www.w3.org/2001/sw/DataAccess/proto-tests/data/construct/simple-data.rdf</para>
<para>Select "Retrieve remote RDF data for all missing source graphs".</para>
<para>Enter as Query:</para>
<programlisting>
SELECT * WHERE {?s ?p ?o}
</programlisting>
<para>Click the "Run Query" button.</para>
<para>As result will be shown the save in the graph http://www.w3.org/2001/sw/DataAccess/proto-tests/data/construct/simple-data.rdf retrieved triples</para>
<programlisting>
s  	                                  p  	                                           o
http://www.example/jose/foaf.rdf#jose 	  http://www.w3.org/1999/02/22-rdf-syntax-ns#type  http://xmlns.com/foaf/0.1/Person
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/nick 	           Jo
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/name 	           Jose Jimen~ez
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/knows 	           http://www.example/jose/foaf.rdf#juan
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/homepage 	           http://www.example/jose/
http://www.example/jose/foaf.rdf#jose 	  http://xmlns.com/foaf/0.1/workplaceHomepage 	   http://www.corp.example/
http://www.example/jose/foaf.rdf#kendall  http://xmlns.com/foaf/0.1/knows                  http://www.example/jose/foaf.rdf#edd
http://www.example/jose/foaf.rdf#julia 	  http://www.w3.org/1999/02/22-rdf-syntax-ns#type  http://xmlns.com/foaf/0.1/Person
http://www.example/jose/foaf.rdf#julia 	  http://xmlns.com/foaf/0.1/mbox 	           mailto:julia@mail.example
http://www.example/jose/foaf.rdf#juan 	  http://www.w3.org/1999/02/22-rdf-syntax-ns#type  http://xmlns.com/foaf/0.1/Person
http://www.example/jose/foaf.rdf#juan 	  http://xmlns.com/foaf/0.1/mbox 	           mailto:juan@mail.example
</programlisting>
<para>Now let's take the CONSTRUCT query:</para>
<programlisting><![CDATA[
PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX myfoaf: <http://www.example/jose/foaf.rdf#>
CONSTRUCT
  { myfoaf:jose foaf:depiction <http://www.example/jose/jose.jpg>.
    myfoaf:jose foaf:schoolHomepage <http://www.edu.example/>.
    ?s ?p ?o.
  }
FROM <http://www.w3.org/2001/sw/DataAccess/proto-tests/data/construct/simple-data.rdf>
WHERE
  {
    ?s ?p ?o. myfoaf:jose foaf:nick "Jo".
    FILTER ( ! (?s = myfoaf:kendall && ?p = foaf:knows && ?o = myfoaf:edd )
    && ! ( ?s = myfoaf:julia && ?p = foaf:mbox && ?o = <mailto:julia@mail.example> )
    && ! ( ?s = myfoaf:julia && ?p = rdf:type && ?o = foaf:Person))
  }
]]></programlisting>
<para>Execute the GET command with the above query added as parameter with encoded value:</para>
<programlisting><![CDATA[
GET -e -s http://host:port/sparql/?query=PREFIX+rdf%3A+%3Chttp%3A%2F%2Fwww.w3.org%2F1999%2F02%2F22-rdf-syntax-ns%23%3E%0D%0APREFIX+foaf%3A+%3Chttp%3A%2F%2Fxmlns.com%2Ffoaf%2F0.1%2F%3E%0D%0APREFIX+myfoaf%3A+%3Chttp%3A%2F%2Fwww.example%2Fjose%2Ffoaf.rdf%23%3E%0D%0A%0D%0ACONSTRUCT+%7B+myfoaf%3Ajose+foaf%3Adepiction+%3Chttp%3A%2F%2Fwww.example%2Fjose%2Fjose.jpg%3E.%0D%0A++++++++++++myfoaf%3Ajose+foaf%3AschoolHomepage+%3Chttp%3A%2F%2Fwww.edu.example%2F%3E.%0D%0A++++++++++++%3Fs+%3Fp+%3Fo.%7D%0D%0AFROM+%3Chttp%3A%2F%2Fwww.w3.org%2F2001%2Fsw%2FDataAccess%2Fproto-tests%2Fdata%2Fconstruct%2Fsimple-data.rdf%3E%0D%0AWHERE+%7B+%3Fs+%3Fp+%3Fo.+myfoaf%3Ajose+foaf%3Anick+%22Jo%22.%0D%0A+++++++FILTER+%28+%21+%28%3Fs+%3D+myfoaf%3Akendall+%26%26+%3Fp+%3D+foaf%3Aknows+%26%26+%3Fo+%3D+myfoaf%3Aedd+%29%0D%0A++++++++++++++%26%26+%21+%28+%3Fs+%3D+myfoaf%3Ajulia+%26%26+%3Fp+%3D+foaf%3Ambox+%26%26+%3Fo+%3D+%3Cmailto%3Ajulia%40mail.example%3E+%29%0D%0A++++++++++%26%26+%21+%28+%3Fs+%3D+myfoaf%3Ajulia+%26%26+%3Fp+%3D+rdf%3Atype+%26%26+%3Fo+%3D+foaf%3APerson%29%29%0D%0A%7D%0D%0A&format=application%2Frdf%2Bxml
]]></programlisting>
<para>As result the response will be:</para>
<programlisting><![CDATA[
200 OK
Connection: close
Date: Fri, 28 Dec 2007 10:06:14 GMT
Accept-Ranges: bytes
Server: Virtuoso/05.00.3023 (Win32) i686-generic-win-32  VDB
Content-Length: 2073
Content-Type: application/rdf+xml; charset=UTF-8
Client-Date: Fri, 28 Dec 2007 10:06:14 GMT
Client-Peer: 83.176.40.177:port
Client-Response-Num: 1

<?xml version="1.0" encoding="utf-8" ?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#">
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#juan"><ns0pred:mbox xmlns:ns0pred="http://xmlns.com/foaf/0.1/" rdf:resource="mailto:juan@mail.example"/></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#jose"><ns0pred:schoolHomepage xmlns:ns0pred="http://xmlns.com/foaf/0.1/" rdf:resource="http://www.edu.example/"/></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#jose"><ns0pred:type xmlns:ns0pred="http://www.w3.org/1999/02/22-rdf-syntax-ns#" rdf:resource="http://xmlns.com/foaf/0.1/Person"/></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#jose"><ns0pred:homepage xmlns:ns0pred="http://xmlns.com/foaf/0.1/" rdf:resource="http://www.example/jose/"/></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#juan"><ns0pred:type xmlns:ns0pred="http://www.w3.org/1999/02/22-rdf-syntax-ns#" rdf:resource="http://xmlns.com/foaf/0.1/Person"/></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#jose"><ns0pred:workplaceHomepage xmlns:ns0pred="http://xmlns.com/foaf/0.1/" rdf:resource="http://www.corp.example/"/></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#jose"><ns0pred:nick xmlns:ns0pred="http://xmlns.com/foaf/0.1/">Jo</ns0pred:nick></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#jose"><ns0pred:depiction xmlns:ns0pred="http://xmlns.com/foaf/0.1/" rdf:resource="http://www.example/jose/jose.jpg"/></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#jose"><ns0pred:name xmlns:ns0pred="http://xmlns.com/foaf/0.1/">Jose Jime?+ez</ns0pred:name></rdf:Description>
<rdf:Description rdf:about="http://www.example/jose/foaf.rdf#jose"><ns0pred:knows xmlns:ns0pred="http://xmlns.com/foaf/0.1/" rdf:resource="http://www.example/jose/foaf.rdf#juan"/></rdf:Description>
</rdf:RDF>
Done
]]></programlisting>

</sect2>

<sect2 id="rdfsparqlendpointimplnotes"><title>Implementation Notes</title>
<para>This service has been implemented using <ulink url="http://docs.openlinksw.com/virtuoso">Virtuoso Server</ulink>.</para>
</sect2>

<sect2 id="rdftables"><title>Virtuoso Semantic Bank end point</title>
    <para>
	<emphasis>What is Piggy Bank?</emphasis>
    </para>
    <para>
	Piggy Bank is an extension to the Firefox Web browser that turns it into a Semantic Web browser, letting you make use of existing information on the Web in more useful and flexible ways not offered by the original Web sites.
    </para>
    <para></para>
    <para>
	<emphasis>What is Semantic Bank?</emphasis>
    </para>
    <para>
	Semantic Bank is the server companion of Piggy Bank that lets you persist, share and publish data collected by individuals, groups or communities. Here is a screen shot of one in action:
    </para>
    <para></para>
    <para></para>
    <para>
	<emphasis>What can I do with this?</emphasis>
    </para>
    <para>
	A Semantic Bank allows you to:
    </para>
    <para></para>
    <para>
	persist your information remotely on a server. This is useful, for example, if you want to share data between two of your computers or to avoid losing it due to mistakes or failure.
    </para>
    <para></para>
    <para>
	share information with other people. The ability to tag resources creates a powerful serendipitous categorization (as proven by things like del.icio.us or Flickr).
    </para>
    <para></para>
    <para>
	lets you publish your information, both in the "pure" RDF form (for those who know how to make use of it) or to regular web pages, with the usual Longwell faceted browsing view of it
    </para>
    <para></para>
    <para></para>
    <para>
	<emphasis>How can I help?</emphasis>
    </para>
    <para>
	Semantic Bank is an open source software and built around the spirit of open participation and collaboration.
    </para>
    <para>
	There are several ways you can help:
    </para>
    <itemizedlist mark="bullet" spacing="compact">
      <listitem>Install a Semantic Bank and lets us know about it, so that we can update the list of available Semantic Banks.</listitem>
      <listitem>Subscribe to our mailing lists to show your interest and give us feedback</listitem>
      <listitem>Report problems and ask for new features through our issue tracking system.</listitem>
      <listitem>Send us patches or fixes to the code</listitem>
    </itemizedlist>
    <para>
	<emphasis>Licensing and Legal Issues</emphasis>
    </para>
    <para>
	Semantic Bank is open source software and is licensed under the BSD license.
    </para>
    <para>
	<emphasis>Note</emphasis>, however, that this software ships with libraries that are not released under the same license; that we interpret their licensing terms to be compatible with ours and that we are redistributing them unmodified. For more information on the licensing terms of the libraries Semantic Bank depends on, please refer to the source code.
    </para>
    <para>
	<emphasis>Download location:</emphasis>
    </para>
    <para>
	<ulink url="http://simile.mit.edu/dist/semantic-bank/">"http://simile.mit.edu/dist/semantic-bank/</ulink>
    </para>
    <para>
	<emphasis>The Virtuoso Semantic Bank end point.</emphasis>
    </para>
    <para>
	Before you can publish, you must register with one or more Semantic Banks:
    </para>
    <itemizedlist mark="bullet" spacing="compact">
      <listitem>Invoke the menu command Tools  Piggy Bank  My Semantic Bank Accounts....</listitem>
      <listitem>Click Add... in the Semantic Bank Accounts dialog box.</listitem>
      <listitem>In the popup dialog box, type in the URL to the Virtuoso Semantic Bank you want to register with. Example: http://server_name:server_port/bank</listitem>
      <listitem>Enter an account a valid Virtuoso DAV user. (Note: currently we do not use encryption during authentication; do not use your precious password here.)</listitem>
      <listitem>Click OK, wait for the account to get registered, and then dismiss the Semantic Bank Accounts dialog box.</listitem>
      <listitem>To publish an item, just click the corresponding Publish button (much like how you save the item). To publish all the items being viewed, click the Publish All button.</listitem>
    </itemizedlist>
    <para>
	<emphasis>What is the Graph Name used by Virtuoso for the Triples from PiggyBank?</emphasis>
    </para>
    <para>
	http://simile.org/piggybank/&lt;piggybank-generated-name&gt;
    </para>
    <para>
	The piggybank-generated-name is Virtuoso DAV user.
    </para>
</sect2>
<sect2 id="rdfsparqlexnpointnorthwindexample"><title>Making RDF View Dereferenceable -- Northwind example</title>
<para>Consider an application that makes some relational data available for SPARQL requests, as described in <link linkend="rdfviewnorthwindexample1">first part of Northwind RDF View example</link>. This may be sufficient for some clients but IRIs of described subjects are not dereferenceable. This means that external SPARQL processors can not retrieve that data using Virtuoso Sponger or the like. This also means that if some external resource refer to IRI of some Northwind subject and a user browses that resource then he can not look at application's data by clicking on the subject link.</para>
<para>To make RDF access complete, application can do the following:</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>create a virtual directory;</listitem>
<listitem>instruct the server how to prepare RDF resources on demand;</listitem>
<listitem>configure rendering of RDF resources for non-RDF clients (including web search engines);</listitem>
<listitem>make the used ontology available;</listitem>
<listitem>provide an index or sitemap page to help users that try to browe published data but do not know proper URLs</listitem>
</itemizedlist>
<para>The following sequence of operations demonstrate how to implement the listed features without writing any special web pages.
All requests (except the application-specific index/sitemap) will be handled by existing web service endpoints.</para>
<para>As a precaution, we erase url rewrite rule list that may be in the database after previous run of the script:</para>
<programlisting><![CDATA[
DB.DBA.URLREWRITE_DROP_RULELIST ('demo_nw_rule_list1', 1)
;
]]></programlisting>
<para>Same for individual rewrite rules:</para>
<programlisting><![CDATA[
DB.DBA.URLREWRITE_DROP_RULE ('demo_nw_rule1', 1)
;
DB.DBA.URLREWRITE_DROP_RULE ('demo_nw_rule2', 1)
;
DB.DBA.URLREWRITE_DROP_RULE ('demo_nw_rule3', 1)
;
DB.DBA.URLREWRITE_DROP_RULE ('demo_nw_rule4', 1)
;
]]></programlisting>
<para>As a sanity check we ensure that there are no more rules named like our rules</para>
<programlisting><![CDATA[
select signal ('WEIRD', sprintf ('Rewrite rule "%s" found', URR_RULE))
from DB.DBA.URL_REWRITE_RULE where URR_RULE like 'demo_nw%'
;

]]></programlisting>
<para>Now we create URI rewrite rules based on regular expressions by calling <link linkend="fn_urlrewrite_create_regex_rule"><function>DB.DBA.URLREWRITE_CREATE_REGEX_RULE</function></link>, so same path will be redirected to different places depending on MIME types the client can accept.</para>
<para>This rule is to construct TURTLE or RDF/XML output of CONSTRUCT. Note dots in regexp for MIME type (<emphasis>rdf.n3</emphasis> and <emphasis>rdf.xml</emphasis> instead of fixed chars) because there exist SPARQL web clients published before the related W3C recommendation, they will produce slightly incorrect &quot;Accept:&quot; string.</para>
<programlisting><![CDATA[
DB.DBA.URLREWRITE_CREATE_REGEX_RULE (
    'demo_nw_rule2',
    1,
    '(/[^#]*)',
    vector('path'),
    1,
    '/sparql?query=CONSTRUCT+{+%%3Chttp%%3A//^{URIQADefaultHost}^%U%%23this%%3E+%%3Fp+%%3Fo+}+FROM+%%3Chttp%%3A//^{URIQADefaultHost}^/Northwind%%3E+WHERE+{+%%3Chttp%%3A//^{URIQADefaultHost}^%U%%23this%%3E+%%3Fp+%%3Fo+}&format=%U',
    vector('path', 'path', '*accept*'),
    null,
    '(text/rdf.n3)|(application/rdf.xml)',
    0,
    null
    );
]]></programlisting>
<note><para>The request URL for SPARQL web service looks terrible due to encoding, the sprintf format string for it is even worse. The easiest way of composing strings of that sort is to open the endpoint page, type desired CONSTRUCT or DESCRIBE statement in the form (using some sample IRI), execute, cut the URL of the page with results from the address line of browser window, paste it to the script and then replace the host name with <emphasis>^{URIQADefaultHost}^</emphasis>, every percent with double percent, some parts of sample IRI with <emphasis>%U</emphasis> and value after <emphasis>&amp;format=</emphasis> with <emphasis>%U</emphasis>; adjust the vector of replacement parameters so that its length is equal to the number of <emphasis>%U</emphasis> or other format specifiers in the template.</para></note>
<para>Next rule is to redirect to the RDF browser that will show the subject description and let the user to explore related subjects.</para>
<programlisting><![CDATA[
DB.DBA.URLREWRITE_CREATE_REGEX_RULE (
    'demo_nw_rule1',
    1,
    '(/[^#]*)',
    vector('path'),
    1,
    '/rdfbrowser/index.html?uri=http%%3A//^{URIQADefaultHost}^%U%%23this',
    vector('path'),
    null,
    '(text/html)|(\\*/\\*)',
    0,
    303
    );
]]></programlisting>
<para>This rule is to remove trailing slash from path. Note that <emphasis>\x24</emphasis> is <emphasis>$</emphasis> for end of line regex pattern, it is written escaped because dollar sign indicates the beginning of macro for ISQL.</para>
<programlisting><![CDATA[
DB.DBA.URLREWRITE_CREATE_REGEX_RULE (
    'demo_nw_rule3',
    1,
    '(/[^#]*)/\x24',
    vector('path'),
    1,
    '%s',
    vector('path'),
    null,
    null,
    0,
    null
    );
]]></programlisting>
<para>This allow the server to describe proper ontology even when the requested IRI contain wrong host name and even if the ontology for <emphasis>http://demo.openlinksw.com/schemas/northwind#</emphasis> is actualy loaded in the database in graph with different IRI (<emphasis>http://demo.openlinksw.com/schemas/NorthwindOntology/1.0/</emphasis>) and the URI of downloadable ontology file differs from both (the file is <emphasis>/DAV/VAD/demo/sql/nw.owl</emphasis>) .</para>
<programlisting><![CDATA[
create procedure DB.DBA.LOAD_NW_ONTOLOGY_FROM_DAV()
{
  declare content1, urihost varchar;
  select cast (RES_CONTENT as varchar) into content1 from WS.WS.SYS_DAV_RES where RES_FULL_PATH = '/DAV/VAD/demo/sql/nw.owl';
  DB.DBA.RDF_LOAD_RDFXML (content1, 'http://demo.openlinksw.com/schemas/northwind#', 'http://demo.openlinksw.com/schemas/NorthwindOntology/1.0/');
  urihost := cfg_item_value(virtuoso_ini_path(), 'URIQA','DefaultHost');
  if (urihost = 'demo.openlinksw.com')
  {
    DB.DBA.VHOST_REMOVE (lpath=>'/schemas/northwind');
    DB.DBA.VHOST_DEFINE (lpath=>'/schemas/northwind', ppath=>'/DAV/VAD/demo/sql/nw.owl', vsp_user=>'dba', is_dav=>1, is_brws=>0);
    DB.DBA.VHOST_REMOVE (lpath=>'/schemas/northwind#');
    DB.DBA.VHOST_DEFINE (lpath=>'/schemas/northwind#', ppath=>'/DAV/VAD/demo/sql/nw.owl', vsp_user=>'dba', is_dav=>1, is_brws=>0);
  }
};

DB.DBA.LOAD_NW_ONTOLOGY_FROM_DAV();

drop procedure DB.DBA.LOAD_NW_ONTOLOGY_FROM_DAV;

DB.DBA.URLREWRITE_CREATE_REGEX_RULE (
    'demo_nw_rule4',
    1,
    '/schemas/northwind#(.*)',
    vector('path'),
    1,
    '/sparql?query=DESCRIBE%20%3Chttp%3A//demo.openlinksw.com/schemas/northwind%23%U%3E%20FROM%20%3Chttp%3A//demo.openlinksw.com/schemas/NorthwindOntology/1.0/%3E',
    vector('path'),
    null,
    '(text/rdf.n3)|(application/rdf.xml)',
    0,
    null
    );
]]></programlisting>
<para>Finally we create the rulelist and define virtual directory <emphasis>/Northwind</emphasis>. Requests that match rewriting rules will be properly redirected and produce the requested data, access to the root will execute default page of the application, namely <emphasis>sfront.vspx</emphasis>.</para>
<programlisting><![CDATA[
DB.DBA.URLREWRITE_CREATE_RULELIST (
    'demo_nw_rule_list1',
    1,
    vector (
                'demo_nw_rule1',
                'demo_nw_rule2',
                'demo_nw_rule3',
                'demo_nw_rule4'
          ));

VHOST_REMOVE (lpath=>'/Northwind');
DB.DBA.VHOST_DEFINE (lpath=>'/Northwind', ppath=>'/DAV/home/demo/', vsp_user=>'dba', is_dav=>1, def_page=>'sfront.vspx',
          is_brws=>0, opts=>vector ('url_rewrite', 'demo_nw_rule_list1'));
]]></programlisting>
</sect2>
<sect2 id="rdfproxyservice"><title>RDF proxy service</title>
    <para>
	In certain cases like for Ajax applications, it's prohibited to issue HTTP requests to another server that origin server.
	In other cases it is needed to transform the content of target to an RDF format. To this purpose the Virtuoso Server provide a RDF proxy service. This service takes as argument target URL and may return content as-is or will try to transform with SPARQL sponger and return RDF data representing the target. In case of transformation to RDF the serialization of the output can be forced by a URL parameter of by content negotiation.
    </para>
    <para>
	Virtuoso reserves the path '/proxy/rdf/' for RDF proxy service. In the current implementation, Virtuoso defines virtual directories for HTTP requests that come to the port specified as 'ServerPort' in the '[HTTPServer]' section of Virtuoso configuration file and refer to the above path string. So if the Virtuoso installation on host example.com listens for HTTP requests on port 8080 then client applications should use the 'service endpoint' string equal to 'http://example.com:8080/proxy/rdf/'.
    </para>
    <para>
	The old pattern '/proxy/' for RDF proxy service is deprecated.
    </para>
    <para><emphasis>Note: </emphasis>In order the RDF proxy service to work correctly, you need to enable SPARQL_UPDATE and grant RDF_SPONGE_UP for SPARQL user:
    </para>
    <para>
       enable SPARQL_UPDATE using Conductor UI:
    </para>
<itemizedlist>
  <listitem>Go to the Virtuoso Administration Conductor i.e. http://host:port/conductor</listitem>
  <listitem>Login as dba user</listitem>
  <listitem>Go to System Admin->User Accounts->Roles</listitem>
  <listitem>Click the link "Edit" for "SPARQL_UPDATE</listitem>
  <listitem>Select from the list of available user/groups "SPARQL" and click the ">>" button so to add it to the right-positioned list.</listitem>
  <listitem>Click the button "Update".</listitem>
</itemizedlist>
    <para>
       grant RDF_SPONGE_UP:
    </para>
<programlisting><![CDATA[
grant execute on DB.DBA.RDF_SPONGE_UP to "SPARQL";
]]></programlisting>
    <para>
	The RDF proxy service takes following URL parameters:
    </para>
    <itemizedlist mark="bullet" spacing="compact">
	<listitem><emphasis>force</emphasis> if 'rdf' is specified will try to extract RDF data from target and return it</listitem>
	<listitem><emphasis>header</emphasis> HTTP headers to be sent to the target</listitem>
	<listitem><emphasis>output-format</emphasis> if 'force=rdf' is given, designate the output MIME type of the RDF data, the default is 'rdf+xml', can be also 'n3' or 'turtle' or 'ttl'.</listitem>
    </itemizedlist>
    <para>
	When no 'output-format' is given and RDF data is asked, the result will be serialized with MIME type depending of 'Accept' header i.e. the proxy service will do content negotiation.
    </para>
    <para>Example: rdf file with URL: http://www.w3.org/People/Berners-Lee/card</para>
<programlisting><![CDATA[
Access the url: http://host:port/proxy/rdf/http://www.w3.org/People/Berners-Lee/card&force=rdf
]]></programlisting>
<para>As result will be shown the rdf file content. Now go to the sparql endpoint, i.e. http://host:port/sparql</para>
<para>Enter for Default Graph URI the url of the rdf file:  http://www.w3.org/People/Berners-Lee/card</para>
<para>Enter for Query:</para>
<programlisting><![CDATA[
SELECT * WHERE {?s ?p ?o}
]]></programlisting>
<para>As result will be shown the saved triples:</para>
<programlisting><![CDATA[
s  	                                        p  	                                           o
http://www.w3.org/People/Berners-Lee/card 	http://www.w3.org/1999/02/22-rdf-syntax-ns#type    http://xmlns.com/foaf/0.1/PersonalProfileDocument
http://www.w3.org/People/Berners-Lee/card 	http://purl.org/dc/elements/1.1/title 	           Tim Berners-Lee's FOAF file
http://www.w3.org/People/Berners-Lee/card 	http://creativecommons.org/ns#license 	           http://creativecommons.org/licenses/by-nc/3.0/
http://www.w3.org/People/Berners-Lee/card 	http://xmlns.com/foaf/0.1/maker 	           http://www.w3.org/People/Berners-Lee/card#i
etc ...
]]></programlisting>
</sect2>
<sect2 id="sparqliniservice"><title>SPARQL ini service</title>
    <para>
	The SPARQL INI section values can be get as RDF via http://cname/sparql?ini
    </para>
    <para>Example: http://demo.openlinksw.com/sparql?ini</para>
<programlisting><![CDATA[
<?xml version="1.0" encoding="utf-8" ?>
<rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:rdfs="http://www.w3.org/2000/01/rdf-schema#">
<rdf:Description rdf:about="http://www.openlinksw.com/schemas/virtini#SPARQL"><ns0pred:MaxQueryCostEstimationTime xmlns:ns0pred="http://www.openlinksw.com/schemas/virtini#">1000</ns0pred:MaxQueryCostEstimationTime></rdf:Description>
<rdf:Description rdf:about="http://www.openlinksw.com/schemas/virtini#SPARQL"><ns0pred:ExternalXsltSource xmlns:ns0pred="http://www.openlinksw.com/schemas/virtini#">1</ns0pred:ExternalXsltSource></rdf:Description>
<rdf:Description rdf:about="http://www.openlinksw.com/schemas/virtini#SPARQL"><ns0pred:DefaultQuery xmlns:ns0pred="http://www.openlinksw.com/schemas/virtini#">select ?Subject ?Concept where {?Subject a ?Concept}</ns0pred:DefaultQuery></rdf:Description>
<rdf:Description rdf:about="http://www.openlinksw.com/schemas/virtini#SPARQL"><ns0pred:ResultSetMaxRows xmlns:ns0pred="http://www.openlinksw.com/schemas/virtini#">100000</ns0pred:ResultSetMaxRows></rdf:Description>
<rdf:Description rdf:about="http://www.openlinksw.com/schemas/virtini#SPARQL"><ns0pred:MaxQueryExecutionTime xmlns:ns0pred="http://www.openlinksw.com/schemas/virtini#">30</ns0pred:MaxQueryExecutionTime></rdf:Description>
<rdf:Description rdf:about="http://www.openlinksw.com/schemas/virtini#SPARQL"><ns0pred:ExternalQuerySource xmlns:ns0pred="http://www.openlinksw.com/schemas/virtini#">1</ns0pred:ExternalQuerySource></rdf:Description>
<rdf:Description rdf:about="http://www.openlinksw.com/schemas/virtini#SPARQL"><ns0pred:DefaultGraph xmlns:ns0pred="http://www.openlinksw.com/schemas/virtini#">http://demo.openlinksw.com/dataspace/person/demo</ns0pred:DefaultGraph></rdf:Description>
<rdf:Description rdf:about="http://www.openlinksw.com/schemas/virtini#SPARQL"><ns0pred:PingService xmlns:ns0pred="http://www.openlinksw.com/schemas/virtini#">http://rpc.pingthesemanticweb.com/</ns0pred:PingService></rdf:Description>
</rdf:RDF>
]]></programlisting>
</sect2>
</sect1>
<sect1 id="sparqlbi"><title>Business Intelligence Extensions for SPARQL</title>
<para>
Virtuoso extends SPARQL with expressions in results, subqueries, aggregates and grouping.
These extensions allow a straightforward translation of arbitrary SQL queries to SPARQL.
This extension is called  "SPARQL BI", because the primary objective is to match needs of Business Intelligence.
The extended features apply equally to querying physical quads or relational tables mapped through RDF views.
</para>
<note><para>In this section, many examples use TPC-H namespace. You may test them on your local demo database.
They use data from TPC-H dataset that is mapped into graph with IRI like
http://example.com/tpch , but you should replace fake host name &quot;example.com&quot; with host name of your own installation,
<strong>verbatim</strong>, that is specified as &quot;DefaultHost&quot; parameter in [URIQA] section of Virtuoso configuration file.</para></note>
<sect2 id="rdfsparqlaggregate"><title>Aggregates in SPARQL</title>
<para>Virtuoso extends SPARQL with SQL like aggregate and group by functionality. This functionality is
also available through embedding SPARQL text inside SQL but the SPARQL extension syntax has the benefit
of working also over the SPARQL protocol and of looking more SPARQL-like.
</para>
<para>The supported aggregates are <emphasis>count</emphasis>, <emphasis>min</emphasis>, <emphasis>max</emphasis>,
<emphasis>avg</emphasis> and <emphasis>sum</emphasis>. These can take an optional <emphasis>distinct</emphasis>
keyword. These are permitted only in the selection part of a select query. If a selection list consists
of a mix of variables and aggregates, the non-aggregate selected items are considered to be grouping
columns and a group by over them is implicitly added at the end of the generated SQL query. There is
no explicit syntax for group by or having in Virtuoso SPARQL.
</para>
<para>If a selection consists of aggregates exclusively, the result set has one row with the values
of the aggregates. If there are aggregates and variables in the selection, the result set as has many
rows as there are distinct combinations of the variables and the aggregates are calculated over each
such distinct combination, as if there were a SQL group by over all non-aggregates.
</para>
<para>With the count aggregate the argument may be either <emphasis>*</emphasis>, meaning counting all rows or a variable
name, meaning counting all the rows where this variable is bound. If there is no implicit <emphasis>group by</emphasis>,
there can be an optional <emphasis>distinct</emphasis> keyword before the variable that is the argument of an aggregate.
</para>
<para>There is a special syntax for counting distinct combinations of selected variables. This is:</para>
<programlisting><![CDATA[
select count distinct ?v1 ... ?vn
  from ....
]]></programlisting>
<para>User-defined aggregate functions are not supported in current version of SPARQL compiler.</para>
<sect3 id="rdfsparqlaggregateexamples"><title>Examples</title>
<programlisting><![CDATA[
sparql
select count (*)
  from <http://mygraph.com>
 where {?s ?p ?o}

-- Returns the count of physical triples in http://mygraph.com.
]]></programlisting>

<programlisting><![CDATA[
sparql define input:inference 'http://mygraph.com'
select ?p count (?o)
  from <http://mygraph.com>
 where {?s ?p ?o};

-- Returns the count of O's for each distinct P.
]]></programlisting>

<programlisting><![CDATA[
sparql define input:inference 'http://mygraph.com'
select count (?p) count (?o) count (distinct ?o)
 from <http://mygraph.com>
where {?s ?p ?o};

-- returns the count of triples, including inferred triples and the count of distinct O values.
]]></programlisting>

<programlisting><![CDATA[
sparql define input:inference 'http://mygraph.com'
select count distinct ?s ?p ?o
  from <http://mygraph.com>
 where {?s ?p ?o};

-- Returns the number of distinct bindings of ?s ?p ?o.
]]></programlisting>
<programlisting><![CDATA[
sparql
prefix tpch: <http://www.openlinksw.com/schemas/tpch#>
select ?status count(*) sum(?extendedprice)
from <http://localhost.localdomain:8310/tpch>
where {
    ?l a tpch:lineitem ;
      tpch:lineextendedprice ?extendedprice ;
      tpch:linestatus ?status .
  };

-- Get counts and total prices of ordered items, grouped by item status.
]]></programlisting>
</sect3>
</sect2>
<sect2 id="rdfsparqlaggregatenote"><title>Note on Aggregates and Inference</title>
<para>Inference is added to a SPARQL query only for the variables whose value is actually used. Thus,
</para>
<programlisting><![CDATA[
select count (*)
 from <http://mygraph.com>
where {?s ?p ?o}
]]></programlisting>
<para>will not make inferred values since s, p, and o are actually not used. Instead,
</para>
<programlisting><![CDATA[
sparql
select count (?s) count (?p) count (?o)
 from <http://mygraph.com>
where {?s ?p ?o};
]]></programlisting>
<para>will get all the inferred triples also.
</para>
<para>
This may be confusing and will likely be corrected in the future.
</para>
</sect2>
<!--
<para>We introduce the <emphasis>sum</emphasis>, <emphasis>count</emphasis>, <emphasis>avg</emphasis>, <emphasis>min</emphasis> and <emphasis>max</emphasis> aggregate functions from SQL.</para>
<para>
Semantics of aggregate functions with respect to unbound variables is inherited from semantics for <emphasis>NULL</emphasis> from SQL.
To count result rows without regard to any value being defined, <emphasis>count (*)</emphasis> can be used.</para>
<para>
If grouping is desired, aggregate expressions can be combined with non-aggregate expressions in a selection list.
The non-aggregate expressions will function as grouping columns, i.e. the aggregates are calculated for each distinct combination of the grouping columns. This makes GROUP BY clauses redundant.
</para> -->
<sect2 id="rdfsparqlarrowop"><title>Pointer Operator (<emphasis>+&gt;</emphasis> and <emphasis>*&gt;</emphasis>)</title>
<para>
When expressions occur in result sets, many variables are introduced only to pass a value from triple pattern to the result expression.
This is inconvenient because many triple patterns are trivial. Big number of variable names masquerade &quot;interesting&quot; variables that are used more than one in pattern part of the query and establish logical relationshops between parts of the query.
As a solution we introduce pointer operators.</para>
<para>
The <emphasis>+&gt;</emphasis> (pointer) operator allows referring to a property value without naming it as a variable and explicit writing triple pattern. We can shorten the example above as</para>
<programlisting><![CDATA[sparql
prefix tpch: <http://www.openlinksw.com/schemas/tpch#>
select ?l+>tpch:linestatus count(*) sum(?l+>tpch:lineextendedprice)
from <http://localhost.localdomain:8310/tpch>
where { ?l a tpch:lineitem }]]></programlisting>
<para>
The <emphasis>?subject+&gt;propertyname</emphasis> notation is exactly equivalent to having a triple pattern <emphasis>?subject propertyname ?aux_var</emphasis> binding an auxiliary variable to the mentioned property of the subject within the group pattern enclosing the reference.
For a SELECT, the enclosing group pattern is considered to be the top level pattern of the where clause or in the event of a union, the top level of each term of the union.
Each distinct pointer adds exactly one triple pattern to the enclosing group pattern, thus multiple uses of <emphasis>+&gt;</emphasis> with same arguments do not each add a triple pattern.
Having multiple copies of an identical pattern might lead to changes in cardinality if multiple input graphs were being considered.
If a lineitem had multiple discounts or extended prices, then we would get the cartesian product of both.
</para>
<para>
If a property referenced via <emphasis>+&gt;</emphasis> is absent, the variable at left side of the operator is not bound in the enclosing group pattern because it should be bound in all triple patterns where it appears as a field, including implicitly added patterns.
The <emphasis>?subject*&gt;propertyname</emphasis> notation is introduced in order to access optional property values. It adds an OPTIONAL group <emphasis>OPTIONAL { ?subject propertyname ?aux_var }</emphasis>, not plain triple pattern, so the binding of ?subject is not changed even if the object variable is not bound. If the property is set for all subjects in question then the results of <emphasis>*&gt;</emphasis> and <emphasis>+&gt;</emphasis> are the same. All other equal, <emphasis>+&gt;</emphasis> operator produces better SQL code than <emphasis>*&gt;</emphasis> so use <emphasis>*&gt;</emphasis> only when it is really needed.</para>
</sect2>
<sect2 id="rdfsparqlnesting"><title>Subqueries in SPARQL</title>
<para>
Pure SPARQL does not allow binding a value that is not retrieved through a triple pattern.
We lift this restriction by allowing expressions in the result set and providing names for result columns.
We also let a sparql SELECT statement to appear in other sparql statement in any place where a group pattern may appear,
so names of result columns become names of variables bound using values from returned rows.
This resembles derived tables in SQL.</para>
<para>
For instance, the following statement finds prices of 1000 order lines with biggest discounts.</para>

<programlisting><![CDATA[sparql
define sql:signal-void-variables 1
prefix tpch: <http://www.openlinksw.com/schemas/tpch#>
select ?line ?discount (?extendedprice * (1 - ?discount)) as ?finalprice
from <http://localhost.localdomain:8310/tpch>
where {
  ?line a tpch:lineitem ;
    tpch:lineextendedprice ?extendedprice ;
    tpch:linediscount ?discount . }
order by desc (?extendedprice * ?discount)
limit 1000;]]></programlisting>

<para>
As soon as we ensure that the query works fine, we can use it in answering more complex question.
Imagine that we want to find how big are customers who has got biggest discounts ever made by company.</para>

<programlisting><![CDATA[sparql
define sql:signal-void-variables 1
prefix tpch: <http://www.openlinksw.com/schemas/tpch#>
select ?cust sum(?extendedprice2 * (1 - ?discount2)) max (?bigdiscount)
from <http://localhost.localdomain:8310/tpch>
where
  {
    {
      select ?line (?extendedprice * ?discount) as ?bigdiscount
      where {
        ?line a tpch:lineitem ;
          tpch:lineextendedprice ?extendedprice ;
          tpch:linediscount ?discount . }
      order by desc (?extendedprice * ?discount)
      limit 1000
    }
    ?line tpch:has_order ?order .
    ?order tpch:has_customer ?cust .
    ?cust tpch:customer_of ?order2 .
    ?order2 tpch:order_of ?line2 .
    ?line2 tpch:lineextendedprice ?extendedprice2 ;
      tpch:linediscount ?discount2 .
  }
order by (sum(?extendedprice2 * (1 - ?discount2)) / max (?bigdiscount));]]></programlisting>

<para>
Looking at the result we can see that customers in question are really big so even big 10% discounts on individual items looks like additional 0.5% to 1.5% discounts from sums of total purchases.</para>
<para>
The inner select finds 1000 biggest (in absolute value) discounts and their order lines. For each line we find order of that line and the customer. For every found customers we find all orders he made and all lines of that orders (variable ?line2). The rest is trivial.</para>
<para>
Note that the inner select does not contains FROM clauses. It is not required because inner select inherits access permissions of all outer queries. It is also important that internal variable bindings of subquery are not visible in outer query, only result set variables are bound, similarly, variables bound in outer query are not accessible inside.</para>
<para>
Note also the declaration <emphasis>define sql:signal-void-variables 1</emphasis> that forces SPARQL compiler to signal errors if some variables can not be bound due to typos in names or attempts of making joins of disjoint domains. This diagnostics is especially important when the query is long.</para>
</sect2>
<sect2 id="rdfsparqlbackq"><title>Expressions in Triple Patterns</title>
<para>In addition to expressions in filters and result sets, Virtuoso allows the use of expressions in triples of
CONSTRUCT pattern or WHERE pattern: an expression can be used instead
of constant or variable name for subject, predicate or object. In this
case, an expression is surrounded by backquotes.</para>

<para>The following return all distinct 'fragment' parts of all subjects in all graphs that have some predicate whose value is equal to 2+2.</para>
<programlisting><![CDATA[
SQL>sparql select distinct (bif:subseq (?s, bif:strchr (?s, '#')))
   where {
     graph ?g {
       ?s ?p `2+2` .
       filter (! bif:isnull (bif:strchr (?s, '#') ) )
     } };

callret
VARCHAR
----------
#four
]]></programlisting>
<para>Inside WHERE part, every expression in a triple pattern is replaced with new variable, a filter expression is added to the enclosing group. The new filter is an equality of new variable and the expression. Hence the sample above is identical to</para>
<programlisting><![CDATA[
sparql
select distinct (bif:subseq (?s, bif:strchr (?s, '#')))
   where {
     graph ?g {
       ?s ?p ?newvariable .
       filter (! bif:isnull (bif:strchr (?s, '#') ) )
       filter (?newvariable = (2+2)) .
     } };
]]></programlisting>
</sect2>	
</sect1>
<sect1 id="sparqldebug"><title>Debugging SPARQL queries</title>

<para>A short SPARQL query can be compiled into a long SQL statement,
especially if data comes from many quad map petterns. Middle-size
application with 50 tables and 10 columns per table may create
thousand of quad map patterns for subjects of hundred different
types. An attempt to &quot;select everything&quot; from RDF view of
that complexity may easily create 5000 lines of SQL code. Thus it is
known in advance that some queries will be rejected even if same
queries work fine on default storage of &quot;physical
quads&quot;.</para>

<para>In addition, an SQL compiler signals an error if table or column
name is unknown, efficiently catching typos. SPARQL uses IRIs that are
long and sometimes unreadable, but there is no &quot;closed
world&quot; schema of the data so a typo in an IRI is not an error, it
is simply some other IRI. So typo in a IRI or in a namespace prefix
cause missing bindings of some triple patterns of the query and
incomplete result, but no errors are usually reported. Typo in graph
or predicate IRI may cause the SPARQL compiler to generate code that
access deafult storage instead of relational source or to generate
empty code that accesses nothing.</para>

<para>The SQL compiler does not signal casting errors when it runs the
statement generated from SPARQL, because the generated SQL code
contains <emphasis>option (QUIETCAST)</emphasis>. This means that
mismatches between expected and actual datatypes of values stay
invisible and may cause rounding errors (say, integer division instead
of floating-point) and even empty joins (due to join conditions that
silently return NULL instead of return comparison error).</para>

<para>In two words, SPARQL queries are so laconic that there is no
room for details that let the compiler distinguish between intention
and a bug. This masquerades query complexity, misuse of names and
mismatches in types. One may make debugging easier by making queries
longer.</para>

<para>Two most helpful debugging tools are automatic void variable
recognition and plain old code inspection. &quot;Automatic&quot; means
&quot;cheap&quot; so the very first step of debugging is to ensure
that every triple pattern of the query may in principle return
something.  This helps in finding typos when the query gets data from
RDF views, this also helps when a query tries to join two disjoint
sorts of subjects.  If <emphasis>define sql:signal-void-variables
1</emphasis> directive is placed at the preambl e of the SPARQL query,
compiler will signal an error if if finds any triple pattern that can
not bind variables or any variable that is proved to be always
unbound. This is especially useful when data are supposed to come from
<emphasis>option (exclusive)</emphasis> or <emphasis>option (soft
exclusive)</emphasis> quad map. Without one of that options SPARQL
compiler will usually bind variables using &quot;physical quad&quot;;
the table of physical quads may contain any rows that match any given
triple pattern; thus many errors will remain undiscovered. If the name
of quad map pattern is known then it is possible to force the SPARQL
compiler to use only that quad map for a whole query or for a
part. This is possible by using the following syntax:</para>

<programlisting><![CDATA[
QUAD MAP quad-map-name { group-pattern }
]]></programlisting>
<para>If some triple pattern inside <emphasis>group-pattern</emphasis> can not be bound using <emphasis>quad-map-name</emphasis> or one of its descendants then <emphasis>define sql:signal-void-variables 1</emphasis> will force the compiler to signal the error.</para>

<note><para>It is technically possible to use <emphasis>QUAD
MAP</emphasis> to improve the performance of query that tries to
access redundant RDF Views but it is much better to achieve the same
effect by providing a more restrictive query or by changing/extending
the RDF View. If an application needs this trick then interoperable SPARQL
clients will experience problems -- they can not use Virtuoso-specific
extensions.</para></note>

<para>
If the automated query checking gives nothing, function <function>sparql_to_sql_text</function> can be used in order to get the SQL text generated from the given query. Its only argument is the text of SPARQL query to compile, without leading SPARQL keyword and semicolon at the end; the returned value is the SQL text. The output may be long but that is the most authoritative source of diagnostic data.
</para>
<para>
When called from ISQL or other ODBC client, return value of <function>sparql_to_sql_text</function> may be transferred as a BLOB so ISQL needs &quot;set blobs on&quot; instruction to avoid data truncation. Even better, the SQL text can be saved into a file:
</para>
<programlisting><![CDATA[
string_to_file ('debug.sql', sparql_to_sql_text ('select * where { graph ?g { ?s a ?type }}'), -2);
]]></programlisting>
<para>(-2 is to overwrite the previous version of the file, as there will be probably many runs of the statement).</para>
<note><para>It is inconvenient to edit query text in order to replace every single quote with two single quotes to make it string constant for <function>sparql_to_sql_text</function>. It is much more convenient to use double quotes in SPARQL queries and replace nothing.</para></note>
<para>As an example, let's find out why the query</para>
<programlisting><![CDATA[
sparql 
prefix northwind: <http://demo.openlinksw.com/schemas/northwind#>
select distinct ?emp
from <http://myhost.example.com/Northwind>
where {
    ?order1 northwind:has_salesrep ?emp ; northwind:shipCountry ?country1 .
    ?order2 northwind:has_salesrep ?emp ; northwind:shipCountry ?country2 .
    filter (?country1 != ?country2) }
]]></programlisting>
<para>is much slower than similar SQL statement. The call of <function>sparql_to_sql_text</function> returns equivalent SQL statement:</para>
<programlisting><![CDATA[
SELECT DISTINCT sprintf_iri ( 'http://myhost.example.com/Northwind/Employee/%U%U%d#this' ,
    /*retval[*/ "s-6-1-t0"."b067b7d~FirstName~0" /* emp */ /*]retval*/ ,
    /*retval[*/  "s-6-1-t0"."b067b7d~FirstName~1" /*]retval*/ ,
    /*retval[*/  "s-6-1-t0"."b067b7d~FirstName~2" /*]retval*/ ) AS /*tmpl*/ "emp"
FROM (SELECT "s-6-1-t0-int~orders"."OrderID" AS /*tmpl*/ "20ffecc~OrderID",
         "s-6-1-t0-int~employees"."FirstName" AS /*as-name-N*/ "b067b7d~FirstName~0",
         "s-6-1-t0-int~employees"."LastName" AS /*as-name-N*/ "b067b7d~FirstName~1",
         "s-6-1-t0-int~employees"."EmployeeID" AS /*as-name-N*/ "b067b7d~FirstName~2"
         FROM Demo.demo.Employees AS "s-6-1-t0-int~employees", Demo.demo.Orders AS "s-6-1-t0-int~orders"
         WHERE /* inter-alias join cond */ 
       "s-6-1-t0-int~orders".EmployeeID = "s-6-1-t0-int~employees".EmployeeID) AS "s-6-1-t0",
    (SELECT "s-6-1-t1-int~orders"."OrderID" AS /*tmpl*/ "20ffecc~OrderID",
        "s-6-1-t1-int~orders"."ShipCountry" AS /*tmpl*/ "e45a7f~ShipCountry"
        FROM Demo.demo.Orders AS "s-6-1-t1-int~orders") AS "s-6-1-t1",
    (SELECT "s-6-1-t2-int~orders"."OrderID" AS /*tmpl*/ "20ffecc~OrderID",
        "s-6-1-t2-int~employees"."FirstName" AS /*as-name-N*/ "b067b7d~FirstName~0",
	"s-6-1-t2-int~employees"."LastName" AS /*as-name-N*/ "b067b7d~FirstName~1",
	"s-6-1-t2-int~employees"."EmployeeID" AS /*as-name-N*/ "b067b7d~FirstName~2"
	FROM Demo.demo.Employees AS "s-6-1-t2-int~employees", Demo.demo.Orders AS "s-6-1-t2-int~orders"
    WHERE /* inter-alias join cond */ 
       "s-6-1-t2-int~orders".EmployeeID = "s-6-1-t2-int~employees".EmployeeID) AS "s-6-1-t2",
    (SELECT "s-6-1-t3-int~orders"."OrderID" AS /*tmpl*/ "20ffecc~OrderID",
        "s-6-1-t3-int~orders"."ShipCountry" AS /*tmpl*/ "e45a7f~ShipCountry"
    FROM Demo.demo.Orders AS "s-6-1-t3-int~orders") AS "s-6-1-t3"
WHERE /* two fields belong to same equiv */ 
    /*retval[*/  "s-6-1-t0"."20ffecc~OrderID" /* order1 */ /*]retval*/  =
    /*retval[*/  "s-6-1-t1"."20ffecc~OrderID" /* order1 */ /*]retval*/ 
    AND /* two fields belong to same equiv */ 
    sprintf_iri ( 'http://myhost.example.com/Northwind/Employee/%U%U%d#this' ,
        /*retval[*/  "s-6-1-t0"."b067b7d~FirstName~0" /* emp */ /*]retval*/ ,
	/*retval[*/  "s-6-1-t0"."b067b7d~FirstName~1" /*]retval*/ ,
	/*retval[*/  "s-6-1-t0"."b067b7d~FirstName~2" /*]retval*/ ) =
    sprintf_iri ( 'http://myhost.example.com/Northwind/Employee/%U%U%d#this' ,
        /*retval[*/  "s-6-1-t2"."b067b7d~FirstName~0" /* emp */ /*]retval*/ ,
	/*retval[*/  "s-6-1-t2"."b067b7d~FirstName~1" /*]retval*/ ,
	/*retval[*/  "s-6-1-t2"."b067b7d~FirstName~2" /*]retval*/ )
    AND /* two fields belong to same equiv */ 
    /*retval[*/  "s-6-1-t2"."20ffecc~OrderID" /* order2 */ /*]retval*/  =
    /*retval[*/  "s-6-1-t3"."20ffecc~OrderID" /* order2 */ /*]retval*/ 
    AND /* filter */ 
   ( /*retval[*/  "s-6-1-t1"."e45a7f~ShipCountry" /* country1 */ /*]retval*/  <>
        /*retval[*/  "s-6-1-t3"."e45a7f~ShipCountry" /* country2 */ /*]retval*/ )
OPTION (QUIETCAST)
]]></programlisting>
<para>The query is next to unreadable but some comments split it into meaningful expressions. Every triple (or list of similar triples) become a subquery that return fields needed to build values of bound variables. That fields are printed outside with comments like

<emphasis>/*retval[*/ expression /* original variable name */
/*]retval*/</emphasis>. Names like<emphasis>"s-6-1-t0"</emphasis>
contain source line number where a group pattern begin (6) and serial
number of triple (0). Comment <emphasis>/* inter-alias join cond
*/</emphasis> means that the expression after that is the condition as
it is written in the declaration of quad map pattern. Comment
<emphasis>/* filter */</emphasis> is before expressions for FILTER
expressions in the source SPARQL. The word &quot;equiv&quot; means
&quot;equivalence class&quot;, i.e. group of occurencies of variables
in the source query that all occurencies are bound to same
value. E.g. when name repeates in many triples of a group, all its
occurencies form an equivalence class. In sone cases compiler can
prove that two variables are always equal even if names differ -- that
variables are also placed into one &quot;equiv&quot;.</para>

<para>Looking at this query, one may notice equality like
<emphasis>sprintf_iri (...) = sprintf_iri (...)</emphasis>. That is
bad because it means that no index will be used to optimize the join
and that there will be one function call per row. When variable
<emphasis>?emp</emphasis> appears in two different triples it means
that the value of the variable is same in both triples. The query
compares IRIs instead of comparing arguments of <link
linkend="fn_sprintf_iri"><function>sprintf_iri</function></link>
because the format string is not proven to be a bijection. Indeed it
can not be bjection for arbitrary strings, but the database is about
real world. If it is known that real names of persons never start with
digit so digits of <emphasis>%d%U</emphasis> fragment will always
be distinguishable from name then the IRI class can be declared as a
bijection even if it is not true for arbitrary strings; the script
can include &quot;suspicious&quot; <emphasis>option (bijection)</emphasis> as
follows:</para>
<programlisting><![CDATA[
create iri class sample:Employee "http://example.com/Employee/%d%U#this"
  (in employee_id integer not null, in employee_lastname varchar not null)
  option (bijection) .
]]></programlisting>
<para>Unfortunately, the attempt of making same trick with the declaration from Northwind example will fail:</para>
<programlisting><![CDATA[
create iri class northwind:Employee "http://^{URIQADefaultHost}^/Northwind/Employee/%U%U%d#this"
  (in employee_firstname varchar not null, in employee_lastname varchar not null, in employee_id integer not null)
  option (bijection) .
]]></programlisting>
<para>Bijection will allow the parsing but it will never give proper result, because first <emphasis>%U</emphasis> will read the whole concatenation of <emphasis>%U%U%d</emphasis>, leave nothing before <emphasis>#this</emphasis> for second <emphasis>%U</emphasis> (that is wrong) and leave nothing for <emphasis>%d</emphasis> (that is explicit parse error, as integer notation can not be empty)</para>.
<para>The string parser will process the string from left to right so it will be unable to parse the string.
The compiler might sometimes report an error if it can prove that the format string is not appropriate for bijection.</para>
<para>The proper way of improving the Northwind example is to make true and reliable bijection by adding strong delimiters:</para>
<programlisting><![CDATA[
create iri class northwind:Employee "http://^{URIQADefaultHost}^/Northwind/Employee/%U/%U/%d#this"
  (in employee_firstname varchar not null, in employee_lastname varchar not null, in employee_id integer not null)
  option (bijection) .
]]></programlisting>
<para>After running the updated script, the query contains three comparisons of fields that were arguments of <function>sprintf_iri</function> in the previous version.</para>
</sect1>
<sect1 id="rdfperformancetuning"><title>Virtuoso RDF Performance Tuning</title>
<para>For RDF query performance, we have the following possible questions:</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>Is the Virtuoso process properly configured to handle big data sets?</listitem>
<listitem>Is the graph always specified?</listitem>
<listitem>Are public web service endpoints protected against bad queries?</listitem>
<listitem>Are there patterns where only a predicate is given?</listitem>
<listitem>Is there a bad query plan because of cost model error?</listitem>
</itemizedlist>
<sect2 id="rdfperfgeneral"><title>General</title>
	<para>For running with large data sets, one should configure the Virtuoso process to use between 2/3 to  3/5 of system RAM and to stripe storage on all available disks.  See <link linkend="VIRTINI">NumberOfBuffers</link> and <link linkend="VIRTINI">Striping</link> ini parameters.</para>
<para> Also, if running with a large database, setting <link linkend="VIRTINI">MaxCheckpointRemap</link> to 1/4th of
 the database size is recommended.  This is in pages, 8K per page.</para>
    </sect2>
    <sect2 id="rdfperfindexes"><title>Index Scheme Selection</title>
<para>If the graph is always given, as one or more <emphasis>FROM</emphasis> or <emphasis>FROM NAMED</emphasis>, and there are no patterns where only graph and predicate are given, then the default indices should be appropriate.
If the predicate and graph are given but subject is not, then it is sometimes useful to add</para>

<programlisting><![CDATA[
create bitmap index RDF_QUAD_PGOS on DB.DBA.RDF_QUAD (G, P, O, S) partition (O varchar (-1, 0hexffff));
]]></programlisting>

<note><para>If the server is pre 5.0.7, leave out the partitioning clause.</para></note>

<para>Making the PGOS index can help in some cases even if it is not readily apparent from the queries that one is needed.  This is so for example if the predicate by itself is selective, i.e. there is a predicate that occurs in only a few triples.</para>
<para>If the graph itself is not given in the queries, then the default index scheme will be unworkable.
For this, the appropriate scheme is:</para>

<programlisting><![CDATA[
create table RDF_QUAD (G iri_id_8, S iri_id_8, P iri_id_8, O any, primary key (S, P, O, G))
alter index RDF_QUAD on RDF_QUAD partition (S int (0hexffff00));
create bitmap index RDF_QUAD_OPGS on DB.DBA.RDF_QUAD (O, P, G, S) partition (O varchar (-1, 0hexffff));
create bitmap index RDF_QUAD_POGS on DB.DBA.RDF_QUAD (P, O, G, S) partition (O varchar (-1, 0hexffff));
create bitmap index RDF_QUAD_GPOS on DB.DBA.RDF_QUAD (G, P, O, S) partition (O varchar (-1, 0hexffff));
]]></programlisting>

<note><para>For a pre 5.0.7 server, leave the partition clauses and the alter index statement out.</para></note>

<para>If there are existing triples and one does not wish to reload them, then the following sequence will convert the data:</para>

<programlisting><![CDATA[
log_enable (2);
drop index RDF_QUAD_OGPS;
checkpoint;
create table R2 (G iri_id_8, S iri_id_8, P iri_id_8, O any, primary key (S, P, O, G))
alter index R2 on R2 partition (S int (0hexffff00));

insert into r2 (g, s, p, o) select g, s, p, o from rdf_quad;

drop table RDF_QUAD;
checkpoint;
alter table r2 rename RDF_QUAD;
create bitmap index RDF_QUAD_OPGS on DB.DBA.RDF_QUAD (O, P, G, S) partition (O varchar (-1, 0hexffff));
create bitmap index RDF_QUAD_POGS on RDF_QUAD (P, O, G, S) partition (O varchar (-1, 0hexffff));
create bitmap index RDF_QUAD_GPOS on RDF_QUAD (G, P, O, S) partition (O varchar (-1, 0hexffff));
checkpoint;
log_enable (1);
]]></programlisting>

<para>First drop the OGPS index to make space.  Then, in row autocommit mode
and without logging, copy the quads into a new primary key layout.
Drop the old and rename the new over the old.  Make the additional
indices.  Do a checkpoint after the drops so as to actually free the
space also in the checkpointed state.  Finish with a checkpoint so as
to finalize the changes, since logging was turned off.  Even if
logging had been on, one would not wish to have to replay the
reindexing if the server terminated abnormally.
Finally turn logging back on for the session.</para>

<note><para>This is all meant to be done with a SQL client like isql and not through a web interface.
The web interface has no real session and the log_enables do nothing there.</para></note>

<para>Other indexing schemes may be tried.  We note however that in all
cases, one or other of the indices should begin with G.  This is
because for schema operations it is necessary to read through a
graph. If no index begins with G, this becomes a full table scan and
is unworkable, leading to an extremely slow server start and making
operations like drop graph as good as unusable.</para>

<para>Public web service endpoints are proven to be sources of especially bad queries. While local application develpers can obtain instructions from database administrator and use ISQL access to the database in order to tune execution plans, &quot;external&quot; clients do not know details of configuration and/or lacks appropriate skills. The most common problem is that public endpoints usually get requests that does not mention the required graph, because that queries were initially written for use with triple stores. If the web service provides access to a single graph (or to a short list of graphs) then it is strongly recommended to configure it by adding a row into <emphasis>DB.DBA.SYS_SPARQL_HOST</emphasis>. The idea is that if the client specifies default graph in the request or uses named graphs and group graph patterns then he is probably smarter than average and will provide meaningful queries. If no graph names are specified then the query will benefit from preset graph because this will give the compiler some more indexes to choose from -- indexes taht begin with G.</para>
<para>Sometimes web service endpoint is used to access data of only one application, not all data in the system. In that case one may wish to declare a separate storage that consists of only RDF Views made by that application and define <emphasis>input:storage</emphasis> in appropriate row of <emphasis>DB.DBA.SYS_SPARQL_HOST</emphasis>.</para>
</sect2>
<sect2 id="rdfperfcost"><title>Erroneous Cost Estimates and Explicit Join Order</title>

	<para>The selectivity of triple patterns is determined at query compile time from sampling the data.
It is possible that misleading data is produced.
To see if the cardinality guesses are generally valid, look at the query plan with <link linkend="fn_explain"><function>explain</function> ()</link>.</para>
<para>Below is a sample from the LUBM qualification data set in the Virtuoso distribution.
After running <emphasis>make test</emphasis> in <emphasis>binsrc/tets/lubm</emphasis>, there is a loaded database with the data.
Start a server in the same directory to see the data.</para>

<programlisting><![CDATA[
SQL> explain ('sparql prefix ub: <http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#>
select * from <lubm>
where { ?x rdf:type ub:GraduateStudent }');

REPORT
VARCHAR
_______________________________________________________________________________

{ 
 
Precode:
      0: $25 "callret" := Call __BOX_FLAGS_TWEAK (<constant (lubm)>, <constant (1)>)
      5: $26 "lubm" := Call DB.DBA.RDF_MAKE_IID_OF_QNAME_SAFE ($25 "callret")
      12: $27 "callret" := Call __BOX_FLAGS_TWEAK (<constant (http://www.w3.org/1999/02/22-rdf-syntax-ns#type)>, <constant (1)>)
      17: $28 "-ns#type" := Call DB.DBA.RDF_MAKE_IID_OF_QNAME_SAFE ($27 "callret")
      24: $29 "callret" := Call __BOX_FLAGS_TWEAK (<constant (http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#GraduateStudent)>, <constant (1)>)
      29: $30 "owl#GraduateStudent" := Call DB.DBA.RDF_MAKE_IID_OF_QNAME_SAFE ($29 "callret")
      36: BReturn 0
from DB.DBA.RDF_QUAD by RDF_QUAD_OGPS    1.9e+03 rows
Key RDF_QUAD_OGPS  ASC ($32 "s-3-1-t0.S")
<col=415 O = $30 "owl#GraduateStudent"> , <col=412 G = $26 "lubm"> , <col=414 P = $28 "-ns#type">
row specs: <col=415 O LIKE <constant (T)>>
 
Current of: <$34 "<DB.DBA.RDF_QUAD s-3-1-t0>" spec 5>
 
After code:
      0: $35 "x" := Call ID_TO_IRI ($32 "s-3-1-t0.S")
      5: BReturn 0
Select ($35 "x", <$34 "<DB.DBA.RDF_QUAD s-3-1-t0>" spec 5>)
}

22 Rows. -- 1 msec.
]]></programlisting>
<para>
This finds the graduate student instances in the lubm graph.  First
the query converts the IRI literals to id's.  Then, using a match of
OG on OGPS it finds the IRI's of the graduate students.  Then it
converts the IRI id to return to the string form.</para>
<para>The cardinality estimate of 1.9e+03 rows is on the FROM line.</para>
<para>Doing an explain on the queries will show the cardinality estimates.  To drill down further, one can split the query into smaller chunks and see the estimates for these, up to doing it at the triple pattern level.  
To indicate a variable that is bound but whose value is not a literal known at compile time, one can use the parameter marker <emphasis>??</emphasis>.</para>

<programlisting><![CDATA[
explain ('sparql define sql:table-option "order"  prefix ub: <http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#>
select * from <lubm>
where { ?x rdf:type ?? }');
]]></programlisting>

<para>This will not know the type but will know that a type will be
provided.  So instead of guessing 1900 matches, this will guess a
smaller number, which is obviously less precise.  Thus literals are generally better.</para>
<para>In some cases, generally to work around an optimization error, one can specify an explicit join order. 
This is done with the sql:select-option "order"  clause in the SPARQL query prefix.</para>

<programlisting><![CDATA[
select sparql_to_sql_text (' define sql:select-option "order" prefix ub: <http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#>
select * from <lubm>
where { ?x rdf:type ub:GraduateStudent . ?x ub:takesCourse <http://www.Department0.University0.edu/GraduateCourse0> }');
]]></programlisting>

<para>shows the SQL text with the order option at the end.</para>
<para>If an estimate is radically wrong then this should be reported as a bug.</para>
<para>If there is a FROM with a KEY on the next line and no column specs then this is a full table scan. The more columns are specified the less rows will be passed to the next operation in the chain. In the example above, there are three columns whose values are known before  reading the table and these columns are leading columns of the index in use so column specs are</para>
<programlisting><![CDATA[
<col=415 O = $30 "owl#GraduateStudent"> , <col=412 G = $26 "lubm"> , <col=414 P = $28 "-ns#type">
]]></programlisting>
<note><para>A KEY with only a row spec is a full table scan with the row spec applied as a filter.
This is usually not good unless this is specifically intended.</para></note>
<para>If queries are compiled to make full table scans when this is not specifically intended, this should be reported as a bug.
The explain output and the query text should be included in the report.</para>
<para>An explicit join order is specified by the <emphasis>define sql:select-option "order"</emphasis> clause in the SPARQL query prefix:
Consider:</para>

<programlisting><![CDATA[
explain ('sparql define sql:select-option "order, loop" prefix ub: <http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl#>
select * from <lubm>
where { ?x ub:takesCourse ?c . ?x rdf:type ub:GraduateStudent   }');
]]></programlisting>

<para>One will see in the output that the first table access is to retrieve
all in the lubm graph which take some course and then later to check
if this is a graduate student.  This is obviously not the preferred
order but the <emphasis>sql:select-option "order"</emphasis> forces the optimizer to join
from left to right.</para>
<para>It is very easy to end up with completely unworkable query plans in
this manner but if the optimizer really is in error, then this is the
only way of overriding its prefrences.  The effect of <emphasis>sql:select-option</emphasis> is pervasive, extending inside unions, optionals, subqueries etc within the statement.</para>
<para>We note that if, in the above query, both the course taken by the
student and the type of the student are given, the query compilation
will be, at least for all non-cluster cases, an index intersection.
This is not overridden by the sql:select-option clause since an index
intersection is always a safe guess, regardless of the correctness of
the cardinality guesses of the patterns involved.</para>
</sect2>
    <sect2 id="rdfperfloading"><title>Loading</title>
	<para>There are many functions for loading RDF text, in RDF/XML and Turtle.</para>
	<para>For loading RDF/XML, the best way is to ssplit the data to be loaded into multiple streams and load these in parallel using <link linkend="fn_rdf_load_rdfxml"><function>rdf_load_rdfxml ()</function></link>. To avoid running out of rollback space for large files and in order to have multiple concurrent loads not interfere with each other, the row autocommit mode should be enabled.</para>
	<para>For example, </para>
<programlisting><![CDATA[
log_enable (2);
-- switch row-by-row autocommit on and logging off for this session
db..rdf_load_rdfxml (file_to_string_output ('file.xml'), 'base_uri', 'target_graph');
-- more files here ...
checkpoint;
]]></programlisting>
<para>Loading a file with text like the above with isql will load the data.  Since the transaction logging is off, make a manual checkpoint at the end to ensure that data is persisted upon server restart since there is no roll forward log.</para>
<para>If large amounts of data are to be loaded, run multiple such streams in parallel. One may have for example 6 streams for 4 cores. This
means that if up to two threads wait for disk, there is still work
for all cores.</para>
<para>Having substantially more threads than processors or disks is not particularly useful.</para>
<para>There exist multithreaded load functions which will load one file on multiple threads.  Experience shows that loading multiple files on one thread per file is better.</para>
<para>For loading Turtle, some platforms may have a non-reentrant Turtle parser.  This means that only one load may run at a time.  One can try this by calling <link linkend="rdfapidataimport"><function>ttlp ()</function></link> from two sessions at the same time. If these do not execute concurrently, then the best way may be to try <link linkend="rdfapidataimport"><function>ttlp_mt</function></link> and see if this runs faster than a single threaded ttlp call.</para>
       <sect3 id="rdfperfloadingunitpro"><title>Loading UniProt RDF data</title>
<para>To load the uniprot data, create a function for example such as:</para>
<programlisting><![CDATA[
create function DB.DBA.UNIPROT_LOAD (in log_mode integer := 1)
{
  DB.DBA.RDF_LOAD_RDFXML_MT (file_to_string_output('filename1'),'http://base_uri_1', 'destination_graph_1', log_mode, 3);
  DB.DBA.RDF_LOAD_RDFXML_MT (file_to_string_output('filename2'),'http://base_uri_2', 'destination_graph_2', log_mode, 3);
  ...
  DB.DBA.RDF_LOAD_RDFXML_MT (file_to_string_output('filename9'),'http://base_uri_9', 'destination_graph_9', log_mode, 3);
}
]]></programlisting>
<para>If you are starting from blank database and you can drop it and re-create in case of error signaled, use it this way:</para>
<programlisting><![CDATA[
checkpoint;
checkpoint_interval(6000);
DB.DBA.UNIPROT_LOAD (0),
checkpoint;
checkpoint_interval(60);
]]></programlisting>
<para>If the database contains important data already and there's no way to stop it and backup before the load then use:</para>
<programlisting><![CDATA[
checkpoint;
checkpoint_interval(6000);
DB.DBA.UNIPROT_LOAD (),
checkpoint;
checkpoint_interval(60);
]]></programlisting>
<para>Note that the 'number of threads' parameter of DB.DBA.RDF_LOAD_RDFXML() mentions threads
used to process data from file, an extra thread will read the text and parse it,
so for 4 CPU cores there's no need in parameter value greater than 3. Three processing
threads per one parsing tread is usually good ratio because parsing is usually three
times faster than the rest of loading so CPU loading is well balanced.
If for example you are using 2 x Quad Xeon, then you can choose between 8
single-threaded parsers or 2 parsers with 3 processing threads each. With 4 cores you may simply load
file after file with 3 processing threads. The most important performance tuning is to set the
[Parameters] section of virtuoso configuration file:</para>
<programlisting><![CDATA[
NumberOfBuffers = 1000000
MaxDirtyBuffers = 800000
MaxCheckpointRemap = 1000000
DefaultIsolation = 2
]]></programlisting>
<para>Note: these numbers are reasonable for 16 GB RAM Linux box. Usually when there are no such massive operations as loading huge database, you can set up the values as:</para>
<programlisting><![CDATA[
NumberOfBuffers = 1500000
MaxDirtyBuffers = 1200000
MaxCheckpointRemap = 1500000
DefaultIsolation = 2
]]></programlisting>
<para>Tip: Thus after loading all data you may wish to shutdown, tweak and start server again.
If you have ext2fs or ext3fs filesystem, then it's better to have enough free space on disk not to
make it more than 80% full.  When it's almost full it may allocate database file badly, resulting
in measurable loss of disk access speed. That is not Virtuoso-specific fact, but a common hint
for all database-like applications with random access to big files.</para>
<para>Here is an example of using awk file for splitting big file smaller ones:</para>
<programlisting><![CDATA[
BEGIN {
	file_part=1000
	e_line = "</rdf:RDF>"
        cur=0
        cur_o=0
	file=0
	part=file_part
      }
	{
	    res_file_i="res/"FILENAME
	    line=$0
	    s=$1
	    res_file=res_file_i"_"file".rdf"

	    if (index (s, "</rdf:Description>") == 1)
	    {
		cur=cur+1
		part=part-1
	    }

	    if (part > 0)
	    {
	    	print line >> res_file
	    }

	    if (part == 0)
	    {
#		print "===================== " cur
	    	print line >> res_file
		print e_line >> res_file
		close (res_file)
		file=file+1
		part=file_part
	    	res_file=res_file_i"_"file".rdf"
		system ("cp beg.txt " res_file)
	    }
        }
END { }
]]></programlisting>
       </sect3>
       <sect3 id="rdfperfloadingdbpedia"><title>Loading DBPedia RDF data</title>
<para>You can use the following script as an example for loading DBPedia RDF data in Virtuoso:</para>
<programlisting><![CDATA[
#!/bin/sh

LOG=load_dbpedia.log
#rm -f $LOG

#Directory structore
mkdir BAD   2>> /dev/null
mkdir READY 2>> /dev/null

ISQL=${ISQL-isql}
PORT=${PORT-1118}
DATABASE_USER=${DATABASE_USER-dba}
DATABASE_PASS=${DATABASE_PASS-dba}
DATABASE_INI=${DATABASE_INI-dbpedia.ini}

DSN="$ISQL $PORT $DATABASE_USER $DATABASE_PASS"
export LOG DSN DATABASE_INI

STAR_NEW_SERVER ()
{
	rm -rf core *.lck   2>> /dev/null
	echo "Starting OpenLink Virtuoso Universal Server"
        virtuoso-iodbc-t +wait -c $DATABASE_INI
	sleep 5
	echo "Starting OpenLink Virtuoso Universal Server - Done."
	STATUS=1
}

CHECK_LOG()
{
    rline=`cat temp.res | grep -i "TURTLE RDF loader"`
    if test "x$rline" != "x"
    then
	echo "BAD FILE. " $1 | tee -a $LOG
	echo $bad_line
	bad_line=`cat temp.res | grep "TURTLE RDF loader" | sed -e "s/\*\*\* Error 37000: \[Virtuoso Driver\]\[Virtuoso Server\]SP029: TURTLE RDF loader, line //g" | cut -f 1 -d :`
	echo "Bad line:" $bad_line " from " $1
	mv $1 BAD
    fi

    rline=`cat temp.res | grep -i "Lost connection to server"`
    if test "x$rline" != "x"
    then
	echo "GPF !!! in " $1 | tee -a $LOG
	STAR_NEW_SERVER
	return
    fi

    rline=`cat temp.res | grep -i "Connect failed"`
    if test "x$rline" != "x"
    then
	STAR_NEW_SERVER
	return
    fi

    sleep 1
    STATUS=0
}

echo "======================================="
echo "Loading dbpedia started."
echo "======================================="

LOAD_ONE ()
{
    FINISH=0
    for f in `find data -name '*.nt'`
    do
	du -h data READY $f
	#   LOAD DATA
	echo "Loading $f (`cat $f | wc -l` lines) `date \"+%H:%M:%S\"`" | tee -a $LOG
	$DSN verbose=on banner=off prompt=off echo=ON errors=stdout  \
	exec="ttlp_mt (file_to_string_output ('$f'), '', 'http://dbpedia.org');" > temp.res
	cat temp.res >> $LOG
	CHECK_LOG $f
	if test $STATUS -ne 0
	then
	    return
	fi

	#   CHECKPOINT
	echo "Checkpoint start at: `date \"+%H:%M:%S\"`" | tee -a $LOG
	$DSN verbose=on banner=off prompt=off echo=ON errors=stdout exec="checkpoint;" > temp.res
	cat temp.res >> $LOG

	CHECK_LOG $f
	if test $STATUS -ne 0
	then
	    return
	fi
	$DSN verbose=on banner=off prompt=off echo=ON errors=stdout  \
	exec="sparql select count(*) where { ?s <http://dbpedia.org/property/wordnet_type> ?o};" > temp.res
	cat temp.res >> $LOG
	CHECK_LOG $f
	echo "Finish.  "
	echo "======================================="
	mv $f READY 2>> /dev/null
	rm temp.res
	return
    done

    FINISH=1
}

while true
do
  LOAD_ONE
  if test $FINISH -ne 0
  then
      echo "If you read this you have a red letter day !!!"
      echo "Check BAD directory for bad skiped files."
      echo "======================================="
      exit 0
  fi
done
]]></programlisting>
       </sect3>
       <sect3 id="rdfperfloadingbio2rdf"><title>Loading Bio2RDF data</title>
<para>The shell script below was used to import files in n3 notation into OpenLink Virtuoso RDF storage.</para>
<para>When an syntax error it will cut content from next line and will retry. This was used on ubuntu linux to import bio2rdf and freebase dumps.</para>
<para>Note it uses gawk, so it must be available on system where is tried. Also for recovery additional disk space is needed at max the size of original file.</para>
<programlisting><![CDATA[
#!/bin/bash

PASS=$1
f=$2
g=$3

# Usage
if [ -z "$PASS" -o -z "$f" -o -z "$g" ]
then
  echo "Usage: $0 [password] [ttl-file] [graph-iri]"
  exit
fi

if [ ! -f "$f" ]
then
    echo "$f does not exists"
    exit
fi

# Your port here
PORT=1111  #`inifile -f dbpedia.ini -s Parameters -k ServerPort`
if test -z "$PORT"
then
    echo "Cannot find INI and inifile command"
    exit
fi

# Initial run
isql $PORT dba $PASS verbose=on banner=off prompt=off echo=ON errors=stdout exec="ttlp_mt (file_to_string_output ('$f'), '', '$g'); checkpoint;" > $0.log

# If disconnect etc.
if [ $? != 0 ]
then
    echo "An error occured, please check $0.log"
    exit
fi

# Check for error
line_no=`grep Error $0.log | awk '{ match ($0, /line [0-9]+/, x) ; match (x[0], /[0-9]+/, y); print y[0] }'`
newf=$f.part
inx=1

# Error recovery
while [ ! -z "$line_no" ]
do
    cat $f |  awk "BEGIN { i = 0 } { if (i==$line_no) { print \$0; exit; } i = i + 1 }"  >> bad.nt
    line_no=`expr $line_no + 1`
    echo "Retryng from line $line_no"
    cat $f |  awk "BEGIN { i = 0 } { if (i>=$line_no) print \$0; i = i + 1 }"  > tmp.nt
    mv tmp.nt $newf
    f=$newf
    mv $0.log $0.log.$inx
    # Run the recovered part
    isql $PORT dba $PASS verbose=on banner=off prompt=off echo=ON errors=stdout exec="ttlp_mt (file_to_string_output ('$f'), '', '$g'); checkpoint;" > $0.log

    if [ $? != 0 ]
    then
	echo "An error occured, please check $0.log"
	exit
    fi
   line_no=`grep Error $0.log | awk '{ match ($0, /line [0-9]+/, x) ; match (x[0], /[0-9]+/, y); print y[0] }'`
   inx=`expr $inx + 1`
done
]]></programlisting>

       </sect3>
    </sect2>
    <sect2 id="rdfperfsparul"><title>Using SPARUL</title>
	<para>Since SPARUL updates are generally not ment to be transactional, it is
	    best to run these in <link linkend="fn_log_enable"><function>log_enable (2)</function></link> mode,
	    which commits every operation as it is done.  This prevents one from running out of rollback space.  Also for bulk updates, transaction logging can be turned off.  If so, one should do a manual checkpoint after the operation to ensure persistence across server restart since there is no roll forward log.</para>
<para>To have a roll forward log and row by row autocommit, one may use <link linkend="fn_log_enable"><function>log_enable (3)</function></link>.  This will write constantly into the log which takes extra time.  Having no logging and doing a checkpoint when the whole work is finished is faster.</para>
<para>Many SPARUL operations can be run in parallel in this way.  If they are independent with respect to their input and output, they can run in parallel and row by row autocommit will ensure they do not end up waiting for each others' locks.</para>
    </sect2>
    <sect2 id="rdfperfgeneraldbpedia"><title>DBpedia Benchmark</title>
<para>We ran the DBpedia benchmark queries again with different configurations of Virtuoso.
Comparing numbers given by different parties is a constant problem. In the case reported here,
we loaded the full DBpedia 3, all languages, with about 198M triples, onto Virtuoso v5 and Virtuoso Cluster
v6, all on the same 4 core 2GHz Xeon with 8G RAM. All databases were striped on 6 disks. The Cluster
configuration was with 4 processes in the same box.
We ran the queries in two variants:
</para>
<itemizedlist>
  <listitem>With graph specified in the SPARQL FROM clause, using the default indices.</listitem>
  <listitem>With no graph specified anywhere, using an alternate indexing scheme.</listitem>
</itemizedlist>
<para>The times below are for the sequence of 5 queries.
As there is a query in the set that specifies no condition on S or O and only P,
thus cannot be done with the default indices With Virtuoso v5. With Virtuoso Cluster v6 it can,
because v6 is more space efficient. So we added the index:</para>
<programlisting><![CDATA[
create bitmap index rdf_quad_pogs on rdf_quad (p, o, g, s);
]]></programlisting>
<table>
<tgroup cols="4">
<thead>
<row>
  <entry></entry>
  <entry>Virtuoso v5 with  gspo, ogps, pogs</entry>
  <entry>Virtuoso Cluster v6 with gspo, ogps</entry>
  <entry>Virtuoso Cluster v6 with gspo, ogps, pogs</entry>
</row>
</thead>
<tbody>
<row><entry>cold</entry><entry>210 s</entry><entry>136 s</entry><entry>33.4 s</entry></row>
<row><entry>warm</entry><entry>0.600 s</entry><entry>4.01 s</entry><entry>0.628 s</entry></row>
</tbody>
</tgroup>
</table>
<para>Now let us do it without a graph being specified. Note that alter index is valid for v6 or higher.
For all platforms, we drop any existing indices, and:</para>
<programlisting><![CDATA[
create table r2 (g iri_id_8, s, iri_id_8, p iri_id_8, o any, primary key (s, p, o, g))
alter index R2 on R2 partition (s int (0hexffff00));

log_enable (2);
insert into r2 (g, s, p, o) select g, s, p, o from rdf_quad;

drop table rdf_quad;
alter table r2 rename RDF_QUAD;
create bitmap index rdf_quad_opgs on rdf_quad (o, p, g, s) partition (o varchar (-1, 0hexffff));
create bitmap index rdf_quad_pogs on rdf_quad (p, o, g, s) partition (o varchar (-1, 0hexffff));
create bitmap index rdf_quad_gpos on rdf_quad (g, p, o, s) partition (o varchar (-1, 0hexffff));
]]></programlisting>
<para>The code is identical for v5 and v6, except that with v5 we use iri_id (32 bit) for
the type, not iri_id_8 (64 bit). We note that we run out of IDs with v5 around a few billion
triples, so with v6 we have double the ID length and still manage to be vastly more space efficient.</para>
<para>With the above 4 indices, we can query the data pretty much in any combination without hitting
a full scan of any index. We note that all indices that do not begin with s end with s as a bitmap.
This takes about 60% of the space of a non-bitmap index for data such as DBpedia.</para>
<para>If you intend to do completely arbitrary RDF queries in Virtuoso, then chances are
you are best off with the above index scheme.</para>
<table>
<tgroup cols="3">
<thead>
<row>
  <entry></entry>
  <entry>Virtuoso v5 with  gspo, ogps, pogs</entry>
  <entry>Virtuoso Cluster v6 with gspo, ogps, pogs</entry>
</row>
</thead>
<tbody>
<row><entry>warm</entry><entry>0.595 s</entry><entry>0.617 s</entry></row>
</tbody>
</tgroup>
</table>
<para>The cold times were about the same as above, so not reproduced.</para>
<para>It is in the SPARQL spirit to specify a graph and for pretty much any application,
there are entirely sensible ways of keeping the data in graphs and specifying which ones are
concerned by queries. This is why Virtuoso is set up for this by default.</para>
<para>On the other hand, for the open web scenario, dealing with an unknown large number of graphs,
enumerating graphs is not possible and questions like which graph of which source asserts x become
relevant. We have two distinct use cases which warrant different setups of the database, simple as that.</para>
<para>The latter use case is not really within the SPARQL spec, so implementations may or may not
support this.</para>
<para>Once the indices are right, there is no difference between specifying a graph and not specifying a
graph with the queries considered. With more complex queries, specifying a graph or set of graphs does
allow some optimizations that cannot be done with no graph specified. For example, bitmap intersections
are possible only when all leading key parts are given.</para>
<para>The best warm cache time is with v5; the five queries run under 600 ms after the first go.
This is noted to show that all-in-memory with a single thread of execution is hard to beat.</para>
<para>Cluster v6 performs the same queries in 623 ms. What is gained in parallelism is lost in latency
if all operations complete in microseconds. On the other hand, Cluster v6 leaves v5 in the dust in any
situation that has less than 100% hit rate. This is due to actual benefit from parallelism if operations
take longer than a few microseconds, such as in the case of disk reads. Cluster v6 has substantially
better data layout on disk, as well as fewer pages to load for the same content.</para>
<para>This makes it possible to run the queries without the pogs index on Cluster v6 even when v5 takes prohibitively long.</para>
<para>The purpose is to have a lot of RAM and space-efficient data representation.</para>
<para>For reference, the query texts specifying the graph are below. To run without specifying
the graph, just drop the FROM &lt;http://dbpedia.org&gt; from each query. The returned row counts are
indicated below each query's text.</para>
<programlisting><![CDATA[
sparql SELECT ?p ?o FROM <http://dbpedia.org> WHERE {
  <http://dbpedia.org/resource/Metropolitan_Museum_of_Art> ?p ?o };

-- 1337 rows

sparql PREFIX p: <http://dbpedia.org/property/>
SELECT ?film1 ?actor1 ?film2 ?actor2
FROM <http://dbpedia.org> WHERE {
  ?film1 p:starring <http://dbpedia.org/resource/Kevin_Bacon> .
  ?film1 p:starring ?actor1 .
  ?film2 p:starring ?actor1 .
  ?film2 p:starring ?actor2 . };

--  23910 rows

sparql PREFIX p: <http://dbpedia.org/property/>
SELECT ?artist ?artwork ?museum ?director FROM <http://dbpedia.org>
WHERE {
  ?artwork p:artist ?artist .
  ?artwork p:museum ?museum .
  ?museum p:director ?director };

-- 303 rows

sparql PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
SELECT ?s ?homepage FROM <http://dbpedia.org>  WHERE {
   <http://dbpedia.org/resource/Berlin> geo:lat ?berlinLat .
   <http://dbpedia.org/resource/Berlin> geo:long ?berlinLong .
   ?s geo:lat ?lat .
   ?s geo:long ?long .
   ?s foaf:homepage ?homepage .
   FILTER (
     ?lat        <=     ?berlinLat + 0.03190235436 &&
     ?long       >=     ?berlinLong - 0.08679199218 &&
     ?lat        >=     ?berlinLat - 0.03190235436 &&
     ?long       <=     ?berlinLong + 0.08679199218) };

-- 56 rows

sparql PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>
PREFIX foaf: <http://xmlns.com/foaf/0.1/>
PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>
PREFIX p: <http://dbpedia.org/property/>
SELECT ?s ?a ?homepage FROM <http://dbpedia.org>  WHERE {
   <http://dbpedia.org/resource/New_York_City> geo:lat ?nyLat .
   <http://dbpedia.org/resource/New_York_City> geo:long ?nyLong .
   ?s geo:lat ?lat .
   ?s geo:long ?long .
   ?s p:architect ?a .
   ?a foaf:homepage ?homepage .
   FILTER (
     ?lat        <=     ?nyLat + 0.3190235436 &&
     ?long       >=     ?nyLong - 0.8679199218 &&
     ?lat        >=     ?nyLat - 0.3190235436 &&
     ?long       <=     ?nyLong + 0.8679199218) };

-- 13 rows
]]></programlisting>
<para></para>
<para></para>
<para></para>
<para></para>
    </sect2>
</sect1>
<sect1 id="rdfstorebenchmarks"><title>RDF Store Benchmarks</title>
    <sect2 id="rdfstorebenchmarksintroduction"><title>Introduction</title>
       <para>In a particular RDF Store Benchmarks there is difference if the queries are
executed with specified graph or with specified multiple graphs. As Virtuoso is quad store,
not triple store with many tables, it runs queries inefficiently if graphs are specified
and there are no additional indexes except pre-set GSPO and OGPS. Proper use of the FROM clause
or adding indexes with graph column will contribute for better results.
       </para>
    </sect2>
    <sect2 id="rdfstorebenchmarksindexusage"><title>Using bitmap indexes</title>
    <para>If is known in advance for the current RDF Store Benchmarks that some
users will not indicate specific graphs then should be done: </para>
    <itemizedlist>
       <listitem>either create indexes with graph in last position</listitem>
       <listitem>or load everything into single graph and specify it somewhere in querying application.</listitem>
    </itemizedlist>
    <para>Both methods do not require any changes in query texts</para>
    <para>Strongly recommended is the usage of additional bitmap indexes:</para>
<programlisting><![CDATA[
SQL> create bitmap index RDF_QUAD_POGS on DB.DBA.RDF_QUAD (P,O,G,S);
SQL> create bitmap index RDF_QUAD_PSOG on DB.DBA.RDF_QUAD (P,S,O,G);
]]></programlisting>
    <para>You can create other indexes as well. Bitmap indexes are preferable, but
if O is the last column, then the index can not be bitmap, so it could be, for e.g.:</para>
<programlisting><![CDATA[
create index RDF_QUAD_PSGO on DB.DBA.RDF_QUAD (P, S, G, O);
]]></programlisting>
    <para>but cannot be:</para>
<programlisting><![CDATA[
create bitmap index RDF_QUAD_PSGO on DB.DBA.RDF_QUAD (P, S, G, O);
]]></programlisting>
    </sect2>
</sect1>
<sect1 id="rdfsparqlimplementationextent"><title>SPARQL Implementation Details</title>
<para>Virtuoso's RDF support includes in-built support for the SPARQL query language. It also includes a number of powerful extensions that cover path traversal and business intelligence features. In addition, there is in-built security based on Virtuoso's support for Row Level policy based security, custom authentication, and Named Graphs.</para>
<para>The current implementation does not support some SPARQL features.</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>Unicode characters in names are not supported.</listitem>
<listitem>Comments inside SPARQL query are not supported when the query is inlined in SQL code.</listitem>
</itemizedlist>
<para>On the other hand, Virtuoso implements some extensions to SPARQL:</para>
<itemizedlist mark="bullet" spacing="compact">
<listitem>SPARUL statements are supported, like <emphasis>insert</emphasis>, <emphasis>modify</emphasis>, <emphasis>load</emphasis> and so on.</listitem>
<listitem>The SPARQL compiler can be configured using <emphasis>define ...</emphasis> clauses, e.g., <emphasis>define output:valmode "LONG"</emphasis>.</listitem>
<listitem>Expressions are allowed in triple patterns both in <emphasis>where</emphasis> and in constructor patterns, that expressions are delimited by backquotes.</listitem>
<listitem>Expressions are allowed in result lists of select statements.</listitem>
<listitem>Parameters can be passed to the query from outside, using <emphasis>?:variablename</emphasis> syntax.</listitem>
<listitem>Aggregate functions are supported.</listitem>
<listitem>Subqueries may appear where group patterns are allowed.</listitem>
<listitem>A set of operators is added in order to configure the mapping of relational data to RDF.</listitem>
</itemizedlist>
<para>The following dump is BNF grammar of SPARQL with all Virtuoso extensions, but without rules for syntax of lexems.
Rule numbers in square brackets are from W3C normative SPARQL grammar,
asterisk means that the rule differs from W3C grammar due to Virtuoso extensions,
<emphasis>[Virt]</emphasis> means that the rule is Virtuoso-specific.
<emphasis>[DML]</emphasis> stands for data manipulation language extensions from SPARUL.
</para>
<programlisting><![CDATA[
[1]*	Query		 ::=  Prolog ( QueryBody | SparulAction* | ( QmStmt ('.' QmStmt)* '.'? ) )
[1]	QueryBody	 ::=  SelectQuery | ConstructQuery | DescribeQuery | AskQuery
[2]*	Prolog		 ::=  Define* BaseDecl? PrefixDecl*
[Virt]	Define		 ::=  'DEFINE' QNAME (QNAME | Q_IRI_REF | String )
[3]	BaseDecl	 ::=  'BASE' Q_IRI_REF
[4]	PrefixDecl	 ::=  'PREFIX' QNAME_NS Q_IRI_REF
[5]*	SelectQuery	 ::=  'SELECT' 'DISTINCT'? ( ( Retcol ( ','? Retcol )* ) | '*' )
			DatasetClause* WhereClause SolutionModifier
[6]	ConstructQuery	 ::=  'CONSTRUCT' ConstructTemplate DatasetClause* WhereClause SolutionModifier
			DatasetClause* WhereClause? SolutionModifier
[8]	AskQuery	 ::=  'ASK' DatasetClause* WhereClause
[9]	DatasetClause	 ::=  'FROM' ( DefaultGraphClause | NamedGraphClause )
[10]*	DefaultGraphClause	 ::=  SourceSelector SpongeOptionList?
[11]*	NamedGraphClause	 ::=  'NAMED' SourceSelector SpongeOptionList?
[Virt]	SpongeOptionList	 ::=  'OPTION' '(' ( SpongeOption ( ',' SpongeOption )* )? ')'
[Virt]	SpongeOption	 ::=  QNAME PrecodeExpn
[Virt]	PrecodeExpn	 ::=  Expn	(* Only global variables can occur in Expn, local can not *)
[13]	WhereClause	 ::=  'WHERE'? GroupGraphPattern
[14]	SolutionModifier	 ::=  OrderClause?
			((LimitClause OffsetClause?) | (OffsetClause LimitClause?))?
[15]	OrderClause	 ::=  'ORDER' 'BY' OrderCondition+
[16]*	OrderCondition	 ::=  ( 'ASC' | 'DESC' )?
			( FunctionCall | Var | ( '(' Expn ')' ) | ( '[' Expn ']' ) )
[17]	LimitClause	 ::=  'LIMIT' INTEGER
[17]	LimitClause	 ::=  'LIMIT' INTEGER
[18]	OffsetClause	 ::=  'OFFSET' INTEGER
[18]	OffsetClause	 ::=  'OFFSET' INTEGER
[19]*	GroupGraphPattern	 ::=  '{' ( GraphPattern | SelectQuery ) '}'
[20]	GraphPattern	 ::=  Triples? ( GraphPatternNotTriples '.'? GraphPattern )?
[21]*	GraphPatternNotTriples	 ::=
			QuadMapGraphPattern
			| OptionalGraphPattern
			| GroupOrUnionGraphPattern
			| GraphGraphPattern
			| Constraint
[22]	OptionalGraphPattern	 ::=  'OPTIONAL' GroupGraphPattern
[Virt]	QuadMapGraphPattern	 ::=  'QUAD' 'MAP' ( IRIref | '*' ) GroupGraphPattern
[23]	GraphGraphPattern	 ::=  'GRAPH' VarOrBlankNodeOrIRIref GroupGraphPattern
[24]	GroupOrUnionGraphPattern	 ::=  GroupGraphPattern ( 'UNION' GroupGraphPattern )*
[25]*	Constraint	 ::=  'FILTER' ( ( '(' Expn ')' ) | BuiltInCall | FunctionCall )
[26]*	ConstructTemplate	 ::=  '{' ConstructTriples '}'
[27]	ConstructTriples	 ::=  ( Triples1 ( '.' ConstructTriples )? )?
[28]	Triples		 ::=  Triples1 ( '.' Triples? )?
[29]	Triples1	 ::=  VarOrTerm PropertyListNotEmpty | TriplesNode PropertyList
[30]	PropertyList	 ::=  PropertyListNotEmpty?
[31]	PropertyListNotEmpty	 ::=  Verb ObjectList ( ';' PropertyList )?
[32]*	ObjectList	 ::=  ObjGraphNode ( ',' ObjectList )?
[Virt]	ObjGraphNode	 ::=  GraphNode TripleOptions?
[Virt]	TripleOptions	 ::=  'OPTION' '(' TripleOption ( ',' TripleOption )? ')'
[Virt]	TripleOption	 ::=  'INFERENCE' ( QNAME | Q_IRI_REF | SPARQL_STRING )
[33]	Verb		 ::=  VarOrBlankNodeOrIRIref | 'a'
[34]	TriplesNode	 ::=  Collection | BlankNodePropertyList
[35]	BlankNodePropertyList	 ::=  '[' PropertyListNotEmpty ']'
[36]	Collection	 ::=  '(' GraphNode* ')'
[37]	GraphNode	 ::=  VarOrTerm | TriplesNode
[38]	VarOrTerm	 ::=  Var | GraphTerm
[39]*	VarOrIRIrefOrBackquoted	 ::=  Var | IRIref | Backquoted
[40]*	VarOrBlankNodeOrIRIrefOrBackquoted	 ::=  Var | BlankNode | IRIref | Backquoted
[Virt]	Retcol	 ::=  ( Var | ( '(' Expn ')' ) | RetAggCall ) ( 'AS' ( VAR1 | VAR2 ) )?
[Virt]	RetAggCall	 ::=  AggName '(', ( '*' | ( 'DISTINCT'? Var ) ) ')'
[Virt]	AggName	 ::=  'COUNT' | 'AVG' | 'MIN' | 'MAX' | 'SUM'
[41]*	Var	 ::=  VAR1 | VAR2 | GlobalVar | ( Var ( '+>' | '*>' ) IRIref )
[Virt]	GlobalVar	 ::=  QUEST_COLON_PARAMNAME | DOLLAR_COLON_PARAMNAME
			| QUEST_COLON_PARAMNUM | DOLLAR_COLON_PARAMNUM
[42]*	GraphTerm	 ::=  IRIref | RDFLiteral | ( '-' | '+' )? NumericLiteral
			| BooleanLiteral | BlankNode | NIL | Backquoted
[Virt]	Backquoted	 ::=  '`' Expn '`'
[43]	Expn		 ::=  ConditionalOrExpn
[44]	ConditionalOrExpn	 ::=  ConditionalAndExpn ( '||' ConditionalAndExpn )*
[45]	ConditionalAndExpn	 ::=  ValueLogical ( '&&' ValueLogical )*
[46]	ValueLogical	 ::=  RelationalExpn
[47]*	RelationalExpn	 ::=  NumericExpn
			( ( ('='|'!='|'<'|'>'|'<='|'>='|'LIKE') NumericExpn )
			| ( 'IN' '(' Expns ')' ) )?
[49]	AdditiveExpn	 ::=  MultiplicativeExpn ( ('+'|'-') MultiplicativeExpn )*
[50]	MultiplicativeExpn	 ::=  UnaryExpn ( ('*'|'/') UnaryExpn )*
[51]	UnaryExpn	 ::=   ('!'|'+'|'-')? PrimaryExpn
[58]	PrimaryExpn	 ::=
			BracketedExpn | BuiltInCall | IRIrefOrFunction
			| RDFLiteral | NumericLiteral | BooleanLiteral | BlankNode | Var
[55]	IRIrefOrFunction	 ::=  IRIref ArgList?
[52]*	BuiltInCall	 ::=
			( 'STR' '(' Expn ')' )
			| ( 'IRI' '(' Expn ')' )
			| ( 'LANG' '(' Expn ')' )
			| ( 'LANGMATCHES' '(' Expn ',' Expn ')' )
			| ( 'DATATYPE' '(' Expn ')' )
			| ( 'BOUND' '(' Var ')' )
			| ( 'sameTERM' '(' Expn ',' Expn ')' )
			| ( 'isIRI' '(' Expn ')' )
			| ( 'isURI' '(' Expn ')' )
			| ( 'isBLANK' '(' Expn ')' )
			| ( 'isLITERAL' '(' Expn ')' )
			| RegexExpn
[53]	RegexExpn	 ::=  'REGEX' '(' Expn ',' Expn ( ',' Expn )? ')'
[54]	FunctionCall	 ::=  IRIref ArgList
[56]*	ArgList	 ::=  ( NIL | '(' Expns ')' )
[Virt]	Expns	 ::=  Expn ( ',' Expn )*
[59]	NumericLiteral	 ::=  INTEGER | DECIMAL | DOUBLE
[60]	RDFLiteral	 ::=  String ( LANGTAG | ( '^^' IRIref ) )?
[61]	BooleanLiteral	 ::=  'true' | 'false'
[63]	IRIref		 ::=  Q_IRI_REF | QName
[64]	QName		 ::=  QNAME | QNAME_NS
[65]*	BlankNode	 ::=  BLANK_NODE_LABEL | ( '[' ']' )
[DML]	SparulAction	 ::=
			CreateAction | DropAction | LoadAction
			| InsertAction | InsertDataAction | DeleteAction | DeleteDataAction
			| ModifyAction | ClearAction
[DML]*	InsertAction	 ::=
			'INSERT' ( ( 'IN' | 'INTO ) 'GRAPH' ( 'IDENTIFIED' 'BY' )? )? PrecodeExpn
			ConstructTemplate ( DatasetClause* WhereClause SolutionModifier )?
[DML]*	InsertDataAction	 ::=
			'INSERT' 'DATA' ( ( 'IN' | 'INTO ) 'GRAPH' ( 'IDENTIFIED' 'BY' )? )?
			PrecodeExpn ConstructTemplate
[DML]*	DeleteAction	 ::=
			'DELETE' ( 'FROM' 'GRAPH' ( 'IDENTIFIED' 'BY' )? )? PrecodeExpn
			ConstructTemplate ( DatasetClause* WhereClause SolutionModifier )?
[DML]*	DeleteDataAction	 ::=
			'DELETE' 'DATA' ( 'FROM' 'GRAPH' ( 'IDENTIFIED' 'BY' )? )?
			PrecodeExpn ConstructTemplate
[DML]*	ModifyAction	 ::=
			'MODIFY' ( 'GRAPH' ( 'IDENTIFIED' 'BY' )? PrecodeExpn?
			'DELETE' ConstructTemplate 'INSERT' ConstructTemplate
			( DatasetClause* WhereClause SolutionModifier )?
[DML]*	ClearAction	 ::=  'CLEAR' ( 'GRAPH' ( 'IDENTIFIED' 'BY' )? PrecodeExpn )?
[DML]*	LoadAction	 ::=  'LOAD' PrecodeExpn
			( ( 'IN' | 'INTO' ) 'GRAPH' ( 'IDENTIFIED' 'BY' )? PrecodeExpn )?
[DML]*	CreateAction	 ::=  'CREATE' 'SILENT'? 'GRAPH' ( 'IDENTIFIED' 'BY' )? PrecodeExpn
[DML]*	DropAction	 ::=  'DROP' 'SILENT'? 'GRAPH' ( 'IDENTIFIED' 'BY' )? PrecodeExpn
[Virt]	QmStmt		 ::=  QmSimpleStmt | QmCreateStorage | QmAlterStorage
[Virt]	QmSimpleStmt	 ::=
			QmCreateIRIClass | QmCreateLiteralClass | QmDropIRIClass | QmDropLiteralClass
			| QmCreateIRISubclass | QmDropQuadStorage | QmDropQuadMap
[Virt]	QmCreateIRIClass	 ::=  'CREATE' 'IRI' 'CLASS' QmIRIrefConst
			( ( String QmSqlfuncArglist )
			| ( 'USING' QmSqlfuncHeader ',' QmSqlfuncHeader ) )
[Virt]	QmCreateLiteralClass	 ::=  'CREATE' 'LITERAL' 'CLASS' QmIRIrefConst
			'USING' QmSqlfuncHeader ',' QmSqlfuncHeader QmLiteralClassOptions?
[Virt]	QmDropIRIClass	 ::=  'DROP' 'IRI' 'CLASS' QmIRIrefConst
[Virt]	QmDropLiteralClass	 ::=  'DROP' 'LITERAL' 'CLASS' QmIRIrefConst
[Virt]	QmCreateIRISubclass	 ::=  'IRI' 'CLASS' QmIRIrefConst 'SUBCLASS' 'OF' QmIRIrefConst
[Virt]	QmIRIClassOptions	 ::=  'OPTION' '(' QmIRIClassOption (',' QmIRIClassOption)* ')'
[Virt]	QmIRIClassOption	 ::=
			'BIJECTION'
			| 'DEREF'
			| 'RETURNS' STRING ('UNION' STRING)*
[Virt]	QmLiteralClassOptions	 ::=  'OPTION' '(' QmLiteralClassOption (',' QmLiteralClassOption)* ')'
[Virt]	QmLiteralClassOption	 ::=
			( 'DATATYPE' QmIRIrefConst )
			| ( 'LANG' STRING )
			| ( 'LANG' STRING )
			| 'BIJECTION'
			| 'DEREF'
			| 'RETURNS' STRING ('UNION' STRING)*
[Virt]	QmCreateStorage	 ::=  'CREATE' 'QUAD' 'STORAGE' QmIRIrefConst QmSourceDecl* QmMapTopGroup
[Virt]	QmAlterStorage	 ::=  'ALTER' 'QUAD' 'STORAGE' QmIRIrefConst QmSourceDecl* QmMapTopGroup
[Virt]	QmDropStorage	 ::=  'DROP' 'QUAD' 'STORAGE' QmIRIrefConst
[Virt]	QmDropQuadMap	 ::=  'DROP' 'QUAD' 'MAP' 'GRAPH'? QmIRIrefConst
[Virt]	QmDrop	 ::=  'DROP' 'GRAPH'? QmIRIrefConst
[Virt]	QmSourceDecl	 ::=
			( 'FROM' QTABLE 'AS' PLAIN_ID QmTextLiteral* )
			| ( 'FROM' PLAIN_ID 'AS' PLAIN_ID QmTextLiteral* )
			| QmCondition
[Virt]	QmTextLiteral	 ::=  'TEXT' 'XML'? 'LITERAL' QmSqlCol ( 'OF' QmSqlCol )? QmTextLiteralOptions?
[Virt]	QmTextLiteralOptions	 ::=  'OPTION' '(' QmTextLiteralOption ( ',' QmTextLiteralOption )* ')'
[Virt]	QmMapTopGroup	 ::=  '{' QmMapTopOp ( '.' QmMapTopOp )* '.'? '}'
[Virt]	QmMapTopOp	 ::=  QmMapOp | QmDropQuadMap | QmDrop
[Virt]	QmMapGroup	 ::=  '{' QmMapOp ( '.' QmMapOp )* '.'? '}'
[Virt]	QmMapOp		 ::=
			( 'CREATE' QmIRIrefConst 'AS' QmMapIdDef )
			| ( 'CREATE' 'GRAPH'? QmIRIrefConst 'USING' 'STORAGE' QmIRIrefConst QmOptions? )
			| ( QmNamedField+ QmOptions? QmMapGroup )
			| QmTriples1
[Virt]	QmMapIdDef	 ::=  QmMapTriple | ( QmNamedField+ QmOptions? QmMapGroup )
[Virt]	QmMapTriple	 ::=  QmFieldOrBlank QmVerb QmObjField
[Virt]	QmTriples1	 ::=  QmFieldOrBlank QmProps
[Virt]	QmNamedField	 ::=  ('GRAPH'|'SUBJECT'|'PREDICATE'|'OBJECT') QmField
[Virt]	QmProps		 ::=  QmProp ( ';' QmProp )?
[Virt]	QmProp		 ::=  QmVerb QmObjField ( ',' QmObjField )*
[Virt]	QmObjField	 ::=  QmFieldOrBlank QmCondition* QmOptions?
[Virt]	QmIdSuffix	 ::=  'AS' QmIRIrefConst
[Virt]	QmVerb		 ::=  QmField | ( '[' ']' ) | 'a'
[Virt]	QmFieldOrBlank	 ::=  QmField | ( '[' ']' )
[Virt]	QmField		 ::=
			NumericLiteral
			| RdfLiteral
			| ( QmIRIrefConst ( '(' ( QmSqlCol ( ',' QmSqlCol )* )? ')' )? )
			| QmSqlCol
[Virt]	QmCondition	 ::=  'WHERE' ( ( '(' SQLTEXT ')' ) | String )
[Virt]	QmOptions	 ::=  'OPTION' '(' QmOption ( ',' QmOption )* ')'
[Virt]	QmOption	 ::=  ( 'SOFT'? 'EXCLUSIVE' ) | ( 'ORDER' INTEGER ) | ( 'USING' PLAIN_ID )
[Virt]	QmSqlfuncHeader	 ::=  'FUNCTION' SQL_QTABLECOLNAME QmSqlfuncArglist 'RETURNS' QmSqltype
[Virt]	QmSqlfuncArglist	 ::=  '(' ( QmSqlfuncArg ( ',' QmSqlfuncArg )* )? ')'
[Virt]	QmSqlfuncArg	 ::=  ('IN' | QmSqlId) QmSqlId QmSqltype
[Virt]	QmSqltype	 ::=  QmSqlId ( 'NOT' 'NULL' )?
[Virt]	QmSqlCol	 ::=  QmSqlId | spar_qm_sql_id
[Virt]	QmSqlId		 ::=  PLAIN_ID | 'TEXT' | 'XML'
[Virt]	QmIRIrefConst	 ::=  IRIref | ( 'IRI' '(' String ')' )
]]></programlisting>
<para><emphasis>Example using OFFSET and LIMIT:</emphasis></para>
<para>Virtuoso uses a zero index in the OFFSET. Thus in the example below, will be taken position at
record 9000 in the result set, and will get the next 1000 rows starting from 9001 record. Note that the
MaxSortedTopRows in parameters Virtuoso ini section needs to be encreased (default is 10000).</para>
<programlisting><![CDATA[
select ?name
ORDER BY ?name
OFFSET 9000
LIMIT 1000
]]></programlisting>
<sect2 id="rdfsparqlandxquery"><title>SPARQL and XQuery Core Function Library</title>
<para>In the current implementation, the XQuery Core Function Library is not available from SPARQL.</para>
<para>As a temporary workaround, string parsing functions are made available, because they are widely used in W3C DAWG examples and the like. They are:</para>
<programlisting>
xsd:boolean (in strg any) returns integer
xsd:dateTime (in strg any) returns datetime
xsd:double (in strg varchar) returns double precision
xsd:float (in strg varchar) returns float
xsd:integer (in strg varchar) returns integer
</programlisting>
<para>(assuming that the query contains declaration 'PREFIX xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt;')</para>
</sect2>
</sect1>
<sect1 id="rdfnativestorageproviders"><title>Native RDF Storage Providers</title>
  <sect2 id="rdfnativestorageprovidersjena"><title>Virtuoso Jena Provider</title>
    <sect3 id="rdfnativestorageprovidersjenawhatis"><title>What is Jena</title>
    <para>Jena is an open source Semantic Web framework for Java. It provides an API to
extract data from and write to RDF graphs. The graphs are represented as an abstract "model".
A model can be sourced with data from files, databases, URIs or a combination of these. A Model
can also be queried through SPARQL and updated through SPARUL.
    </para>
    </sect3>
    <sect3 id="rdfnativestorageprovidersjenawhatis"><title>What is the Virtuoso Jena Provider</title>
    <para>The Virtuoso Jena RDF Data Provider is a fully operational Native Graph Model Storage
Provider for the Jena Framework, enables Semantic Web applications written using the Jena RDF
Frameworks to query the Virtuoso RDF Quad store directly. The Provider has been tested against
the <ulink url="http://jena.sourceforge.net/">Jena 2.5.5</ulink> version currently available.
    </para>
    <figure id="rdfnativestorageprovidersjena1" float="1">
      <title>Virtuoso Jena RDF Data Provider</title>
      <graphic fileref="ui/VirtJenaProvider.png"/>
    </figure>
    </sect3>
    <sect3 id="rdfnativestorageprovidersjenasetup"><title>Setup</title>
    <sect4 id="rdfnativestorageprovidersjenareqfiles"><title>Required Files</title>
    <itemizedlist mark="bullet">
      <listitem>Virtuoso Jena Provider JAR file, <ulink url="http://virtuoso.openlinksw.com/wiki/main/Main/VirtJenaProvider/virt_jena.jar">virt_jena.jar</ulink></listitem>
      <listitem>Virtuoso JDBC Driver JAR file, <ulink url="http://virtuoso.openlinksw.com/wiki/main/Main/VirtJenaProvider/virtjdbc3.jar">virtjdbc3.jar</ulink></listitem>
      <listitem>Jena Framework and associated classes, <ulink url="http://virtuoso.openlinksw.com/wiki/main/Main/VirtJenaProvider/jenajars.zip">jenajars.zip</ulink></listitem>
      <listitem>Sample programs, <ulink url="http://virtuoso.openlinksw.com/wiki/main/Main/VirtJenaProvider/virtjenasamples.zip">virtjenasamples.zip</ulink></listitem>
    </itemizedlist>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenacmsmpr"><title>Compiling Jena Sample Programs</title>
    <orderedlist>
      <listitem>Edit the sample programs VirtuosoSPARQLExampleX.java, where X = 1 to 9. Set the JDBC connection strings within to point to a valid Virtuoso Server instance of the form:
<programlisting><![CDATA[
"jdbc:virtuoso://localhost:1111/charset=UTF-8/log_enable=2", "dba", "dba"
]]></programlisting>
      <itemizedlist>
         <listitem>charset=UTF-8: to work correctly with different encodings such as cirilic, etc.
         </listitem>
         <listitem>log_enable=2: to use row auto commit
         </listitem>
         <listitem>use these settings to process large rdf data.
         </listitem>
      </itemizedlist>
      </listitem>
      <listitem>Ensure that full paths to <emphasis>jena.jar, arq.jar,</emphasis> and
<emphasis>virtjdbc3.jar</emphasis> are included in the active CLASSPATH setting.
      </listitem>
      <listitem>Compile the Jena Sample applications using the following command:
<programlisting><![CDATA[
javac -cp "jena.jar:arq.jar:virtjdbc3.jar:virt_jena.jar:." VirtuosoSPARQLExample1.java
VirtuosoSPARQLExample2.java VirtuosoSPARQLExample3.java VirtuosoSPARQLExample4.java
VirtuosoSPARQLExample5.java VirtuosoSPARQLExample6.java VirtuosoSPARQLExample7.java
VirtuosoSPARQLExample8.java VirtuosoSPARQLExample9.java
]]></programlisting>
      </listitem>
    </orderedlist>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenatesting"><title>Testing</title>
    <para>Once the Provider classes and sample program have been successfully compiled,
the Provider can be tested using the sample programs included. Ensure your active CLASSPATH
includes full paths to all of the following files, before executing the example commands:
    </para>
    <itemizedlist mark="bullet">
      <listitem>icu4j_3_4.jar</listitem>
      <listitem>iri.jar</listitem>
      <listitem>xercesImpl.jar</listitem>
      <listitem>axis.jar</listitem>
      <listitem>commons-logging-1.1.1.jar</listitem>
      <listitem>jena.jar</listitem>
      <listitem>arq.jar</listitem>
      <listitem>virtjdbc3.jar</listitem>
      <listitem>virt_jena.jar</listitem>
    </itemizedlist>
    <orderedlist>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples1">VirtuosoSPARQLExample1</link> returns the contents of the RDF Quad store of the targeted Virtuoso instance, with the following command:
<programlisting><![CDATA[
java VirtuosoSPARQLExample1
]]></programlisting>
      </listitem>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples2">VirtuosoSPARQLExample2</link> reads in the contents of the following FOAF URIs --
<programlisting><![CDATA[
http://kidehen.idehen.net/dataspace/person/kidehen#this
http://www.w3.org/People/Berners-Lee/card#i
http://demo.openlinksw.com/dataspace/person/demo#this
]]></programlisting>
      <para>-- and returns the RDF data stored, with the following command:</para>
<programlisting><![CDATA[
java VirtuosoSPARQLExample2
]]></programlisting>
      </listitem>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples3">VirtuosoSPARQLExample3</link> performs simple addition and deletion operation on
the content of the triple store, with the following command:
<programlisting><![CDATA[
java VirtuosoSPARQLExample3
]]></programlisting>
      </listitem>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples4">VirtuosoSPARQLExample4</link> demonstrates the use of the <emphasis>graph.contains</emphasis> method for searching triples, with the following command:
<programlisting><![CDATA[
java VirtuosoSPARQLExample4
]]></programlisting>
</listitem>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples5">VirtuosoSPARQLExample5</link> demonstrates the use of the <emphasis>graph.find</emphasis> method for searching triples, with the following command:
<programlisting><![CDATA[
java VirtuosoSPARQLExample5
]]></programlisting>
      </listitem>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples6">VirtuosoSPARQLExample6</link> demonstrates the use of the <emphasis>graph.getTransactionHandler</emphasis> method, with the following command:
<programlisting><![CDATA[
java VirtuosoSPARQLExample6
]]></programlisting>
      </listitem>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples7">VirtuosoSPARQLExample7</link> demonstrates the use of the graph.getBulkUpdateHandler method, with the following command:
<programlisting><![CDATA[
java VirtuosoSPARQLExample7
]]></programlisting>
      </listitem>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples8">VirtuosoSPARQLExample8</link> demonstrates how to insert triples into a graph, with the following command:
<programlisting><![CDATA[
java VirtuosoSPARQLExample8
]]></programlisting>
      </listitem>
      <listitem><link linkend="rdfnativestorageprovidersjenaexamples9">VirtuosoSPARQLExample9</link> demonstrates the use of the <emphasis>CONSTRUCT, DESCRIBE,</emphasis> and <emphasis>ASK</emphasis> SPARQL query forms, with the following command:
<programlisting><![CDATA[
java VirtuosoSPARQLExample9
]]></programlisting>
      </listitem>
    </orderedlist>
    </sect4>
    </sect3>
    <sect3 id="rdfnativestorageprovidersjenaexamples"><title>Examples</title>
    <sect4 id="rdfnativestorageprovidersjenaexamples1"><title>VirtJenaSPARQLExample1</title>
<programlisting><![CDATA[
import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.rdf.model.RDFNode;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample1 {

	/**
	 * Executes a SPARQL query against a virtuoso url and prints results.
	 */
	public static void main(String[] args) {

		String url;
		if(args.length == 0)
		    url = "jdbc:virtuoso://localhost:1111";
		else
		    url = args[0];

/*			STEP 1			*/
		VirtGraph set = new VirtGraph (url, "dba", "dba");

/*			STEP 2			*/


/*			STEP 3			*/
/*		Select all data in virtuoso	*/
		Query sparql = QueryFactory.create("SELECT * WHERE { GRAPH ?graph { ?s ?p ?o } } limit 100");

/*			STEP 4			*/
		VirtuosoQueryExecution vqe = VirtuosoQueryExecutionFactory.create (sparql, set);

		ResultSet results = vqe.execSelect();
		while (results.hasNext()) {
			QuerySolution result = results.nextSolution();
		    RDFNode graph = result.get("graph");
		    RDFNode s = result.get("s");
		    RDFNode p = result.get("p");
		    RDFNode o = result.get("o");
		    System.out.println(graph + " { " + s + " " + p + " " + o + " . }");
		}
	}
}

]]></programlisting>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenaexamples2"><title>VirtJenaSPARQLExample2</title>
<programlisting><![CDATA[
import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.rdf.model.RDFNode;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample2 {

	/**
	 * Executes a SPARQL query against a virtuoso url and prints results.
	 */
	public static void main(String[] args) {

		String url;
		if(args.length == 0)
		    url = "jdbc:virtuoso://localhost:1111";
		else
		    url = args[0];

/*			STEP 1			*/
		VirtGraph graph = new VirtGraph ("Example2", url, "dba", "dba");

/*			STEP 2			*/
/*		Load data to Virtuoso		*/
		graph.clear ();

		System.out.print ("Begin read from 'http://www.w3.org/People/Berners-Lee/card#i'  ");
		graph.read("http://www.w3.org/People/Berners-Lee/card#i", "RDF/XML");
		System.out.println ("\t\t\t Done.");

		System.out.print ("Begin read from 'http://demo.openlinksw.com/dataspace/person/demo#this'  ");
		graph.read("http://demo.openlinksw.com/dataspace/person/demo#this", "RDF/XML");
		System.out.println ("\t Done.");

		System.out.print ("Begin read from 'http://kidehen.idehen.net/dataspace/person/kidehen#this'  ");
		graph.read("http://kidehen.idehen.net/dataspace/person/kidehen#this", "RDF/XML");
		System.out.println ("\t Done.");


/*			STEP 3			*/
/*		Select only from VirtGraph	*/
		Query sparql = QueryFactory.create("SELECT ?s ?p ?o WHERE { ?s ?p ?o }");

/*			STEP 4			*/
		VirtuosoQueryExecution vqe = VirtuosoQueryExecutionFactory.create (sparql, graph);

		ResultSet results = vqe.execSelect();
		while (results.hasNext()) {
			QuerySolution result = results.nextSolution();
		    RDFNode graph_name = result.get("graph");
		    RDFNode s = result.get("s");
		    RDFNode p = result.get("p");
		    RDFNode o = result.get("o");
		    System.out.println(graph_name + " { " + s + " " + p + " " + o + " . }");
		}

		System.out.println("graph.getCount() = " + graph.getCount());
	}
}
]]></programlisting>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenaexamples3"><title>VirtJenaSPARQLExample3</title>
<programlisting><![CDATA[
import java.util.*;

import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.rdf.model.RDFNode;
import com.hp.hpl.jena.graph.Node;
import com.hp.hpl.jena.graph.Triple;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample3
{
    public static void main(String[] args)
    {
	String url;

	if(args.length == 0)
	    url = "jdbc:virtuoso://localhost:1111";
	else
	    url = args[0];

	Node foo1 = Node.createURI("http://example.org/#foo1");
	Node bar1 = Node.createURI("http://example.org/#bar1");
	Node baz1 = Node.createURI("http://example.org/#baz1");

	Node foo2 = Node.createURI("http://example.org/#foo2");
	Node bar2 = Node.createURI("http://example.org/#bar2");
	Node baz2 = Node.createURI("http://example.org/#baz2");

	Node foo3 = Node.createURI("http://example.org/#foo3");
	Node bar3 = Node.createURI("http://example.org/#bar3");
	Node baz3 = Node.createURI("http://example.org/#baz3");

	List <Triple> triples = new ArrayList <Triple> ();

	VirtGraph graph = new VirtGraph ("Example3", url, "dba", "dba");

	graph.clear ();

	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("Add 3 triples to graph <Example3>.");

	graph.add(new Triple(foo1, bar1, baz1));
	graph.add(new Triple(foo2, bar2, baz2));
	graph.add(new Triple(foo3, bar3, baz3));

	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("graph.getCount() = " + graph.getCount());

	triples.add(new Triple(foo1, bar1, baz1));
	triples.add(new Triple(foo2, bar2, baz2));

	graph.isEmpty();

	System.out.println("Remove 2 triples from graph <Example3>");
	graph.remove(triples);
	System.out.println("graph.getCount() = " + graph.getCount());
	System.out.println("Please check result with isql tool.");

	/* EXPECTED RESULT:

SQL> sparql select ?s ?p ?o from <Example3> where {?s ?p ?o};
s                                                    p                                                             o
VARCHAR                                    VARCHAR                                              VARCHAR
_______________________________________________________________________________

http://example.org/#foo3              http://example.org/#bar3                         http://example.org/#baz3

1 Rows. -- 26 msec.
SQL>

*/

	}
}
]]></programlisting>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenaexamples4"><title>VirtJenaSPARQLExample4</title>
<programlisting><![CDATA[
import java.util.*;

import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.rdf.model.RDFNode;
import com.hp.hpl.jena.graph.Node;
import com.hp.hpl.jena.graph.Triple;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample4
{

    public static void main(String[] args)
    {
	String url;
	if(args.length == 0)
	    url = "jdbc:virtuoso://localhost:1111";
	else
	    url = args[0];

	Node foo1 = Node.createURI("http://example.org/#foo1");
	Node bar1 = Node.createURI("http://example.org/#bar1");
	Node baz1 = Node.createURI("http://example.org/#baz1");

	Node foo2 = Node.createURI("http://example.org/#foo2");
	Node bar2 = Node.createURI("http://example.org/#bar2");
	Node baz2 = Node.createURI("http://example.org/#baz2");

	Node foo3 = Node.createURI("http://example.org/#foo3");
	Node bar3 = Node.createURI("http://example.org/#bar3");
	Node baz3 = Node.createURI("http://example.org/#baz3");

	VirtGraph graph = new VirtGraph ("Example4", url, "dba", "dba");

	graph.clear ();

	System.out.println("graph.isEmpty() = " + graph.isEmpty());

	System.out.println("Add 3 triples to graph <Example4>.");

	graph.add(new Triple(foo1, bar1, baz1));
	graph.add(new Triple(foo2, bar2, baz2));
	graph.add(new Triple(foo3, bar3, baz3));

	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("graph.getCount() = " + graph.getCount());

	System.out.println ("graph.contains(new Triple(foo2, bar2, baz2) - " + graph.contains(new Triple(foo2, bar2, baz2)));
	System.out.println ("graph.contains(new Triple(foo2, bar2, baz3) - " + graph.contains(new Triple(foo2, bar2, baz3)));

	graph.clear ();

    }
}
]]></programlisting>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenaexamples5"><title>VirtJenaSPARQLExample5</title>
<programlisting><![CDATA[
import java.util.*;

import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.util.iterator.ExtendedIterator;
import com.hp.hpl.jena.graph.Node;
import com.hp.hpl.jena.graph.Triple;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample5
{

    public static void main(String[] args)
    {
	String url;
	if(args.length == 0)
	    url = "jdbc:virtuoso://localhost:1111";
	else
	    url = args[0];

	Node foo1 = Node.createURI("http://example.org/#foo1");
	Node bar1 = Node.createURI("http://example.org/#bar1");
	Node baz1 = Node.createURI("http://example.org/#baz1");

	Node foo2 = Node.createURI("http://example.org/#foo2");
	Node bar2 = Node.createURI("http://example.org/#bar2");
	Node baz2 = Node.createURI("http://example.org/#baz2");

	Node foo3 = Node.createURI("http://example.org/#foo3");
	Node bar3 = Node.createURI("http://example.org/#bar3");
	Node baz3 = Node.createURI("http://example.org/#baz3");

	VirtGraph graph = new VirtGraph ("Example5", url, "dba", "dba");

	graph.clear ();

	System.out.println("graph.isEmpty() = " + graph.isEmpty());

	System.out.println("Add 3 triples to graph <Example5>.");

	graph.add(new Triple(foo1, bar1, baz1));
	graph.add(new Triple(foo2, bar2, baz2));
	graph.add(new Triple(foo3, bar3, baz3));
	graph.add(new Triple(foo1, bar2, baz2));
	graph.add(new Triple(foo1, bar3, baz3));

	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("graph.getCount() = " + graph.getCount());

	ExtendedIterator iter = graph.find(foo1, Node.ANY, Node.ANY);
	System.out.println ("\ngraph.find(foo1, Node.ANY, Node.ANY) \nResult:");
	for ( ; iter.hasNext() ; )
	    System.out.println ((Triple) iter.next());

	iter = graph.find(Node.ANY, Node.ANY, baz3);
	System.out.println ("\ngraph.find(Node.ANY, Node.ANY, baz3) \nResult:");
	for ( ; iter.hasNext() ; )
	    System.out.println ((Triple) iter.next());

	iter = graph.find(foo1, Node.ANY, baz3);
	System.out.println ("\ngraph.find(foo1, Node.ANY, baz3) \nResult:");
	for ( ; iter.hasNext() ; )
	    System.out.println ((Triple) iter.next());

	graph.clear ();

    }
}
]]></programlisting>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenaexamples6"><title>VirtJenaSPARQLExample6</title>
<programlisting><![CDATA[
import java.util.*;

import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.util.iterator.ExtendedIterator;
import com.hp.hpl.jena.graph.Node;
import com.hp.hpl.jena.graph.Triple;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample6
{

    public static void main(String[] args)
    {
	String url;
	if(args.length == 0)
	    url = "jdbc:virtuoso://localhost:1111";
	else
	    url = args[0];

	Node foo1 = Node.createURI("http://example.org/#foo1");
	Node bar1 = Node.createURI("http://example.org/#bar1");
	Node baz1 = Node.createURI("http://example.org/#baz1");

	Node foo2 = Node.createURI("http://example.org/#foo2");
	Node bar2 = Node.createURI("http://example.org/#bar2");
	Node baz2 = Node.createURI("http://example.org/#baz2");

	Node foo3 = Node.createURI("http://example.org/#foo3");
	Node bar3 = Node.createURI("http://example.org/#bar3");
	Node baz3 = Node.createURI("http://example.org/#baz3");

	VirtGraph graph = new VirtGraph ("Example6", url, "dba", "dba");

	graph.clear ();

	System.out.println("graph.isEmpty() = " + graph.isEmpty());

	System.out.println("test Transaction Commit.");
	graph.getTransactionHandler().begin();
	System.out.println("begin Transaction.");
	System.out.println("Add 3 triples to graph <Example6>.");

	graph.add(new Triple(foo1, bar1, baz1));
	graph.add(new Triple(foo2, bar2, baz2));
	graph.add(new Triple(foo3, bar3, baz3));

	graph.getTransactionHandler().commit();
	System.out.println("commit Transaction.");
	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("graph.getCount() = " + graph.getCount());

	ExtendedIterator iter = graph.find(Node.ANY, Node.ANY, Node.ANY);
	System.out.println ("\ngraph.find(Node.ANY, Node.ANY, Node.ANY) \nResult:");
	for ( ; iter.hasNext() ; )
	    System.out.println ((Triple) iter.next());

	graph.clear ();
	System.out.println("\nCLEAR graph <Example6>");
	System.out.println("graph.isEmpty() = " + graph.isEmpty());

	System.out.println("Add 1 triples to graph <Example6>.");
	graph.add(new Triple(foo1, bar1, baz1));

	System.out.println("test Transaction Abort.");
	graph.getTransactionHandler().begin();
	System.out.println("begin Transaction.");
	System.out.println("Add 2 triples to graph <Example6>.");

	graph.add(new Triple(foo2, bar2, baz2));
	graph.add(new Triple(foo3, bar3, baz3));

	graph.getTransactionHandler().abort();
	System.out.println("abort Transaction.");
	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("graph.getCount() = " + graph.getCount());

	iter = graph.find(Node.ANY, Node.ANY, Node.ANY);
	System.out.println ("\ngraph.find(Node.ANY, Node.ANY, Node.ANY) \nResult:");
	for ( ; iter.hasNext() ; )
	    System.out.println ((Triple) iter.next());

	graph.clear ();
	System.out.println("\nCLEAR graph <Example6>");

    }
}


]]></programlisting>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenaexamples7"><title>VirtJenaSPARQLExample7</title>
<programlisting><![CDATA[
import java.util.*;

import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.util.iterator.ExtendedIterator;
import com.hp.hpl.jena.graph.Node;
import com.hp.hpl.jena.graph.Triple;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample7
{

    public static void main(String[] args)
    {
	String url;
	if(args.length == 0)
	    url = "jdbc:virtuoso://localhost:1111";
	else
	    url = args[0];

	Node foo1 = Node.createURI("http://example.org/#foo1");
	Node bar1 = Node.createURI("http://example.org/#bar1");
	Node baz1 = Node.createURI("http://example.org/#baz1");

	Node foo2 = Node.createURI("http://example.org/#foo2");
	Node bar2 = Node.createURI("http://example.org/#bar2");
	Node baz2 = Node.createURI("http://example.org/#baz2");

	Node foo3 = Node.createURI("http://example.org/#foo3");
	Node bar3 = Node.createURI("http://example.org/#bar3");
	Node baz3 = Node.createURI("http://example.org/#baz3");

	List triples1 = new ArrayList();
	triples1.add(new Triple(foo1, bar1, baz1));
	triples1.add(new Triple(foo2, bar2, baz2));
	triples1.add(new Triple(foo3, bar3, baz3));

	List triples2 = new ArrayList();
	triples2.add(new Triple(foo1, bar1, baz1));
	triples2.add(new Triple(foo2, bar2, baz2));

	VirtGraph graph = new VirtGraph ("Example7", url, "dba", "dba");

	graph.clear ();

	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("Add List with 3 triples to graph <Example7> via BulkUpdateHandler.");

	graph.getBulkUpdateHandler().add(triples1);

	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("graph.getCount() = " + graph.getCount());

	ExtendedIterator iter = graph.find(Node.ANY, Node.ANY, Node.ANY);
	System.out.println ("\ngraph.find(Node.ANY, Node.ANY, Node.ANY) \nResult:");
	for ( ; iter.hasNext() ; )
	    System.out.println ((Triple) iter.next());


	System.out.println("\n\nDelete List of 2 triples from graph <Example7> via BulkUpdateHandler.");

	graph.getBulkUpdateHandler().delete(triples2);

	System.out.println("graph.isEmpty() = " + graph.isEmpty());
	System.out.println("graph.getCount() = " + graph.getCount());

	iter = graph.find(Node.ANY, Node.ANY, Node.ANY);
	System.out.println ("\ngraph.find(Node.ANY, Node.ANY, Node.ANY) \nResult:");
	for ( ; iter.hasNext() ; )
	    System.out.println ((Triple) iter.next());

	graph.clear ();
	System.out.println("\nCLEAR graph <Example7>");

    }
}
]]></programlisting>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenaexamples8"><title>VirtJenaSPARQLExample8</title>
<programlisting><![CDATA[
import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.rdf.model.RDFNode;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample8 {

	/**
	 * Executes a SPARQL query against a virtuoso url and prints results.
	 */
	public static void main(String[] args) {

		String url;
		if(args.length == 0)
		    url = "jdbc:virtuoso://localhost:1111";
		else
		    url = args[0];

/*			STEP 1			*/
		VirtGraph set = new VirtGraph (url, "dba", "dba");

/*			STEP 2			*/
System.out.println("\nexecute: CLEAR GRAPH <http://test1>");
                String str = "CLEAR GRAPH <http://test1>";
                VirtuosoUpdateRequest vur = VirtuosoUpdateFactory.create(str, set);
                vur.exec();

System.out.println("\nexecute: INSERT INTO GRAPH <http://test1> { <aa> <bb> 'cc' . <aa1> <bb1> 123. }");
                str = "INSERT INTO GRAPH <http://test1> { <aa> <bb> 'cc' . <aa1> <bb1> 123. }";
                vur = VirtuosoUpdateFactory.create(str, set);
                vur.exec();

/*			STEP 3			*/
/*		Select all data in virtuoso	*/
System.out.println("\nexecute: SELECT * FROM <http://test1> WHERE { ?s ?p ?o }");
		Query sparql = QueryFactory.create("SELECT * FROM <http://test1> WHERE { ?s ?p ?o }");

/*			STEP 4			*/
		VirtuosoQueryExecution vqe = VirtuosoQueryExecutionFactory.create (sparql, set);

		ResultSet results = vqe.execSelect();
		while (results.hasNext()) {
			QuerySolution rs = results.nextSolution();
		    RDFNode s = rs.get("s");
		    RDFNode p = rs.get("p");
		    RDFNode o = rs.get("o");
		    System.out.println(" { " + s + " " + p + " " + o + " . }");
		}


System.out.println("\nexecute: DELETE FROM GRAPH <http://test1> { <aa> <bb> 'cc' }");
                str = "DELETE FROM GRAPH <http://test1> { <aa> <bb> 'cc' }";
                vur = VirtuosoUpdateFactory.create(str, set);
                vur.exec();

System.out.println("\nexecute: SELECT * FROM <http://test1> WHERE { ?s ?p ?o }");
		vqe = VirtuosoQueryExecutionFactory.create (sparql, set);
                results = vqe.execSelect();
		while (results.hasNext()) {
			QuerySolution rs = results.nextSolution();
		    RDFNode s = rs.get("s");
		    RDFNode p = rs.get("p");
		    RDFNode o = rs.get("o");
		    System.out.println(" { " + s + " " + p + " " + o + " . }");
		}


	}
}
]]></programlisting>
    </sect4>
    <sect4 id="rdfnativestorageprovidersjenaexamples9"><title>VirtJenaSPARQLExample9</title>
<programlisting><![CDATA[
import com.hp.hpl.jena.query.*;
import com.hp.hpl.jena.rdf.model.RDFNode;
import com.hp.hpl.jena.graph.Triple;
import com.hp.hpl.jena.graph.Node;
import com.hp.hpl.jena.graph.Graph;
import com.hp.hpl.jena.rdf.model.*;
import java.util.Iterator;

import virtuoso.jena.driver.*;

public class VirtuosoSPARQLExample9 {

	/**
	 * Executes a SPARQL query against a virtuoso url and prints results.
	 */
	public static void main(String[] args) {

		String url;
		if(args.length == 0)
		    url = "jdbc:virtuoso://localhost:1111";
		else
		    url = args[0];

/*			STEP 1			*/
		VirtGraph set = new VirtGraph (url, "dba", "dba");

/*			STEP 2			*/
                String str = "CLEAR GRAPH <http://test1>";
                VirtuosoUpdateRequest vur = VirtuosoUpdateFactory.create(str, set);
                vur.exec();

                str = "INSERT INTO GRAPH <http://test1> { <http://aa> <http://bb> 'cc' . <http://aa1> <http://bb> 123. }";
                vur = VirtuosoUpdateFactory.create(str, set);
                vur.exec();


/*		Select all data in virtuoso	*/
		Query sparql = QueryFactory.create("SELECT * FROM <http://test1> WHERE { ?s ?p ?o }");
		VirtuosoQueryExecution vqe = VirtuosoQueryExecutionFactory.create (sparql, set);
		ResultSet results = vqe.execSelect();
                System.out.println("\nSELECT results:");
		while (results.hasNext()) {
			QuerySolution rs = results.nextSolution();
		    RDFNode s = rs.get("s");
		    RDFNode p = rs.get("p");
		    RDFNode o = rs.get("o");
		    System.out.println(" { " + s + " " + p + " " + o + " . }");
		}

		sparql = QueryFactory.create("DESCRIBE <http://aa> FROM <http://test1>");
		vqe = VirtuosoQueryExecutionFactory.create (sparql, set);

		Model model = vqe.execDescribe();
 	        Graph g = model.getGraph();
                System.out.println("\nDESCRIBE results:");
	        for (Iterator i = g.find(Node.ANY, Node.ANY, Node.ANY); i.hasNext();)
	           {
	              Triple t = (Triple)i.next();
		      System.out.println(" { " + t.getSubject() + " " +
		      				 t.getPredicate() + " " +
		      				 t.getObject() + " . }");
	        }



		sparql = QueryFactory.create("CONSTRUCT { ?x <http://test> ?y } FROM <http://test1> WHERE { ?x <http://bb> ?y }");
		vqe = VirtuosoQueryExecutionFactory.create (sparql, set);

		model = vqe.execConstruct();
 	        g = model.getGraph();
                System.out.println("\nCONSTRUCT results:");
	        for (Iterator i = g.find(Node.ANY, Node.ANY, Node.ANY); i.hasNext();)
	           {
	              Triple t = (Triple)i.next();
		      System.out.println(" { " + t.getSubject() + " " +
		      				 t.getPredicate() + " " +
		      				 t.getObject() + " . }");
	        }


		sparql = QueryFactory.create("ASK FROM <http://test1> WHERE { <http://aa> <http://bb> ?y }");
		vqe = VirtuosoQueryExecutionFactory.create (sparql, set);

		boolean res = vqe.execAsk();
                System.out.println("\nASK results: "+res);


	}
}
]]></programlisting>
    </sect4>
    </sect3>
    <sect3 id="rdfnativestorageprovidersjenajavadoc"><title>Javadoc API Documentation</title>
    <para><ulink url="http://docs.openlinksw.com/jena/">Jena Provider Javadoc API Documentation</ulink> is available enabling the complete set of classes, interfaces and methods implemented for the provider to be viewed.
    </para>
    </sect3>
  </sect2>
  <sect2 id="rdfnativestorageproviderssesame"><title>Virtuoso Sesame Provider</title>
    <sect3 id="rdfnativestorageproviderssesamewhatis"><title>What is Sesame</title>
      <para>Sesame is an open source Java framework for storing, querying and reasoning
with RDF and RDF Schema. It can be used as a database for RDF and RDF Schema, or as a Java
library for applications that need to work with RDF internally. For example, suppose you need
to read a big RDF file, find the relevant information for your application, and use that information.
Sesame provides you with the necessary tools to parse, interpret, query and store all this information,
embedded in your own application if you want, or, if you prefer, in a separate database or even on a remote
server. More generally: Sesame provides an application developer a toolbox that contains useful hammers
screwdrivers etc. for doing 'Do-It-Yourself' with RDF.
      </para>
    </sect3>
    <sect3 id="rdfnativestorageproviderssesamewhatisvirtuososesameprovider"><title>What is the Virtuoso Sesame Provider</title>
    <para>The Virtuoso Sesame Provider is a fully oprerational Native Graph Model Storage Providers
for the Sesame Framework, allowing users of Virtuoso to leverage the Sesame framework for modifying,
querying, and reasoning with the Virtuoso quad store using the Java language. The Sesame Repository
API offers a central access point for connecting to the Virtuoso quad store. Its purpose is to
provides a java-friendly access point to Virtuoso. It offers various methods for querying and
updating the data, while abstracting the details of the underlying machinery. The Provider has
been tested against the <ulink url="http://sourceforge.net/project/showfiles.php?group_id=46509&amp;package_id=168413">Sesame 2.1.2</ulink> version currently available.
    </para>
    <figure id="rdfnativestorageprovidersjena1" float="1">
      <title>Fig. 1 Sesame Component Stack</title>
      <graphic fileref="ui/VirtSesame2Provider.png"/>
    </figure>
    <para>If you need more information about how to set up your environment for working with the
Sesame APIs, take a look at Chapter 4 of the Sesame User Guide, <ulink url="http://www.openrdf.org/doc/sesame2/users/RDF">Setting up to use the Sesame libraries</ulink>.
    </para>
    </sect3>
    <sect3 id="rdfnativestorageproviderssesamesetup"><title>Setup</title>
    <sect4 id="rdfnativestorageproviderssesamereqfiles"><title>Required Files</title>
    <para>This tutorial assumes you have Virtuoso server installed and that the database
is accessible at "localhost:1111". In addition, you will need the latest version of the
Virtuoso Sesame Provider, and Sesame 2 or greater installed.
    </para>
    <itemizedlist mark="bullet">
      <listitem>Virtuoso Sesame 2 Provider JAR file, <ulink url="http://virtuoso.openlinksw.com/wiki/main/Main/VirtSesame2Provider/virt_sesame2.jar">virt_sesame2.jar</ulink></listitem>
      <listitem>The Virtuoso JDBC Driver JAR file, <ulink url="http://virtuoso.openlinksw.com/wiki/main/Main/VirtSesame2Provider/virtjdbc3.jar">virtjdbc3.jar</ulink></listitem>
      <listitem>Sesame Framework and associated classes, <ulink url="http://virtuoso.openlinksw.com/wiki/main/Main/VirtSesame2Provider/sesame2jars.zip">sesame2jars.zip</ulink></listitem>
      <listitem>Sample programs, <ulink url="http://virtuoso.openlinksw.com/wiki/main/Main/VirtSesame2Provider/virtsesame2samples.zip">virtsesame2samples.zip</ulink></listitem>
    </itemizedlist>
    </sect4>
    <sect4 id="rdfnativestorageproviderssesamecmppr"><title>Compiling Jena Sample Programs</title>
    <orderedlist>
      <listitem>Ensure that full paths to the following files are all included in the active CLASSPATH setting --
    <itemizedlist mark="bullet">
      <listitem>openrdf-sesame-2.1.2-onejar.jar</listitem>
      <listitem>slf4j-api-1.5.0.jar</listitem>
      <listitem>slf4j-jdk14-1.5.0.jar</listitem>
      <listitem>virtjdbc3.jar</listitem>
      <listitem>virt_sesame2.jar</listitem>
   </itemizedlist>
      </listitem>
      <listitem>Execute the following command --
<programlisting><![CDATA[
javac VirtuosoTest.java
]]></programlisting>
      <para>Note: to use utf-8 and to use row auto commit is recommended
to add the following to the connect string:
      </para>
<programlisting><![CDATA[
"/charset=UTF-8/log_enable=2"
-- i.e. in VirtuosoTest.java the line:
Repository repository = new VirtuosoRepository("jdbc:virtuoso://" + sa[0] + ":" + sa[1], sa[2], sa[3]);
-- should become:
Repository repository = new VirtuosoRepository("jdbc:virtuoso://" + sa[0] + ":" + sa[1]+ "/charset=UTF-8/log_enable=2", sa[2], sa[3]);
]]></programlisting>
      </listitem>
    </orderedlist>
    </sect4>
    <sect4 id="rdfnativestorageproviderssesametesting"><title>Testing</title>
    <orderedlist>
      <listitem>Ensure that full paths to the following files are all included in the active CLASSPATH setting --
    <itemizedlist mark="bullet">
      <listitem>openrdf-sesame-2.1.2-onejar.jar</listitem>
      <listitem>slf4j-api-1.5.0.jar</listitem>
      <listitem>slf4j-jdk14-1.5.0.jar</listitem>
      <listitem>virtjdbc3.jar</listitem>
      <listitem>virt_sesame2.jar</listitem>
      <listitem>virtuoso_driver</listitem>
    </itemizedlist>
      </listitem>
      <listitem>Test the Sesame 2 Provider with the following command
<programlisting><![CDATA[
java VirtuosoTest <hostname> <port> <uid> <pwd>
]]></programlisting>
      </listitem>
      <listitem>The test run should look like this --
<programlisting><![CDATA[
$ java VirtuosoTest localhost 1111 dba dba

== TEST 1:  : Start
   Loading data from URL: http://www.openlinksw.com/dataspace/person/kidehen@openlinksw.com/foaf.rdf
== TEST 1:  : End
PASSED: TEST 1

== TEST 2:  : Start
   Clearing triple store
== TEST 2:  : End
PASSED: TEST 2

== TEST 3:  : Start
   Loading data from file: virtuoso_driver/data.nt
== TEST 3:  : End
PASSED: TEST 3

== TEST 4:  : Start
   Loading UNICODE single triple
== TEST 4:  : End
PASSED: TEST 4

== TEST 5:  : Start
   Loading single triple
== TEST 5:  : End
PASSED: TEST 5

== TEST 6:  : Start
   Casted value type
== TEST 6:  : End
PASSED: TEST 6

== TEST 7:  : Start
   Selecting property
== TEST 7:  : End
PASSED: TEST 7

== TEST 8:  : Start
   Statement does not exists
== TEST 8:  : End
PASSED: TEST 8

== TEST 9:  : Start
   Statement exists (by resultset size)
== TEST 9:  : End
PASSED: TEST 9

== TEST 10:  : Start
   Statement exists (by hasStatement())
== TEST 10:  : End
PASSED: TEST 10

== TEST 11:  : Start
   Retrieving namespaces
== TEST 11:  : End
PASSED: TEST 11

== TEST 12:  : Start
   Retrieving statement (http://myopenlink.net/dataspace/person/kidehen http://myopenlink.net/foaf/name null)
== TEST 12:  : End
PASSED: TEST 12

== TEST 13:  : Start
   Writing the statements to file: (/Users/hughwilliams/src/virtuoso-opensource/binsrc/sesame2/results.n3.txt)
== TEST 13:  : End
PASSED: TEST 13

== TEST 14:  : Start
   Retrieving graph ids
== TEST 14:  : End
PASSED: TEST 14

== TEST 15:  : Start
   Retrieving triple store size
== TEST 15:  : End
PASSED: TEST 15

== TEST 16:  : Start
   Sending ask query
== TEST 16:  : End
PASSED: TEST 16

== TEST 17:  : Start
   Sending construct query
== TEST 17:  : End
PASSED: TEST 17

== TEST 18:  : Start
   Sending describe query
== TEST 18:  : End
PASSED: TEST 18

============================
PASSED:18 FAILED:0
]]></programlisting>
      </listitem>
    </orderedlist>
    </sect4>
    </sect3>
    <sect3 id="rdfnativestorageproviderssesamegettingstarted"><title>Getting Started</title>
    <para>This section covers the essentials for connecting to and manipulating data stored in
a Virtuoso repository using the Sesame API. More information on the Sesame Framework, including
extended examples on how to use the API, can be found in Chapter 8 of the Sesame Users guide,
<ulink url="http://www.openrdf.org/doc/sesame2/2.1.2/users/ch08.html#d0e833">the RepositoryConnection API</ulink>.
    </para>
    <para>The interfaces for the Repository API can be found in packages virtuoso.sesame2.driver
and org.openrdf.repository. Several implementations for these interface exist in the
Virtuoso Provider download package.
The <ulink url="http://www.openrdf.org/doc/sesame2/2.1.2/apidocs/">Javadoc reference for the Sesame API</ulink>
is available online and can also be found in the doc directory of the download.
    </para>
    <sect4 id="rdfnativestorageproviderssesamegettingstartedcrrep"><title>Creating a VirtuosoRepositoryRDF object</title>
      <para>The first step to connecting to Virtuoso through the Sesame API is to create a
Repository for it. The Repository object operates on (stacks of) Sail object(s) for storage
and retrieval of RDF data.
      </para>
      <para>One of the simplest configurations is a repository that just stores RDF data in main
memory without applying any inference or whatsoever. This is also by far the fastest type of
repository that can be used. The following code creates and initialize a non-inferencing main-memory
repository:
      </para>
<programlisting><![CDATA[
import virtuoso.sesame2.driver.VirtuosoRepository;

Repository myRepository = VirtuosoRepositoryRDF?("jdbc:virtuoso://localhost:1111,dba,dba);

myRepository.initialize();
]]></programlisting>
      <para>The constructor of the VirtuosoRepositoryRDF class accepts the JDBC URL of the Virtuoso
engine (the default port is 1111), the username and password of an authorized user. Following this
example, the repository needs to be initialized to prepare the Sail(s) that it operates on, which
includes operations such as restoring previously stored data, setting up connections to a relational
database, etc.
      </para>
      <para>The repository that is created by the above code is volatile: its contents are lost
when the object is garbage collected or when the program is shut down. This is fine for cases where,
for example, the repository is used as a means for manipulating an RDF model in memory. Using the
Virtuoso repository with RepositoryConnecton.
      </para>
      <para>Now that we have created a VirtuosoRepositoryRDF, we want to do something with it.
This is achieved through the use of the VirtuosoRepositoryConnection, which can be created by
the VirtuosoRepositoryRDF.
      </para>
      <para>A VirtuosoRepositoryConnection represents - as the name suggests - a connection
to the actual Virtuoso quad store. We can issue operations over this connection, and close it
when we are done to make sure we are not keeping resources unnecessarily occupied.
      </para>
      <para>In the following sections, we will show some examples of basic operations
using the Northwind dataset.
      </para>
    </sect4>
    <sect4 id="rdfnativestorageproviderssesamegettingstartedrdfvirt"><title>Adding RDF to Virtuoso</title>
      <para>The Repository implements the Sesame Repository API offers various methods for adding
data to a repository. Data can be added pro grammatically by specifying the location of a file
that contains RDF data, and statements can be added individually or in collections.
      </para>
      <para>We perform operations on the repository by requesting a RepositoryConnection from
the repository, which returns a VirtuosoRepositoryConnection object. On this VirtuosoRepositoryConnection
object we can perform the various operations, such as query evaluation, getting, adding, or removing
statements, etc.
      </para>
      <para>The following example code adds two files, one local and one located on the WWW, to a repository:
      </para>
<programlisting><![CDATA[
import org.openrdf.repository.RepositoryException;

import org.openrdf.repository.Repository;

import org.openrdf.repository.RepositoryConnection;

import org.openrdf.rio.RDFFormat;

import java.io.File;

import java.net.URL;

File file = new File("/path/to/example.rdf");

String baseURI = "http://example.org/example/localRDF";

?

try {

   RepositoryConnection con = myRepository.getConnection();

   try {

      con.add(file, baseURI, RDFFormat.RDFXML);

      URL url = new URL("http://example.org/example/remoteRDF");

      con.add(url, url.toString(), RDFFormat.RDFXML);

   }

   finally {

      con.close();

   }

}

catch (RepositoryException rex) {

   // handle exception

}

catch (java.io.IOEXception e) {

   // handle io exception

}
]]></programlisting>
      <para>More information on other available methods can be found in the javadoc
reference of the RepositoryConnection interface.
      </para>
    </sect4>
    <sect4 id="rdfnativestorageproviderssesamegettingstartedqr"><title>Querying Virtuoso</title>
      <para>The Repository API has a number of methods for creating and evaluating queries.
Three types of queries are distinguished: tuple queries, graph queries and boolean queries.
The query types differ in the type of results that they produce.
      </para>
      <para><emphasis>Select Query:</emphasis> The result of a select query is a set of tuples
(or variable bindings), where each tuple represents a solution of a query. This type of query
is commonly used to get specific values (URIs, blank nodes, literals) from the stored RDF data.
The method QueryFactory.executeQuery() returns a Value[][] for sparql "SELECT" queries.
The method QueryFactory.executeQuery() also calls the QueryFactory.setResult() which populates
a set of tuples for SPARQL "SELECT" queries. The graph can be retrieved using
QueryFactory.getBooleanResult().
      </para>
      <para><emphasis>Graph Query:</emphasis> The result of graph queries is an RDF graph
(or set of statements). This type of query is very useful for extracting sub-graphs from
the stored RDF data, which can then be queried further, serialized to an RDF document,
etc. The method QueryFactory.executeQuery() calls the QueryFactory.setGraphResult()
which populates a graph for SPARQL "DESCRIBE" and "CONSTRUCT" queries. The graph can
be retrieved using QueryFactory.getGraphResult().
      </para>
      <para><emphasis>Boolean Query:</emphasis> The result of boolean queries is a simple
boolean value, i.e. true of false. This type of query can be used to check if a repository
contains specific information. The method QueryFactory.executeQuery() calls the
QueryFactory.setBooleanResult() which sets a boolean value for sparql "ASK" queries.
The value can be retrieved using QueryFactory.getBooleanResult().
      </para>
      <para>Note: Although Sesame 2 currently supports two query languages: SeRQL and SPARQL,
the Virtuoso provider only supports the W3C SPARQL specification.
      </para>
    </sect4>
    <sect4 id="rdfnativestorageproviderssesamegettingstartevq"><title>Evaluating a SELECT Query</title>
      <para>To evaluate a tuple query we simply do the following:
      </para>
<programlisting><![CDATA[
import java.util.List;

import org.openrdf.OpenRDFException;

import org.openrdf.repository.RepositoryConnection;

import org.openrdf.query.TupleQuery;

import org.openrdf.query.TupleQueryResult;

import org.openrdf.query.BindingSet;

import org.openrdf.query.QueryLanguage;

?

try {

   RepositoryConnection? con = myRepository.getConnection();

   try {

      String queryString = "SELECT x, y FROM  WHERE {x} p {y}";

      TupleQuery? tupleQuery = con.prepareTupleQuery(QueryLanguage.SPARQL, queryString);

      TupleQueryResult? result = tupleQuery.evaluate();

      try {

         ? // do something with the result

      }

      finally {

         result.close();

      }

   }

   finally {

      con.close();

   }

}

catch (RepositoryException? e) {

   // handle exception

}
]]></programlisting>
      <para>This evaluates a SPARQL query and returns a TupleQueryResult, which consists of a
sequence of BindingSet objects. Each BindingSet contains a set of pairs called Binding objects.
A Binding object represents a name/value pair for each variable in the querys projection.
      </para>
      <para>We can use the TupleQueryResult to iterate over all results and get each individual
result for x and y:
      </para>
<programlisting><![CDATA[
while (result.hasNext()) {

   BindingSet bindingSet = result.next();

   Value valueOfX = bindingSet.getValue("x");

   Value valueOfY = bindingSet.getValue("y");

   // do something interesting with the query variable values here?

}
]]></programlisting>
      <para>As you can see, we retrieve values by name rather than by an index. The names used
should be the names of variables as specified in your query. The TupleQueryResult.getBindingNames()
method returns a list of binding names, in the order in which they were specified in the query.
To process the bindings in each binding set in the order specified by the projection, you can do
the following:
      </para>
<programlisting><![CDATA[
List bindingNames = result.getBindingNames();

while (result.hasNext()) {

   BindingSet bindingSet = result.next();

   Value firstValue = bindingSet.getValue(bindingNames.get(0));

   Value secondValue = bindingSet.getValue(bindingNames.get(1));

   // do something interesting with the values here?

}
]]></programlisting>
      <para>It is important to invoke the close() operation on the TupleQueryResult, after we are
done with it. A TupleQueryResult evaluates lazily and keeps resources (such as connections to
the underlying database) open. Closing the TupleQueryResult frees up these resources. Do not
forget that iterating over a result may cause exceptions! The best way to make sure no connections
are kept open unnecessarily is to invoke close() in the finally clause.
      </para>
      <para>An alternative to producing a TupleQueryResult is to supply an object that implements
the TupleQueryResultHandler interface to the query's evaluate() method. The main difference is that
when using a return object, the caller has control over when the next answer is retrieved, whereas
with the use of a handler, the connection simply pushes answers to the handler object as soon as it
has them available.
      </para>
      <para>As an example we will use SPARQLResultsXMLWriter, which is a TupleQueryResultHandler
implementation that writes SPARQL Results XML documents to an output stream or to a writer:
      </para>
<programlisting><![CDATA[
import org.openrdf.query.resultio.sparqlxml.SPARQLResultsXMLWriter;

?

FileOutputStream out = new FileOutputStream("/path/to/result.srx");

try {

   SPARQLResultsXMLWriter sparqlWriter = new SPARQLResultsXMLWriter(out);

   RepositoryConnection con = myRepository.getConnection();

   try {

      String queryString = "SELECT * FROM  WHERE {x} p {y}";

      TupleQuery tupleQuery = con.prepareTupleQuery(QueryLanguage.SPARQL, queryString);

      tupleQuery.evaluate(sparqlWriter);

   }

   finally {

      con.close();

   }

}

finally {

   out.close();

}
]]></programlisting>
      <para>You can just as easily supply your own application-specific implementation of
TupleQueryResultHandler though.
      </para>
      <para>Lastly, an important warning: as soon as you are done with the RepositoryConnection
object, you should close it. Notice that during processing of the TupleQueryResult object
(for example, when iterating over its contents), the RepositoryConnection should still be open.
We can invoke con.close() after we have finished with the result.
      </para>
    </sect4>
    <sect4 id="rdfnativestorageproviderssesamegettingstartevcnq"><title>Evaluating a CONSTRUCT query</title>
      <para>The following code evaluates a graph query on a repository:
      </para>
<programlisting><![CDATA[
import org.openrdf.query.GraphQueryResult;

GraphQueryResult graphResult = con.prepareGraphQuery(

      QueryLanguage.SPARQL, "CONSTRUCT * FROM {x} p {y}").evaluate();
]]></programlisting>
      <para>A GraphQueryResult is similar to TupleQueryResult in that is an object that iterates
over the query results. However, for graph queries the query results are RDF statements, so a
GraphQueryResult iterates over Statement objects:
      </para>
<programlisting><![CDATA[
while (graphResult.hasNext()) {

   Statement st = graphResult.next();

   // ? do something with the resulting statement here.


}
]]></programlisting>
      <para>The TupleQueryResultHandler equivalent for graph queries is org.openrdf.rio.RDFHandler.
Again, this is a generic interface, each object implementing it can process the reported RDF statements
in any way it wants.
      </para>
      <para>All writers from Rio (such as the RDFXMLWriter, TurtleWriter, TriXWriter, etc.) implement
the RDFHandler interface. This allows them to be used in combination with querying quite easily. In
the following example, we use a TurtleWriter to write the result of a SPARQL graph query to standard
output in Turtle format:
      </para>
<programlisting><![CDATA[
import org.openrdf.rio.turtle.TurtleWriter;

?

RepositoryConnection con = myRepository.getConnection();

try {

   TurtleWriter turtleWriter = new TurtleWriter(System.out);

   con.prepareGraphQuery(QueryLanguage.SPARQL, "CONSTRUCT * FROM  WHERE {x} p {y}").evaluate(turtleWriter);

}

finally {

   con.close();

}
]]></programlisting>
      <para>Again, note that as soon as we are done with the result of the query (either after iterating over the contents of the GraphQueryResult or after invoking the RDFHandler),
we invoke con.close() to close the connection and free resources.
      </para>
    </sect4>
    </sect3>
    <sect3 id="rdfnativestorageproviderssesamejavadoc"><title>Javadoc API Documentation</title>
    <para><ulink url="http://docs.openlinksw.com/sesame/">Sesame Provider Javadoc API Documentation</ulink>
is available enabling the complete set of classes, interfaces and methods implemented for the provider to be viewed.
    </para>
    </sect3>
  </sect2>
</sect1>
</chapter>
