<?xml version="1.0" encoding="ISO-8859-1"?>
<!--
 -  
 -  This file is part of the OpenLink Software Virtuoso Open-Source (VOS)
 -  project.
 -  
 -  Copyright (C) 1998-2019 OpenLink Software
 -  
 -  This project is free software; you can redistribute it and/or modify it
 -  under the terms of the GNU General Public License as published by the
 -  Free Software Foundation; only version 2 of the License, dated June 1991.
 -  
 -  This program is distributed in the hope that it will be useful, but
 -  WITHOUT ANY WARRANTY; without even the implied warranty of
 -  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
 -  General Public License for more details.
 -  
 -  You should have received a copy of the GNU General Public License along
 -  with this program; if not, write to the Free Software Foundation, Inc.,
 -  51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
 -  
 -  
-->

<sect2 id="ptune"><title>Performance Tuning</title>
	<!-- ======================================== -->
	<sect3 id="IO">
		<title>I/O</title>
		<sect4 id="DISKIO">
			<title>Optimizing Disk I/O</title>
			<para>
Virtuoso allows splitting a database over several files that
may be on different devices.  By allocating database fragments onto
independent disks I/O performance in both random and sequential database
operations can be greatly enhanced.
</para>
			<para>
The basic unit of a database is the segment. A segment consists of an
integer number of 8K pages.  A segment may consist of one or more files called
stripes. If a segment has multiple stripes these will be of identical
length and the segment size will be an integer multiple of the stripe size.
</para>
			<para>
The size limit on individual database files is platform dependent, but 64 bit file offsets are used where available. 
For large databases use of multiple disks and segments is recommended for reasons of parallelism even though a single database file can get very large. 
A database can in principle grow up to 32TB (32-bit page number with 8KB per page).
</para>
			<para>
When a segment is striped each logically consecutive page resides in a different
file, thus for a segment of 2 stripes the first stripe will contain
all even numbered pages and the second all the odd numbered pages. The
stripes of a segment should always be located on independent disks.
</para>
			<para>
In serving multiple clients that do random access to tables the server can
perform multiple disk operations in parallel, taking advantage of the independent
devices. Striping guarantees a statistically uniform access frequency to all
devices holding stripes of a segment.
</para>
			<para>
The random access advantages of striping are available without any specific configuring
besides that of the stripes themselves.
</para>
		</sect4>
		<sect4 id="IOQS">
			<title>Configuring I/O queues</title>
			<para>
Striping is also useful for a single client doing long sequential
read operations.  The server can detect the serial nature of an operation, for
example a count of all rows in a table and can intelligently prefetch rows.
</para>
			<para>
If the table is located in a striped segment then the server will read all
relevant disks in parallel if these disks are allocated to different I/O queues.
</para>
			<para>
All stripes of different segments on one device should form an I/O queue.
The idea is that the database stripes that benefit
from being sequentially read form a separate queue. All queues are then read and written
independently, each on a different thread. This means that a thread
will be allocated per queue. If no queues are declared all database files, even if located
on different disks share one queue.
</para>
			<para>
A queue is declared in the striping section by specifying a stripe
id after the path of the file, separated by an equal sign.
</para>
			<programlisting>
[Striping]
Segment1 = 200M, disk1/db-seg1-1.db = iq1, disk2/db-seg1-2.db = iq2
Segment2 = 200M, disk1/db-seg2-1.db = iq1, (disk2/db-seg2-2.db = iq2
</programlisting>
			<para>
In the above example the first stripes of the segments form one queue
and the second stripes form another. This makes sense because now all database files
on /disk1 are in iq1 and all on /disk2 are on iq2.
</para>
			<para>
This configuration could have resulted from originally planning a 200 MB database
split on 2 disks and subsequently expanding that by another 200 MB.
</para>
			<para>
The I/O queue identifier can be an arbitrary string. As many background I/O threads
will be made as there are distinct I/O queues.
</para>
			<para>
Striping and using I/O queues can multiply sequential read rates by a factor almost
equal to the number of independent disks.  On the other hand assigning stripes on one disk
to different queues can have a very detrimental effect. The rule is that all that is physically
accessed sequentially will be on the same queue.
</para>
		</sect4>
		</sect3>
	<!-- ======================================== -->
	<sect3 id="SCHEMAS">
		<title>Schema Design Considerations</title>
		<sect4 id="DataOrg">
			<title>Data Organization</title>
			<para>
One should keep the following in mind when designing a schema for
maximum efficiency.
</para>
		</sect4>
		<sect4 id="IndexUsage">
			<title>Index Usage</title>
			<para>
A select from a table using a non-primary key will need to retrieve the
main row if there are search criteria on columns appearing on the main row
or output columns that have to be fetched from the main row. Operations
are noticeably faster if they can be completed without fetching the main
row if the driving key is a non-primary key. This is the case when search
criteria and output columns are on the secondary key parts or primary
key parts. Note that all secondary keys contain a copy of the primary
key. For this purpose it may be useful to add trailing key parts to a
secondary key.	Indeed, a secondary key can hold all the columns of a row
as trailing key parts. This slows insert and update but makes reference
to the main row unnecessary when selecting using the secondary key.
</para>
			<para>
A sequential read on the primary key is always fastest.  A sequential
search with few hits can be faster on a secondary key if the criterion
can be evaluated without reference to the main row. This is because a
short key can have more entries per page.
</para>
		</sect4>
		<sect4 id="SpaceConsump">
			<title>Space Consumption</title>
			<para>
Each column takes the space &apos;naturally&apos; required by its value. No field
lengths are preallocated.  Space consumption for columns is the following:
</para>
			<table colsep="1" frame="all" rowsep="0" shortentry="0" tocentry="1" tabstyle="decimalstyle" orient="land" pgwide="0">
				<title>Data type Space Consumption</title>
				<tgroup align="char" charoff="50" char="." cols="2">
					<colspec align="left" colnum="1" colsep="0" colwidth="20pc"/>
					<thead>
						<row>
							<entry>Data</entry>
							<entry>Bytes</entry>
						</row>
					</thead>
					<tbody>
						<row>
							<entry>Integer below 128</entry>
							<entry>1</entry>
						</row>
						<row>
							<entry>Smallint</entry>
							<entry>2</entry>
						</row>
						<row>
							<entry>long</entry>
							<entry>4</entry>
						</row>
						<row>
							<entry>float</entry>
							<entry>4</entry>
						</row>
						<row>
							<entry>timestamp</entry>
							<entry>10</entry>
						</row>
						<row>
							<entry>double</entry>
							<entry>8</entry>
						</row>
						<row>
							<entry>string</entry>
							<entry>2 + characters</entry>
						</row>
						<row>
							<entry>NULL</entry>
							<entry>data length for fixed length column, as value of 0 length for variable length column. </entry>
						</row>
						<row>
							<entry>BLOB</entry>
							<entry>88 on row + n x 8K (see note below)</entry>
						</row>
					</tbody>
				</tgroup>
			</table>
			<para>
If a BLOB fits in the remaining free bytes on a row after non-LOBs are stored,
it is stored inline and consumes only 3 bytes + BLOB length.
			</para>
			<para>
Each index entry  has an overhead of 4 bytes.
 This applies to the primary key as well as any other keys. The
length of the concatenation of the key parts is added to this. For the
primary key the length of all columns are summed. For any other key the
lengths of the key parts plus any primary key parts not on the secondary
key are summed.  The maximum length of a row is 4076 bytes.
</para>
			<para>
In light of these points primary keys should generally be short.
</para>
		</sect4>
		<sect4 id="PageAlloc">
			<title>Page Allocation</title>
			<para>
For data inserted in random order pages tend to be 3/4 full. For data
inserted in ascending order pages will be about 90% full due to a
different splitting point for a history of rising inserts.
</para>
		</sect4>
		</sect3>
	<!-- ======================================== -->
	<sect3 id="EfficientSQL">
		<title>Efficient Use of SQL - SQL Execution profiling</title>
		<para>
Virtuoso offers an execution profiling mechanism that keeps track of the
relative time consumption and response times of different SQL statements.
	</para>
		<para>
Profiling can be turned on or off with the prof_enable function.  When profiling is on, the
real time between the start and end of each SQL statement execute call is logged on the server.
When prof_enable is called for the second time the statistics gathered between the
last call to prof_enable and this call are dumped to the virtprof.out file in the server&apos;s
working directory.
	</para>
		<para>
Profiling is off by default.  Profiling can be turned on with the statement:
	</para>
		<programlisting>
prof_enable (1);
</programlisting>
		<para>
The virtprof.out file will be generated when prof_enable is called for the second time, e.g.
	</para>
		<programlisting>
prof_enable (0);
</programlisting>
		<para>
will write the file and turn profiling back off.
	</para>
		<para>
Below is a sample profile output file:
	</para>
		<screen>
Query Profile (msec)
Real 183685, client wait 2099294, avg conc 11.428772 n_execs 26148 avg exec  80

99 % under 1 s
99 % under 2 s
99 % under 5 s
100 % under 10 s
100 % under 30 s

23 stmts compiled 26 msec, 99 % prepared reused.

 %  total n-times n-errors
49 % 1041791  7952     53     new_order
28 % 602789   8374     490   delivery_1
12 % 259833   8203     296   payment
5  % 123162   821      35    slevel
2  % 54182    785      0     ostat (?, ?, ?, ?)
0  % 11614    4        0     checkpoint
0  % 2790     2        1     select no_d_id, count (*) from new_orde
0  % 2457     3        1     select count (*) from new_order
0  % 662      2        0     status ()
0  % 11       1        1     set autocommit on
0  % 3        1        0     select * from district
</screen>
		<para>
This file was produced by profiling the TPC C sample application for 3 minutes.
The numbers have the following meanings:
	</para>
		<para>
The real time is the real time interval of the measurement, that is the space in
time between the prof_enable call that started the profiling and the call that wrote the report.
The client wait time is the time cumulatively spent inside the execute call server side, only calls completely
processed between profiling start and end are counted. The average concurrency is the exec time divided by real time
and indicates how many requests were on the average concurrently pending during the measurement interval.
The count of executes and their average duration is also shown.
	</para>
		<para>
The next section shows the percentage of executes that finished under 1, 2, 5, 10 and 30 seconds
of real time.
	</para>
		<para>
The compilation section indicates how many statements were compiled during the
interval.  These will be SQLPrepare calls or SQLExecDirect calls where the text of the statement
was not previously known to the server.  The real time spent compiling the statements is added up.
The percentage of prepared statement reuses, that is, executes not involving compiling over all executes is
shown. This is an important performance metric, considering that it is always better to use prepared
statements with parameters than statements with parameters as literals.
	</para>
		<para>
The next section shows individual statements executed during the measurement interval sorted by
descending cumulative real time between start and completion of the execute call.
	</para>
		<para>
The table shows the percentage of real time spent during the calls to the statement as a percentage
of the total real time spent in all execute calls.  Note that these real times can be higher than the measurement
interval since real times on all threads are added up.
	</para>
		<para>
The second column shows the total execute real time, the third column shows the count of executes of
the statement during the measurement interval.  The fourth column is the count of executes that resulted in an error.
The error count can be used for instance to spot statements that often produce deadlocks.
	</para>
		<para>
Statements are distinguished for profiling purposes using the 40 first characters of their text.
Two distinct statements that do not differ in their first 40 characters will be considered the same for profiling.
If a statement is a procedure call then only the name of the procedure will be
considered, not possibly differing literal parameters.
	</para>
		<para>
The profiler will automatically write the report after having 10000 distinct statements in the measurement
interval.  This is done so as to have a maximum on the profiling memory consumption for applications that always
compile a new statement with different literals, resulting in a potentially infinitely long list of
statements each executed once.  It is obvious that such a profile will not be very useful.
	</para>
		<para>
It cannot be overemphasized that if an application does any sort of repetitive processing then this should be
done with prepared statements and parameters, for both reasons of performance and profilability.
	</para>
		<note>
			<title>Note:</title>
			<para>
Note that measurements are made with a one millisecond precision.  Percentages are rounded to
2 digits.  Timing of fast operations, under a few milliseconds, will be imprecise as a result of the 1 ms resolution.
Also the cumulative compilation time may be substantially off, since the compilation may often take
less than 1 ms at a time.  Also note that the precision may also vary between platforms.
</para>
		</note>
	</sect3>
	<!-- ======================================== -->
	<sect3 id="readingqueryprofile">
    <title>Reading a Query profile</title>
    <para>A query plan is essentially a pipeline of operations that can be read from top to bottom. The operator 
    above produces the input for the operator below. Nesting is indicated by braces. Operators enclosed in braces 
    must completely process their input before continuing with operators after the section in braces. A case in 
    point is aggregation which is demoted by for { ... } ... . The section in braces generates the rows to be 
    aggregated and one it is fully evaluated the execution proceeds with the operators after the fork. These will 
    typically read the temporary table filled by the operators inside the fork.</para>
    <para>Query variables are demoted with &lt;$v...&gt;. A reference to a query variable set by an operator that is not 
    immediately the preceding one is demoted by &lt;r $...&gt; via ...&gt;. The position of the variable in the query 
    state follows the $ sign.</para>
    <para>After this is given the symbolic name of the variable, e.g. a column name or the name of the function of 
    which this is the return value.</para>
    <para>If this is a reference to a query variable assigned earlier in the plan with one or more operators between 
    the reference and the assignment, the reference contains a via .... section which lists the operators that are 
    between the reference and the assignment.</para>
    <para>The operators are identified by a unique number. Internally this is the location in the query state where 
    the operator keeps the mapping between its rows of input and output.</para>
    <para>With vectored execution, an operator may receive any number of rows of input which it may reorder at its 
    convenience. For this it must record for each row of output which row of input this corresponds to. Thus when 
    referencing a query variable set earlier in the plan one must take into account the reordering and 
    addition/dropping of rows in affected by the operators between the reference and the initial assignment. 
    This is the meaning of the via section in the &lt;r $xx &gt; notation.</para>
    <para>The last part of the query variable reference is a two letter indication of the type. If the first 
    letter is followed by n this means that the variable is non-null. The type letters are a for any, r for IRI, 
    i for integer, d for double, f for float, t for datetime,  N for wide string, s for string.</para>
    <para>If the plan is a profile, i.e. it is annotated with actual times and cardinalities from running the query, 
    each operator is preceded by a passage that indicates its percentage of total CPU time of the plan, its fanout 
    and the number of rows of input. The fanout is the number of rows of Output divided by the number of rows 
    of input.</para>
    <para>The principal operators in a plan are:</para>
    <itemizedlist>
      <listitem>
        <emphasis>Cluster outer seq start</emphasis> -- Queries and subqueries begin with this operator. It assigns numbers to the sets of parameter  bindings the query gets as input. An optional section also begins with this. An optional section is the table or derived table on the right hand side of a left outer join.
      </listitem>                               
      <listitem>
        <emphasis>From</emphasis> -- Accessing an index
      </listitem>  
      <listitem>
        <emphasis>Sort</emphasis> -- Group by, order by, distinct or hash join build side
      </listitem>  
      <listitem>
        <emphasis>group by read</emphasis> -- Accessing the results of a group by filled in by a previous sort operator.
      </listitem>  
      <listitem>
        <emphasis>Top oby</emphasis> -- Accessing a previously filled top k order by temporary space
      </listitem>         
      <listitem>
        <emphasis>Select</emphasis> -- At the end of a plan, send the rows of input to the client. If Subq select return the rows of input as rows of output of the enclosing subquery.
      </listitem>                           
      <listitem>
        <emphasis>Subquery</emphasis> -- Indicates a SQL derived table, i.e. select in a from clause. The input is the output of the previous node.
      </listitem>  
      <listitem>
        <emphasis>Cluster ocation fragment</emphasis> -- A set of operators partitioned and shipped to a partition 
        in a cluster. There are single part fragments and distributed fragments (DFG). A DFG is a query fragment 
        which begins with a stage operator. The stage is a partitioning exchange operator which routes its rows 
        of input to the next operator in the appropriate partition based on the partitioningh columns of the row 
        of input.
      </listitem>                    
      <listitem>
        <emphasis>Outer seq end</emphasis> -- This demotes the end of an optional section, i.e. the table/derived 
        table on the right side of left outer join. This has the effect of making aligned vectors of all the vector 
        query variables assigned in the optional section. When producing rows for which there was no match in the 
        optional section, these will be set to null. References to these variables downstream in the plan will be 
        to the shadowing variables, listed in the shadow clause following this.
      </listitem>           
    </itemizedlist>
    <para>Some operators such as index access (From) have a section called vector param casts. This is used when 
    the vector that serves as parameter is not aligned or not of the type required by the operator. For example, 
    a vector of column values in an index lookup might not be of the type of the column but might be of a type 
    castable to this. In any case, the para,key values for index lookup are made into solid aligned, materialized 
    vectors. After this there is a sort step that orders the input according to key order, so that matches are 
    generated from left to right.</para>
    <para>In the event of a cluster partitioned operation, i.e. stage node or non-DFG location fragment (QF), the 
    first table source (From) after the stage or fragment serves to both align and cast all inputs needed 
    downstream in the DFG or QF. The operator first partitions the rows of input. Each partition then gets solid, 
    aligned vectors of input values which are directly suitable for input to the first index lookup (From).</para>
    <para>We consider an example: The setup is a cluster of 4 server processes with each 8 slices of data. There 
    is one table, ct_c, which is joined to itself:</para>
<programlisting><![CDATA[
profile ('SELECT COUNT (*) from ct_c a, ct_c b WHERE a.k2 = b.k1 option (loop, order)');
]]></programlisting>
    <para>This returns a result set in which is found the below. This is also in the ql_plan column of the 
    sys_query_log view:</para>
<programlisting><![CDATA[    
{
-- In the below excerpt some lines are omitted. Comments are inline.


time   1.3e-06% fanout         1          1 rows in

s# 98 cluster outer seq start, set no <V $40 set_ctr in>   
save ctx:()

-- the section inside the fork is executed to the end 
-- before the section after the fork is started.
s# 161 Fork 42
{ 
wait time   0.00012% of exec real time, fanout         0

-- The below introduces the 2 stage DFG which does most of the work.
s# 108   { Cluster location fragment 51 0 unordered
   Params: ()
Output: ()   

-- First partitioning step. Each of the 32 slices gets the 
-- identical plan to execute, this is a flood for all is this 
-- first step does not specify a partitioning key value.

time     8e-05% fanout         0          0 rows in
s# 115 Stage 1: Params: (<V $112 set_ctr in>)

-- This is the initial table scan. The operator is executed 
-- 32 times, once for each slice of data. 

time      0.33% fanout   3.1e+06         32 rows in
s# 121 from DB.DBA.ct_c by ct_c    1.7e+08 rows  
Key ct_c  ASC  (<V $28 a.k2 in>)
 [copies params]
vector param casts: <V $40 set_ctr in>-> <V $112 set_ctr in>
 
-- This is the second partitioning step, each of the rows of 
-- output of the previous is sent to the slice corresponding to K2.

time        31% fanout       0.9      2e+08 rows in
s# 131 Stage 2: Params: (<v $134 q_a.k2 S136 in>, <V $138 set_ctr in>)

-- This is the 2nd index lookup, by a.k2 = b.k1 There are two 
-- parameters given in the vector param cast section, the first 
-- is a.k2 and the second is the set number. This is constant 
-- since the query is run on a single row of input but if the 
-- query were run on multiple rows of input this would specify 
-- which row of input the a.k2 comes from so that this would be 
-- aggregated in the right place. Many aggregates can be produced 
-- in a single invocation with multiple rows of input.

time        68% fanout         0      2e+08 rows in
s# 144 from DB.DBA.ct_c by ct_c  unq         1 rows   
Key ct_c  ASC  ()
 inlined  k1 = <v $134 q_a.k2 S136 in>
vector param casts: <V $28 a.k2 in>-> <v $134 q_a.k2 S136 in>, <r $112 set_ctr via  S121>-> <V $138 set_ctr in>
 
 
-- This is the aggregation.

After code:
      0:  sum <V $43 count i> 1 set no <r $138 set_ctr via  S144>
      5: BReturn 0
 
}
}

-- The below returns the aggregate to the client. The combining of 
-- aggregates from different slices is implicit in the for operator 
-- whenever this ends with a cluster location fragment.

time   5.4e-07% fanout         0          1 rows in
s# 164 Select (<V $43 count i>)
  set no: <V $40 set_ctr in>
}
]]></programlisting>
	</sect3>		
	<sect3 id="tunvectoredexecandhashjoins">
    <title>Tuning Vectored Execution and Hash Joins</title>
    <para><emphasis>Note</emphasis>: This section applies only to versions 7.00 and up.</para>
    <para>Query evaluation performance is significantly affected by parallelization, the vector size and the use 
    of hash joins and fast in-memory hash tables for group by and distinct.</para>
    <para>This section explains the configuration parameters and event counters that allow optimizing these 
    factors.</para>
    <para>The SQL function <emphasis>cl_sys_stat</emphasis> (in name varchar, in clr int := 0) allows reading 
    and optionally resetting these counters. In the case of a cluster, the value returned is the sum of the 
    values of the metric gathered from all server processes, for a single server this is the local value. 
    In a cluster, to get individual counter values, use sys_stat instead when connected to the process of 
    interest.</para>
    <para>To do TPC H at scale 100G on a 32 thread machine, the virtuoso.ini should have the following settings. 
    Only settings at non-default values are mentioned:</para>
<programlisting><![CDATA[    
[Parameters]
ServerThreads              = 100
NumberOfBuffers            = 5000000

;  The working set of the 100G database when stored column-wise is 38G, 
;  so configure this much for database buffers:
; 5000000 / 128            = 39G 

;  Read committed
DefaultIsolation           = 2

;  SQL optimizer work space
MaxMemPoolSize             = 100000000

;  Use large vectors when appropriate
AdjustVectorSize           = 1

;  Split queries in up to 32 independently evaluable fragments, 
;  so up to 32 threads per query.
ThreadsPerQuery            = 32

;  Thread pool has 32 worker threads divided across all queries 
;  in addition to the client thread taken by each query.
AsyncQueueMaxThreads       = 32

;  All queries can collectively take up to 30G without space 
;  saving measures being applied. Space saving measures are 
;  running on small vector size when a large size would be 
;  faster and the use of partitioned (multiple pass) hash join.
MaxQueryMem                = 30G

;  Of the MaxQueryMem, hash join hash tables can take up to 30G,
;  i.e. all of it. A single hash table will only take half of the
;  remaining space, though. So if 2 queries that both need a 30G 
;  hash table run at the same time, the first will do 2 passes, 
;  taking a hash table of 15G at a time, This leaves 20G space. 
;  The second will have 15G left, of which it will take half, 
;  7.5G. This will require 4 passes over the data. 
HashJoinSpace              = 30G

]]></programlisting>
    <para>To analyze the performance of a query workload:</para>
    <orderedlist>
      <listitem>Turn on query logging in sys_query_log. This view contains most metrics and the full 
      text of the query plan with the per operator timings and cardinalities:
<programlisting><![CDATA[
SQL> prof_enable (1);
]]></programlisting>          
      </listitem>Print long result columns without truncating:
<programlisting><![CDATA[
SQL> set blobs on;
]]></programlisting>      
      <listitem>Run the query.</listitem>
      <listitem>Use the profile function for a convenient overview of query execution. For example:
<programlisting><![CDATA[
SQL> profile ('SELECT COUNT (*) FROM orders, lineitem WHERE l_orderkey = o_orderkey');
]]></programlisting>
      <para><link linkend="EfficientSQL">See summary of execution time, CPU%, compilation time and IO</link></para>
      </listitem>
      <listitem>Read the relevant event counters, resetting the count for the next query. For example:
<programlisting><![CDATA[
SQL> select cl_sys_stat ('tc_qp_thread', clr => 1);
]]></programlisting>    
      <para>The relevant counters are:</para>
      <itemizedlist mark="bullet">
        <listitem>
          <emphasis>tc_qp_thread</emphasis> -- How many threads were started for query parallelization. This is not 
          the number of parallel threads as not all of these threads needed to be running at the same time.
         </listitem>
        <listitem>
          <emphasis>tc_part_hash_join</emphasis> -- If a hash join is partitioned, i.e. needs to make multiple passes 
          on over the data, this is the count of passes. This is incremented by 2 if the hash join does 2 passes and not 
          incremented if the hash join goes in a single pass. Normally this should stay at 0 or the hash join space 
          (HashJoinSpace in init, see above) should be increased.
        </listitem>
        <listitem>
          <emphasis>tc_no_mem_for_longer_batch</emphasis> -- This is the count of times the execution engine did not 
          switch to large vectors because there was not enough space. This should normally be 0, if this is not so, 
          increase MaxQueryMem in the ini file.</listitem>
        <listitem>
          <emphasis>tc_slow_temp_insert</emphasis> -- If a distinct or group by temporary space grows over the available 
          query memory, a another data structure will be used so that the hash table can be paged out to disk. This is 
          tens of times less efficient than the memory only structure. This counter is the count of rows inserted into 
          a page-able group by or distinct hash table. This should be 0, if not, increase MaxQueryMem.
        </listitem>
        <listitem>
          <emphasis>mp_max_large_in_use</emphasis> -- This is the maximum amount of query memory that has been allocated 
          to date. Reset this before the query of interest, run and read the counter. This is the peak simultaneous 
          memory use by the query.
        </listitem>
        <listitem>
          <emphasis>mp_large_in_use</emphasis> -- This is the current amount of query memory in use. Do not reset 
          this.
        </listitem>          
        <listitem>
          <emphasis>c_max_large_vec</emphasis> -- This is the MaxQueryMem init setting in bytes. This can be 
          altered at run time with <link linkend="fn_dbf_set"><function>__dbf_set</function></link>.
        </listitem>  
      </itemizedlist>  
      </listitem>
    </orderedlist>    
	</sect3>	
	<sect3 id="ptunehighcardin">
		<title>High Cardinality Group By And Distinct</title>
	  <para>There are multiple implementations of group by and distinct used in different circumstances by different
	  	versions of Virtuoso. Versions of Virtuoso 7 prior to 7.5 have a memory based cuckoo hash based group by and
	  	hash join. 7.5 and onwards have a linear hash based implementation of same. Additionally, all version 7's have a
	  	pageable, i.e. potentially disk based implementation of group by that may get used if there is no space in memory.</para>
	</sect3>
	<sect3 id="ptuneprtgroupby">
		<title>Partitioned Group By</title>
	  <para>Versions 7.5 and onwards have a choice of a partitioned or re-aggregated parallel group by. The group by with
	  	re-aggregation produces an intermediate result on each thread and then adds these up after the completion of each
	  	thread into a final result. This is efficient for low-cardinality cases (few distinct values of grouping columns)
	  	but inefficient for high cardinality. In the high cardinality case, it is better to use a partitioned group by
	  	operator. In the partitioned case, a hash is computed from the grouping columns and this decides which thread gets
	  	to do the aggregation. There is information exchange between threads (or processes in a scale-out setting) but
	  	there is no need for re-aggregation. This is significantly more performant with many groups. The peak memory
	  	utilization is the same.</para>
      <para>The <code>c_setp_partition_threshold</code> setting in the <emphasis>Flags</emphasis> section of virtuoso.ini
      defines when to use a partitioned group by. The default threshold is 100000. If the cost model estimates more distinct
      values than this threshold, a partitioned group by will be produced. In a query plan with explain of profile, a
      partitioned group by is present when there is a stage &lt;n&gt; operator in front of the sort operator for the group by.
      	In a cluster plan, the partitioned group by may be colocated with the table right before it, so the stage operator
      	will be before this.</para>
	</sect3>
	<sect3 id="ptuneorderedgroupby">
		<title>Ordered Group By</title>
	  <para>For version 7.5 onwards, a special case for ordered group by is recognized. In an ordered group by, the system takes
	  	advantage of physical data ordering to limit the memory footprint of the group by: If it is known that grouping keys will
	  	not reoccur past a certain window, then all grouping keys that are processed can be emitted as results in a continuous
	  	stream so that only a window of groups need be kept at a time.</para>
    <para>This is recognized in a query plan by the word "streaming" occurring on the line of the sort operator for the group by.</para>
    <para>Ordered group by is rare to non-existent with RDF, where data orderings with an application meaning do not occur, due
    	to having no multipart keys. With a SQL schema grouping on a set of columns including the primary ordering column of the
    	outermost scan in a plan, an ordered group by will typically be generated. An ordered group by is usually better than a
    	partitioned or re-aggregated group by. The exception may be cases where the outermost ordering column has low cardinality,
    	or a very biased value distribution e.g. the P in the RDF PSOG.</para>
	</sect3>
	<sect3 id="ptunememorygroupby">
		<title>Memory for Group By</title>
	  <para>In Virtuoso 7 prior to 7.5, the memory for a group by is <code> (3 + n_keys + n_aggregates) * 8 </code> bytes per
	  group plus the natural length of any dates or strings in the keys or aggregates. In 7.5 onwards, the memory is 8 bytes
	  less per entry.</para>
	</sect3>
	<sect3 id="ptuneslowgroupby">
		<title>Slow Group By</title>
	  <para>The use of the disk-based group by is easily 100x less efficient than the vectored memory based hash tables.
	  	Falling back to this can be considered an error in configuration and can be detected with the
	  	<code>tc_slow_temp_insert</code> counter readable with <code>sys_stat</code>. If this increases then there is not
	  	enough memory for group by/distinct as used by the application. The monitor may print a message about this in
	  	the server messages log.</para>
    <para>The maximum size of a memory based group by hash table is by default 1/10 of the MaxQueryMem setting in
    	Parameters in the virtuoso.ini file. If a single hash table exceeds this or if the hash table plus the amount
    	of large query memory blocks (e.g. vectors) allocated across the process exceeds MaxQueryMem, then the switch
    	to the slow pageable hash table takes place. This can be avoided by explicitly setting a larger MaxQueryMem
    	or by changing the <code>cha_max_gby_bytes</code> setting in the Flags section of the ini file. We note that
    	for parallel query plans the first condition to trigger is the condition on <code>mp_large_in_use</code> +
    	the hash table memory exceeding MaxQueryMem. A query will have more than 10 threads and the hash tables for
    	memory based hashes are included in <code>mp_large_in_use</code>.</para>
    	<para>To track the general memory consumption in large blocks, use the <code>mp_large_in_use</code> and
    	<code>mp_max_large_in_use meters</code> accessible with <code>sys_stat</code> and <code>__dbf_set</code>.
    	Large blocks are for example vectors, group by's and hash join build sides.</para>
    <para>To interactively try different memory settings, one can modify <code>c_max_large_vec</code> with
    <code>__dbf_set</code>. This is the number of bytes of outstanding large blocks after which pageable group
    by's will be used and vector sizes will no longer be adjusted upward.</para>
	</sect3>
	<sect3 id="tunparamsmworkload">
		<title>Tuning Parameters for Multiuser Workloads</title>
		<para>This section describes parameters and event counters that affect memory and parallelism and are 
			specially relevant for high concurrency situations. Different switches and event counters are described. 
			Setting these requires dba privileges. The event counters are accessed with <emphasis>sys_stat</emphasis> 
			and set with <link linkend="fn_dbf_set"><function>__dbf_set</function></link>. For example: </para>
<programlisting><![CDATA[
select sys_stat ('tc_qp_thread');

or 

__dbf_set ('dc_batch_sz', 10000);
]]></programlisting>
    <para>Some of the parameters have corresponding INI file settings. All of these can be set in an 
    	INI file in the [Flags] section with a stanza like: </para>
<programlisting><![CDATA[
dc_batch_sz = 10000 
]]></programlisting>
    <para>First, make sure that the <emphasis>ServerThreads</emphasis> in either 
    <link linkend="ini_Parameters">[Parameters] ini section</link> (if application with connected clients) or 
    in <link linkend="ini_HTTPServer">[HTTPServer] ini section</link> (if over web protocols), is larger than 
    the number of actual connections.
    </para>
    <para>Next, the workload should be profiled to see its memory utilization and intrinsic use of parallelism. 
    	Splitting a query into independently executable parallel fragments increases its memory utilization, 
    	Switching to larger vector sizes also increases a query's memory utilization while providing increased 
    	locality of data access. Together these techniques can result in a 100+ fold difference in the transient 
    	memory consumption of a query. Use of hash joins may also increase the transient memory consumption since 
    	in a hash join one half of the join must be materialized.</para>
    <para>With relatively small numbers of concurrent queries, e.g. 10, these techniques are nearly always 
    	beneficial. With hundreds of queries the memory overhead may become a limiting factor.</para>
    <para>The <emphasis>MaxQueryMem</emphasis> INI file setting controls how much memory is kept for query 
    execution. This amount of space is maintained allocated. Transient memory use may exceed this but then the 
    memory above this amount is unmapped when no longer needed. Mapping and unmapping memory takes time. 
    Concurrent mmap and munmap calls on different threads will serialize and bring down the CPU utilization of a 
    process even when there is enough parallelism for full platform utiliization.</para>
    <para>The <emphasis>mp_mmap_clocks</emphasis> counter is the cumulative amount of real time in CPU clocks 
    spent in mmap or munmap systtem calls. This can increase faster than real time if multiple threads are 
    involved.</para>     
    <para>If this value grows fast, e.g. at over 10% of real time, the monitor prints a message in the server 
    	event log. This is an indication that memory utilization may have to be tuned.</para>
    <para>For analyzing the behavior of a workload as concerns memory, do the following:</para>  
    <itemizedlist mark="bullet">
      <listitem>Run the workload once for warmup.</listitem>
      <listitem>Clear the following counters:
        <itemizedlist mark="bullet">
          <listitem><code>mp_max_large_in_use</code> -- Highest to date amount of concurrently used memory</listitem>
          <listitem><code>tc_qp_thread</code> -- count of times a query made a parallel executive fragment. Not increased if running single threaded.</listitem>
          <listitem><code>tc_adjust_batch_sz</code> -- count of times vector size was switched to larger</listitem>
          <listitem><code>mp_mmap_clocks</code> -- cumulative time inside mmap or munmap.</listitem>
        </itemizedlist>      	
       <para>For example:</para>
<programlisting><![CDATA[
__dbf_set ('mp_max_large_in_use', 0);	
]]></programlisting>
      </listitem>
      <listitem>Run the workload on a single thread with default settings.</listitem>
      <listitem>Observe the values.</listitem>
    </itemizedlist>
	  <sect4 id="tunparamsmworkloadex">    
	  	<title>Example Scenario</title>
      <orderedlist>
        <listitem>Clear the counters.</listitem>
        <listitem>Set <code>enable_dyn_batch_sz</code> to 0, causing queries to only use the initial vector size, 
        <code>dc_batch_sz</code> or <code>VectorSize</code> in the INI file.</listitem>
        <listitem>Set <code>enable_qp</code> to 1, causing queries to run single threaded.</listitem>
        <listitem>Run the workload on a single thread. The difference in elapsed real time will show how much 
        	benefit the workload has from intra-query parallelization and long vector sizes. If the workload used 
        	any of the memory consuming techniques the peak memory in the second case will be lower. How much lower 
        	is entirely workload dependent.
          <itemizedlist mark="bullet">
            <listitem><emphasis>Note</emphasis>: One may additionally try the workload on a single thread with 
            <code>hash_join_enable</code> set to 0 and 2, respectively. 0 means no hash join plans are made, 
            2 means that hash join is used when appropriate for either SQL or SPARQL queries. The peak memory 
            utilization and run times may be significantly affected.</listitem>
          </itemizedlist>
        </listitem>
        <listitem>Having completed these metrics, one may move to the multi-user case. Note that 
        	<code>MaxQueryMem</code>, (<code>c_max_large_vec</code> setting with 
        	<link linkend="fn_dbf_set"><function>__dbf_set</function></link>) should be set to a reasonable 
        	value, e.g. the peak consumption with the chosen settings times the number of parallel sessions. 
        </listitem>
        <listitem>Expected Results -- Experience demonstrates for example that with a 128 concurrent users 
        	<code>setting enable_qp</code> to 1 and <code>enable_dyn_batch_sz</code> to 0 increased the throughput 
        	of the sample workload by a factor of 2.5. The workload derived, even in single user mode, little 
        	benefit from dynamic vector size or multithreading, under 30%. In the case at hand, the difference in 
        	performance was mostly accounted for by <code>mmap</code>, see the <code>mp_mmap_clocks</code> 
        	counter. 
          <itemizedlist mark="bullet">
            <listitem>If more concurrent queries are than CPU cores are expected, there is little point in 
            	intra-query parallelism, controlled by <code>ThreadsPerQuery</code> in the INI file or 
            	<code>enable_qp</code> in <link linkend="fn_dbf_set"><function>__dbf_set</function></link>.
            </listitem>
            <listitem>If <code>mp_mmap_clocks</code> continues to grow fast during the execution one may 
            increase <code>c_max_large_vec</code>. This will cause more <code>mmap</code>'s to be kept in 
            reserve, thus in principle decreasing the frequency of <code>mmap</code> and <code>munmap</code> 
            system calls.</listitem>            
            <listitem>If running with hash join enabled, there is a possibility of partitioned hash joins where 
              the query executes in multiple passes in order to build smaller hash tables. This is given by 
            	<code>tc_part_hash_join</code>, which is increased by 1 for each non-first pass over a hash join. 
            	If the counter increases the <code>HashJoinSpace</code> setting in the INI file should be 
            	increased. With <link linkend="fn_dbf_set"><function>__dbf_set</function></link> this is 
            	<code>chash_space_avail</code>.
            </listitem>
          </itemizedlist>        	
        </listitem>
        <listitem>Notes:
          <itemizedlist mark="bullet">
            <listitem>The results from above may not happen with a single user but happen all the time with 
            	multiple users. The monitor will print a warning message about this in the message log.
            </listitem>
            <listitem>One may also try a different default vector size, specially if very pressed on memory. 
            	The default of 10000 values is generally a good small value but smaller may be possible, 
            	however not under 1000.
            </listitem>
          </itemizedlist>        	
        </listitem>                                                
      </orderedlist>
	  </sect4>
	</sect3>			
  <sect3 id="querylogging">
    <title>Query Logging</title>
    <para>As of version 7.00 Virtuoso offers optional server side query logging. This records a large number 
    of execution statistics as well as the full query text and execution plan with per-operator timing and 
    cardinality information.</para>
    <para>This feature is enabled in the Parameters section of virtuoso.ini:</para> 
<programlisting><![CDATA[
; virtuoso.ini
...
[Parameters]
QueryLog = filename 
]]></programlisting>    
    <para>At run time, this may be enabled or disabled with prof_enable (), overriding the specification of the 
    ini file. The default file name for the query log is virtuoso.qrl in the server's working directory, if not 
    otherwise specified in the ini file.</para>
    <para>The file is in binary format and is not conveniently readable as such. It is most easily accessed via 
    the DB.DBA.SYS_QUERY_LOG system view. This view has parameters for specifying a non-default file path for the 
    query log file as well as a datetime range for selecting the entries of interest.</para>
    <para>For example:</para>
<programlisting><![CDATA[
SELECT * 
  FROM sys_query_log 
 WHERE qrl_file = 'other.qrl' 
   AND qrl_start_dt = cast ('2011-10-1' as datetime) 
   AND qrl_end_dt = cast ('2011-10-2' as datetime)
]]></programlisting>
    <para>This will select entries from the file other.qrl that are between the given dates. Note that the qrl* 
    columns are not actual result column of the view but are considered as parameters, hence the use of = instead 
    of a range condition.</para>
    <para>All statements executed over a SQL client connection (ODBC/JDBC etc) are logged, as well as any 
    statements executed on an SPARQL end point. DAV and web services requests are not logged unless they perform 
    an exec function call. CPU usage for committing transactions or for background data organization such as 
    autocompact or automatic checkpoints are not logged.</para>
    <para>A select statement is logged at the time it produces its last row of output, not at the time this row of 
    output is fetched by a client.</para>
    <para>Some of the columns are in units of clocks, whose meaning varies from system to system. On Intel this 
    corresponds to the value returned by the RDTSC instruction. All values are intervals. The relation of this 
    to real time may vary in function of automatic variation of clock frequency.</para>
    <para>The columns of SYS_QUERY_LOG  are as follows:</para>
    <itemizedlist mark="bullet">
      <listitem>
        <emphasis>ql_id</emphasis> -- Serial number of the log entry. If the server was started many times 
        with the same file these will not be unique. Combine with <emphasis>ql_start_dt</emphasis> for unique identification.
      </listitem>
      <listitem>
        <emphasis>ql_start_dt</emphasis> -- datetime of the start of the query
      </listitem>
      <listitem>
        <emphasis>ql_rt_msec</emphasis> -- real time elapsed in milliseconds between the start and the logging 
        of the query
      </listitem>
      <listitem>
        <emphasis>ql_rt_clocks</emphasis> -- Clock cycles of real time spent running the query, not including time 
        between consecutive fetches from a client if the query was a cursor fetched in multiple chunks by a client. 
        This is the number of clocks during which there was at least one thread running on behalf of the query. 
        The average CPU% of the query is given by:
<programlisting><![CDATA[
100 * ql_thread_clocks / ql_rt_clocks
]]></programlisting>        
        </listitem>
      <listitem>
        <emphasis>ql_client_ip</emphasis> -- Requesting client IP as dotted decimal.
      </listitem>
      <listitem>
        <emphasis>ql_user</emphasis> -- User account on behalf of which the query was executed.
      </listitem>
      <listitem>
        <emphasis>ql_sqlstate</emphasis> -- SQL state if query terminated with error, NULL otherwise.
      </listitem>
      <listitem>
        <emphasis>ql_error</emphasis> -- Error message if query terminated with error, NULL otherwise.
      </listitem>
      <listitem>
        <emphasis>ql_swap</emphasis> -- Cumulative count of major page faults since startup of this Virtuoso process.
      </listitem>
      <listitem>
        <emphasis>ql_user_cpu</emphasis> -- Cumulative user CPU in milliseconds for this server process.
      </listitem>        
      <listitem>
        <emphasis>ql_sys_cpu</emphasis> -- Cumulative system CPU in milliseconds for this server process.
      </listitem>   
      <listitem>
        <emphasis>ql_text</emphasis> -- Source text of the query
      </listitem>   
      <listitem>
        <emphasis>ql_params</emphasis> -- NULL.
      </listitem>   
      <listitem>
        <emphasis>ql_plan_hash</emphasis> -- Hash number calculated from the  execution plan, ignoring literals. 
        Can be used for grouping executions of the same query with differing literals together. If difference of 
        literals causes a different plan, this number will be different.
      </listitem>     
      <listitem>
        <emphasis>ql_c_clocks</emphasis> -- CPU clocks of real time used for query compilation. This will be 0 if 
        the query is separately prepared of if the query compilation comes from a cache of recently compiled 
        queries. This is likely if the query is parametrized and executed multiple times.
      </listitem>   
      <listitem>
        <emphasis>ql_c_msec</emphasis> -- Real time used for query compilation in milliseconds.
      </listitem>                                                     
      <listitem>
        <emphasis>ql_c_disk</emphasis> -- Count of disk reads done on behalf of the query compilation, this 
        stands for index sampling initiated by the compilation and does not include any speculative read 
        possibly triggered by the sampling.
      </listitem>    
      <listitem>
        <emphasis>ql_c_disk_wait</emphasis> -- Count of clocks the compilation was blocked waiting for disk.
      </listitem>  
      <listitem>
        <emphasis>ql_c_cl_wait</emphasis> -- Count of clocks the compilation was waiting for information from 
        cluster peers. Such waiting indicates sampling done on remote partitions. If the run time of the query 
        is small, this may be a significant factor in query execution real time.
      </listitem>  
      <listitem>
        <emphasis>ql_cl_messages</emphasis> -- Count of distinct cluster messages sent on behalf of the compilation. 
        These are all related to sampling. Many samples can be combined into one message in some situations. 
        Samples are also cached on the requesting server so repeatedly compiling the same statement will send 
        the messages only the first time in unless the cache has timed out in the meantime.
      </listitem>  
      <listitem>
        <emphasis>ql_c_rnd_rows</emphasis> -- Count of rows retrieved as part of compile time sampling.
      </listitem>                               
    </itemizedlist>
    <para>The below columns correspond directly to the fields returned by db_activity (). These are summed over 
    all the threads in all the hosts that have done something on behalf of the logged query.</para>
    <itemizedlist>
      <listitem>
        <emphasis>ql_rnd_rows</emphasis> -- Count of random row lookups. Each sequential lookup begins with 
        one random one for each partition concerned.
      </listitem>                               
      <listitem>
        <emphasis>ql_seq_rows</emphasis> -- Sequential rows fetched, each non-first row which is selected counts 
        as one. A row that is looked at but does not satisfy applicable query criteria does not count.
      </listitem>  
      <listitem>
        <emphasis>ql_same_seg</emphasis> -- Count of times the next random lookup in a vectored lookup falls on 
        the same column-wise segment as the previous.
      </listitem>  
      <listitem>
        <emphasis>ql_same_page</emphasis> -- Ibid, for the next lookup falling on the same row-wise leaf page.
      </listitem>  
      <listitem>
        <emphasis>ql_same_parent</emphasis> -- Ibid, for the case of the next lookup falling on a sibling page 
        of the row-wise leaf page of the previous lookup.
      </listitem>                          
    </itemizedlist>
    <para>For column-wise indices, all the three above counters can be non-zero since these consist of multi-row 
    segments each under a row on a row-wise leaf page. For a row-wise index the same seg counter is always 0.</para>
    <itemizedlist>
      <listitem>
        <emphasis>ql_thread_clocks</emphasis> -- Sum of clocks spent on any thread for the logged query. Time 
        spent waiting for other threads, for disk or for replies from cluster peers is not counted, thus only 
        running cycles count. These are added up across all hosts in a cluster.
      </listitem>                               
      <listitem>
        <emphasis>ql_disk_wait_clocks</emphasis> -- Total clocks any thread spends waiting for disk on behalf 
        of the query. If two threads wait for the same page which is fetched once the wait is counted double. 
        This is not the same as the total read time of the pages since read ahead can fetch pages before they 
        are needed, thus involving no wait.
      </listitem>  
      <listitem>
        <emphasis>ql_cl_wait_clocks</emphasis> -- Total clocks a thread running on behalf of the query spends 
        waiting for a cluster reply. This may be zero if the coordinating thread has work until any cluster 
        replies arrive, in which case there will be no wait.  If this is high then the workload is bound by 
        interconnect or is unevenly distributed across a cluster.
      </listitem>  
      <listitem>
        <emphasis>ql_pg_wait_clocks</emphasis> -- Count get page wait.
      </listitem>  
      <listitem>
        <emphasis>ql_disk_reads</emphasis> -- Count disc reads.
      </listitem>   
      <listitem>
        <emphasis>ql_spec_disk_reads</emphasis> -- Count of speculative disk reads triggered on behalf of the 
        query. Any read ahead or any upgrading of a single page read into a read of a whole extent counts towards 
        this, only allocated pages that are read are counted but merging near-adjacent reads may cause actually 
        more disk IO to take place.
      </listitem>  
      <listitem>
        <emphasis>ql_messages</emphasis> -- Count of distinct cluster messages sent on behalf of the query. Any 
        message is counted once. Client-server messages are not counted.
      </listitem>   
      <listitem>
        <emphasis>ql_message_bytes</emphasis> -- Total bytes sent in all cluster messages  sent on behalf 
        of the query.
      </listitem>  
      <listitem>
        <emphasis>ql_qp_threads</emphasis> -- Count of times an extra thread is created for parallelizing 
        work on the query in question. The count may be high since consecutively launched threads are counted, 
        this is not a maximum degree of concurrency.
      </listitem>   
      <listitem>
        <emphasis>ql_vec_bytes</emphasis> -- reserved
      </listitem>  
      <listitem>
        <emphasis>ql_vec_bytes_max</emphasis> -- reserved
      </listitem>   
      <listitem>
        <emphasis>ql_lock_waits</emphasis> -- Count of times a thread has waited for a lock on behalf of the query.
      </listitem>  
      <listitem>
        <emphasis>ql_lock_wait_msec</emphasis> -- Total milliseconds any thread has spent waiting for a lock on 
        behalf of the query. This may be longer than real time since many threads may wait at the same time.
      </listitem>                                                  
      <listitem>
        <emphasis>ql_plan</emphasis> -- Text representation of the execution plan, annotated with CPU time 
        percentages and fanouts for the different operators. Fanout is the count of output rows divided by 
        the count of input rows.
      </listitem>     
      <listitem>
        <emphasis>ql_node_stat</emphasis> -- reserved
      </listitem>     
      <listitem>
        <emphasis>ql_c_memory</emphasis> -- Count of bytes allocated for compiling the query. This is the peak 
        size of the memory pool for query compilation.
      </listitem>     
      <listitem>
        <emphasis>ql_rows_affected</emphasis> -- Count of inserted/updated/deleted rows. If the query was a 
        select with a top and an order by, this is is the count of produced rows before the top restriction 
        was applied.
      </listitem>     
    </itemizedlist>
  </sect3>    
	<!-- ======================================== -->
	<sect3 id="METERS_SYSVIEWS">
		<title>Meters &amp; System Views</title>
		<sect4 id="kdlstat">
			<title>DB.DBA.SYS_K_STAT, DB.DBA.SYS_D_STAT, DB.DBA.SYS_L_STAT view</title>
			<para>
These views provide statistics on the database engine
</para>
			<programlisting>
create view SYS_K_STAT as
  select KEY_TABLE, name_part (KEY_NAME, 2) as index_name,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;n_landings&apos;) as landed,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;total_last_page_hits&apos;) as consec,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;page_end_inserts&apos;) as right_edge,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;page_end_inserts&apos;) as lock_esc
	from SYS_KEYS;
</programlisting>
			<programlisting>
create view SYS_L_STAT as
  select KEY_TABLE, name_part (KEY_NAME, 2) as index_name,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;lock_set&apos;) as locks,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;lock_waits&apos;) as waits,
	(key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;lock_waits&apos;) * 100)
	  / (key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;lock_set&apos;) + 1) as wait_pct,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;deadlocks&apos;) as deadlocks,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;lock_escalations&apos;) as lock_esc
	from SYS_KEYS;
</programlisting>
			<programlisting>
create view sys_d_stat as
  select KEY_TABLE, name_part (KEY_NAME, 2) as index_name,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;touches&apos;) as touches,
	key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;reads&apos;) as reads,
	(key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;reads&apos;) * 100)
	&#10; &gt; / (key_stat (KEY_TABLE, name_part (KEY_NAME, 2), &apos;touches&apos;) + 1) as read_pct
	from SYS_KEYS;
</programlisting>
			<para>
These views offer detailed statistics on index access locality, lock contention and
disk usage.
</para>
<!--
- Uncommented for Virtuoso 6 Release-->
 <para>
 'reset' specified as the stat name will reset all counts for  the key in question.
 </para>
		</sect4>
		<sect4 id="keystats">
			<title>SYS_K_STAT - Key statistics</title>
			<itemizedlist mark="bullet">
				<listitem>
					<para>KEY_TABLE	The fully qualified table name, e.g. DB.DBA.SYS_PROCEDURES</para>
				</listitem>
				<listitem>
					<para>INDEX_NAME	The name of the index. This will be equal to the table name for the table&apos;s primary key.</para>
				</listitem>
				<listitem>
					<para>LANDED		The count of random accesses, including inserts.  Any insert or select, whether empty, single line or multi-
		line counts as one. Updates and deletes do not count, as they imply a select in the same or previous statement.</para>
				</listitem>
				<listitem>
					<para>CONSEC		The number of times a random access falls on the same page as the previous random access. This is always less than LANDED.
		For repetitive access to the same place or an ascending insert, this will be near LANDED. For a totally
		random access pattern this will be near 0.</para>
				</listitem>
				<listitem>
					<para>RIGHT_EDGE	The number of times an insert has added a row to the right edge of the page where the insert was made. </para>
				</listitem>
				<listitem>
					<para>LOCK_ESC	The count of lock escalations, see SYS_L_STAT.</para>
				</listitem>
			</itemizedlist>
		</sect4>
		<sect4 id="lstats">
			<title>SYS_L_STAT</title>
			<itemizedlist mark="bullet">
				<listitem>
					<para>KEY_TABLE	The fully qualified table name, e.g. DB.DBA.SYS_PROCEDURES</para>
				</listitem>
				<listitem>
					<para>INDEX_NAME	The name of the index. This will be equal to the table name for the table&apos;s primary key.</para>
				</listitem>
				<listitem>
					<para>LOCKS		The number of times a lock has been set on the index. Making a new row or page lock
		counts as one. Entering a row or page lock either after a wait or without wait (for a shared lock) counts as one.</para>
				</listitem>
				<listitem>
					<para>WAITS		The number of times a cursor reading this index waited for a lock. Note that this can be higher
		than the number of locks set, e.g. a &apos;read committed&apos; cursor may wait for a lock but will never make one.</para>
				</listitem>
				<listitem>
					<para>WAIT_PCT	The percentage of lock set events that involved a wait. </para>
				</listitem>
				<listitem>
					<para>DEADLOCKS	The number of times a deadlock was detected when trying to wait for a lock on this index.
		Note that one deadlock event may involve locks on several indices.  Each deadlock detection counts as one.</para>
				</listitem>
				<listitem>
					<para>LOCK_ESC	The number of times the set of row locks on a page of this index where escalated into one page lock
		or a page lock was set initially. This is always less than LOCKS. This value will
		be near LOCKS when there are many sequential selects which switch to page lock mode.
		This happens when a cursor has performed over 2 lock escalations and the page being entered has no
		locks, i.e. the lock can be set over the entire page.</para>
				</listitem>
			</itemizedlist>
		</sect4>
		<sect4 id="dstats">
			<title>SYS_D_STAT</title>
			<itemizedlist mark="bullet">
				<listitem>
					<para>KEY_TABLE	The fully qualified table name, e.g. DB.DBA.SYS_PROCEDURES</para>
				</listitem>
				<listitem>
					<para>INDEX_NAME	The name of the index. This will be equal to the table name for the table&apos;s primary key.</para>
				</listitem>
				<listitem>
					<para>TOUCHES		The number of times a row is located on the index. Every row retrieved by a select or inserted counts as
		one. All rows scanned by an select count or other aggregate counts as one. </para>
				</listitem>
				<listitem>
					<para>READS		The number of times a disk read was caused by a read operation on this index.
		This may theoretically be higher than TOUCHES, since several levels of the index tree may have
		to be read to get to a leaf.</para>
				</listitem>
				<listitem>
					<para>READ_PCT	The percentage of READS in TOUCHES.</para>
				</listitem>
			</itemizedlist>
			<example>
				<title>Examples:</title>
				<programlisting>
select index_name, locks, waits, wait_pct, deadlocks
    from sys_l_stat order by 2 desc;

Get lock data, indices in descending order of lock count.

select index_name, touches, reads, read_pct
    from sys_d_stat order by 3 desc;

Get disk read counts, index with most reads first.

select index_name, (consec * 100) / (landed + 1)
    from sys_k_stat where landed &gt; 1000  order by 2;
</programlisting>
			</example>
			<para>Get the percentage of consecutive page access on indices with over 1000 accesses so far,
most randomly accessed first.
</para>
		</sect4>
		<sect4 id="statusfunc">
			<title>status SQL function - status ();</title>
			<para>This function returns a summary of the database
status as a result set. The result set has one varchar column,
which has consecutive lines of text. The lines can be up to several hundred
characters.
</para>
			<para>The contents of the status summary are described in
      the <link linkend="DBSTAT">Administrator's Guide</link>.</para>
		</sect4>

		<sect4 id="statusfunc">
			<title>Virtuoso db file usage detailed info</title>

<para>All data in a virtuoso database are logically stored as database key rows.
Thus the primary key for a table holds the entire row (including the dependent
part) and the secondary keys just hold their respective key parts.
So the space that the table occupies is the sum of the space occupied by it's
primary key and all the secondary keys.</para>
<para>The main physical unit of allocation in a virtuoso db file is the database page
(about 8k in virtuoso 3.x).
So the server needs to map the key rows and outline blobs to database pages.</para>
<para>Virtuoso will store as many rows in a db page as it can, so usually one DB page
will contain more than 1 row of a given key. No page contains rows from more than one key. However
blobs (when not inlined on the row) will be placed in consecutive DB pages (up
to their size). In addition to the blob and key pages the Virtuoso DB will hold a number of
pages containing internal data. So the sum of the pages occupied by the key rows and the blobs is
leas then the amount of occupied pages (as reported by the 
<link linkend="fn_status"><function>status()</function></link> BIF).</para>

<para>To provide detailed information about the space consumption of each key there's
a system view:</para>
<programlisting>
DB.DBA.SYS_INDEX_SPACE_STATS
    ISS_KEY_TABLE       varchar -- name of the table
    ISS_KEY_NAME        varchar -- name of the key
    ISS_KEY_ID          integer -- id of the key (corresponding to KEY_ID from DB.DBA.SYS_KEYS)
    ISS_NROWS           integer -- number of rows in the table
    ISS_ROW_BYTES       integer -- sum of the byte lengths of all the rows in the table
    ISS_BLOB_PAGES      integer -- sum of the blob pages occupied by the outline blobs on all the rows of the table
    ISS_ROW_PAGES       integer -- sum of all the db pages containing rows of this key
    ISS_PAGES           integer -- = ISS_BLOB_PAGES + ISS_ROW_PAGES (for convenience).
</programlisting>

<para>Each select on that view causes the server to go over all the db pages in the db file
(similarly to how the crash dump operates) and collect the statistics above. The pages are
traversed 1 time per select, but still on large database files that may take some time.	</para>
</sect4>

	</sect3>

<sect3 id="TRANSACTION_ISOLATION_LEVELS"><title>Transaction Metrics, Diagnostics and Optimization</title>


    <para>Bad design and implementation of transactions affects
    applications in the following ways:</para>

    <itemizedlist mark="bullet" spacing="compact">
      <listitem>Performance is lost by having to needlessly retry
      transactions that are aborted by deadlock.</listitem>

      <listitem>Concurrency is lost by having rows stay locked for too
      long.</listitem>

      <listitem>Memory is transiently consumed, adversely affecting
      working set, by keeping data structures for too many simultaneous locks,
      rollback records and uncommitted roll forward logs.</listitem>
    </itemizedlist>


    <para>The following rules should be observed when writing
    transactions:</para>

    <itemizedlist mark="bullet" spacing="compact">
      <listitem>Do not lock needlessly. For example, any report
      transaction that reads the data once can always be done as read committed instead of
    repeatable read without affecting semantics. Even if some data
    is read multiple times, the repeatable read semantic is
    typically not relevant for reports.</listitem>

      <listitem>Lock for what is needed. If you mean to update later, do
      the initial read with exclusive locks. See the for update clause in select,
    for example.</listitem>

      <listitem>Lock in constant order. If you must lock different
      resources in one
    transaction, lock them always in the same order. When updating
    stock for an order, update the quantity on hand in increasing
    order of item number, for instance. 
	  </listitem>

      <listitem>Lock the item with the least contention first. For
      example, update the
    detail before updating the summary. Update the quantity in
    stock for the ordered items before updating the orders count of
    the whole warehouse. 
	  </listitem>

      <listitem>Keep transactions short. Use stored procedures. Use the
      explicit commit work statement.</listitem>

      <listitem>For each transaction in a stored procedure, make sure
      that if it is
    deadlocked, the deadlocked transaction gets retried. For
    example, have a "declare exit handler for sqlstate 40001" for
    every transaction context. Make sure that a deadlocking
    transaction is never retried endlessly. Two mutually
    deadlocking transactions can keep retrying and again
    deadlocking each other endlessly. To avoid this, have a maximum
    count of retries and a random delay before restarting. The
    restart delay should be between 0 and the expected duration of
    the transaction. 
	  </listitem>

      <listitem>Always break batch updates into multiple transactions.
      Update a few
    thousand or tens of thousands of rows per transaction, never
    more than that. Failing to do this makes for prohibitive cost
    of retry with deadlocks and can cause swapping by keeping tens
    or hundreds of megabytes in rollback state, locks and other
    transaction temporary structures. This happens if one inserts,
    updates, deletes several million rows in a single transaction.
    If this is really needed and concurrency is no issue, use the
    atomic mode, effectively making the server single user for the
    transaction, thus having no locking or rollback. See the use of 
    the <emphasis>__atomic()</emphasis> function in the 
    <link linkend="faultfaulttoleradmapi">Cluster Administration API</link>.
	  </listitem>
    </itemizedlist>

    <sect4 id="ptuneprogvirtpl"><title>Programming Virtuoso/PL</title>

    <para>The isolation level is set in Virtuoso/PL with the</para>

<programlisting>
set isolation := level;
</programlisting>

    <para>statement, where level is one of 'serializable',
    'repeatable', 'committed', 'uncommitted'. Example :</para>
<programlisting>
set isolation = 'serializable';
</programlisting>

    <para>
	The standard SQL syntax is also supported :</para>

<programlisting>
SET TRANSACTION ISOLATION LEVEL &lt;isolation_level&gt;
isolation level : READ UNCOMMITTED | READ COMMITTED | REPEATABLE READ | SERIALIZABLE
</programlisting>

 <para>
 Example :</para>

<programlisting>
SET TRANSACTION ISOLATION LEVEL READ COMMITTED
</programlisting>

    <para>The effect is for the rest of the procedure and any
    procedure called from this procedure. The effect stops when the
    procedure having executed the set isolation statement
    returns.</para>

	</sect4>
  <sect4 id="ptunetransparallel"><title>Transaction Parallelism</title>
	  <para>From Virtuoso 7.1 onwards, data manipulation statements can be multithreaded. The virtuoso.ini <code>[Flags]</code> settings
	  <code>enable_mt_txn</code> and <code>enable_mt_transact</code> control transaction parallelism. These are 1 for enable and 0 for
	  disable. Defaults are version dependent, off in 7.1 and 7.2 and on in subsequent. The
	  <link linkend="fn_sys_stat"><function>sys_stat()</function></link> and
	  <link linkend="fn_dbf_set"><function>__dbf_set()</function></link> functions can be used for querying and altering these.</para>

	  <para>DML statements, e.g. insert, delete, update, are automatically multithreaded according to <code>enable_qp</code>
	  (<code>ThreadsPerQuery</code> in ini [Parameters] when <code>enable_mt_txn</code> is on).</para>

	  <para>RDF load (<link linkend="fn_ttlp"><function>DB.DBA.ttlp()</function></link> and related functions) when in transactional
	  mode (log_enable 0 or 1)  are also multithreaded when <code>enable_mt_txn</code> is on. Otherwise these are multithreaded when
	  in non-transactional mode (<code>log_enable</code> 2 or 3) and  single threaded in transactional mode.</para>

	  <para>The <code>enable_mt_transact</code> setting controls the parallelism of a commit or rollback. Specially for column store
	  deletes, the commit may take a long time, as this is the point where the physical deletion takes place. To accelerate this,
	  <code>enable_mt_transact</code> should be on. This is however not worthwhile for short row store transactions, as the overhead
	  of making threads is greater than the time it takes to finalize a transaction om a page. Again, large, scattered row store
	  deletes may justify use of <code>enable_mt_transact</code>.</para>
  </sect4>
	<sect4 id="ptunesampledeadlockhandler"><title>Sample Deadlock Handler</title>


    <para>The text for a deadlock handler is</para>

<programlisting>
  declare retry_count int;
  retry_count := 0;
retry:
  {
    declare exit handler for sqlstate '40001' 
      {
        rollback work;
        ... 
        delay (rnd (2.5));  --- if 2.5 seconds is the expected duration of
the transaction.
        ...

        retry_count := retry_count + 1;
        if (retry_count > 5)
          signal ("xxxxx", "Too many deadlock retries in xxxxx.");
        goto retry;
      }
   -- do the operations.  The actual working code here.

    commit work;
  }
</programlisting>

    <para>An exclusive read is done with</para>
<programlisting>
select s_quantity from stock where s_i_id = 111 for update;
</programlisting>

	</sect4>

	<sect4 id="ptuneodbciso"><title>ODBC</title>

    <para>For the Virtuoso ODBC driver the isolation
    is set by :</para>

    <itemizedlist mark="bullet" spacing="compact">
      <listitem>connection option (in either <ulink url="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/odbc/htm/odbcsqlsetconnectattr.asp">SQLSetConnectAttr ()</ulink> or <ulink url="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/odbc/htm/odbcsqlsetconnectoption.asp">SQLSetConnectOption ()</ulink>)

<programlisting>
   rc = SQLSetConnectOption (hdbc, SQL_TXN_ISOLATION, SQL_TXN_READ_COMMITTED);
</programlisting>
    or 
<programlisting>
  rc = SQLSetConnectAttr (hdbc, SQL_TXN_ISOLATION, SQL_TXN_READ_COMMITTED, NULL);
</programlisting>

    <para>Constants are : SQL_TXN_READ_UNCOMMITTED,
    SQL_TXN_READ_COMMITTED, SQL_TXN_REPEATABLE_READ,
    SQL_TXN_SERIALIZABLE</para>

	</listitem>

      <listitem>ODBC setup dialog
      option : In Windows there is a drop-down combo box to set the
      default transaction isolation level for a connection.</listitem>

	  <listitem><para>connection string element : You may specify the default
      transaction isolation level for a given connection in it's
      connect string (passed to the <ulink url="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/odbc/htm/odbcsqldriverconnect.asp">SQLDriverConnect ()</ulink> ODBC API). Example
      :</para>

<programlisting>
   SQLDriverConnect (hdbc, hwnd, 
       "DSN=MyDSN;IsolationLevel=Repeatable Read;UID=dba;PWD=dbapwd", SQL_NTS, 
       NULL, 0, 
       NULL,
       SQL_DRIVER_NOPROMPT).
</programlisting>

	<para>The possible options for the connection string are : "Read
    Uncommitted", "Read Committed", "Repeatable Read" and
    "Serializable"</para>
  </listitem>
</itemizedlist>

  </sect4>

  <sect4 id="ptunejdbciso"><title>JDBC</title>

    <para>In the Virtuoso JDBC driver the isolation is set by
	the <ulink url="http://java.sun.com/j2se/1.5.0/docs/api/java/sql/Connection.html#setTransactionIsolation(int)">java.sql.Connection.setTransactionIsolation()</ulink>
    JDBC API.</para>
	
<programlisting>
  conn.setTransactionIsolation (java.sql.Connection.TRANSACTION_SERIALIZABLE)
</programlisting>

	<para>The constants are described in the <ulink url="http://java.sun.com/j2se/1.5.0/docs/api/java/sql/Connection.html#field_summary">Java Docs</ulink></para>

	</sect4>

  <sect4 id="ptunedotnetiso"><title>.Net</title>

    <para>In the VirtuosoClient.NET provider the isolation is set by the
	  <ulink url="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpref/html/frlrfsystemdataidbconnectionclassbegintransactiontopic2.asp">System.Data.IDbConnection.BeginTransaction Method (IsolationLevel)</ulink> function.</para>
	
<programlisting>
  System.Data.IDBTransaction trx = conn.BeginTransaction (System.Data.IsolationLevel.ReadCommitted)
</programlisting>

    <para>The constants are described <ulink url="http://msdn.microsoft.com/library/default.asp?url=/library/en-us/cpref/html/frlrfsystemdataisolationlevelclasstopic.asp">here</ulink></para>

	</sect4>
	</sect3>

	<sect3 id="ptunemetricsdiag"><title>Metrics and Diagnostics</title>

    <para>Metrics are presented at the server and the table level.</para>

    <para>The first metric to check is the output of status ('');</para>

    <para>The paragraph titled transaction status contains the
    following:</para>

    <itemizedlist mark="bullet" spacing="compact">
      <listitem>Count of deadlocks since server startup. There is a total
      number of 
    deadlocks and the number of 2r1w deadlocks. The latter is a
    special case where two transactions both hold a shared lock on
    a resource and one tries to convert the lock to exclusive. This
    situation can kill the transaction attempting to write. Such
    deadlocks are essentially always needless. These are avoided by
    reading for update when first reading the resource. 
	</listitem>

      <listitem>Count of waits since server startup. This is incremented
      every time
    some operation waits for a lock, except if this wait leads into
    a deadlock. If the number of deadlocks is high, let's say over
    5% of the number of waits, transactions are likely badly
    designed and deadlock too often, either because of not locking
    for write at the get go (2r1w) or because of locking resources
    in varying order. 
	  </listitem>

      <listitem>Count of threads running. This is the count of all
      threads that are
    somehow occupied, whether running or waiting. This count minus
    the count of waiting minus the count of threads in vdb is the
    count of threads that in principle could be on CPU. 
	  </listitem>

      <listitem>Count of threads waiting. This is the count of threads
      that are
    waiting for a lock. If this is a high percentage of the count
    of threads running, say over 30%, resources are likely locked
    inefficiently, keeping too many locked. 
	  </listitem>

      <listitem>Count of threads in vdb. This is the count of threads that
      are at the time waiting for I/O either from a remote database or any sort
    of network operation, including access to web services on other
    serbers, access to web pages on other hosts etc. 
	  </listitem>
	  </itemizedlist>

    <para>The system view db.dba.sys_l_stat is used for locating
    bottlenecks.</para>

    <para>The columns are:</para>

    <itemizedlist mark="bullet" spacing="compact">
      <listitem>index - The index being locked. Note that when reading on
      non-primary
    key, the lock is set on the index first, only then on the pk,
    that is if the pk is accessed at all. For all updates however,
    the pk will always be accessed. *locks - The count of times a
    lock was set on this index. 
	  </listitem>

      <listitem>waits - The number of times there was a wait on this
      index. There can
    be more waits than locks because a read committed cursor can
    wait but will not lock, thus all waits do not result in locks. 
	  </listitem>

      <listitem>wait_pct - The percentage of times setting a lock waited.
      (100 *  waits) / locks
	  </listitem>

      <listitem>deadlocks - The number of times a deadlock was signalled
      when attempting to wait for a lock on this index.
	  </listitem>

      <listitem>lock_esc - The number of times a set of row locks was
      converted into a page lock on this index.
	  </listitem>

      <listitem>wait_msecs - The total amount of real time spent by some
      thread waiting for a lock on this index. This may be greater than elapsed time
    because many threads can wait at the same time. 
	  </listitem>
	</itemizedlist>

    <para>All counts and times are cumulative from server startup.</para>

    <para>The interpretation is as follows:</para>

    <para>If deadlocks is high in relation to waits or locks, i.e.
    over 5%, there are many deadlocks and the transaction profiles
    may have to be adjusted. The table where deadlocks is
    incremented is the table where the deadlock was detected but
    the deadlock may involve any number of tables. So, if A and B
    are locked in the order A, B half of the time and B, a the rest
    of the time, then the deadlocks of the tables of A and B will
    be about the same, half of the deadlocks being detected when
    locking A, the other half when locking B.</para>

    <para>If waits is high in relation to locks, for example 20%, then
    there is probably needless contention. Things are kept locked
    needlessly. Use read committed or make shorter transactions or
    lock items with the less contention first.</para>

    <para>Because transaction duration varies, the place with the
    highest count of waits is not necessarily the place with the
    heaviest contention if the waits are short. Use wait_msecs in
    addition to waits for determining where the waiting takes
    place.</para>

    <para>To get a general picture, use the Conductor's Statistics page or simply do:</para>
<programlisting><![CDATA[
select top 5 *
  from sys_l_statt
 order by wait_msecs desc;
]]></programlisting>
    <para>to get a quick view of where time is spent. You can also sort by waits desc or locks desc.</para>

    <sect4 id="ptunemetricdiagsqlissues"><title>SQL Issues</title>

    <para>It is possible to get bad locking behavior if the SQL
    compiler decides to make linear scans of tables or indices and
    the isolation is greater than read committed. The presence of a
    linear scan on an index with locking is often indicated by
    having a large number of lock escalations. If lock_esc is near
    locks then a large part of the activity is likely sequential
    reads.</para>

    <para>The general remedy is to do any long report type
    transactions as read committed unless there are necessary
    reasons to do otherwise.</para>

    <para>To see how a specific query is compiled, one can use the
    explain () function. To change how a query is compiled, one can
    use the table option or option SQL clauses.</para>

	</sect4>
	
    <sect4 id="ptunemetricdiagdynmicsobs"><title>Observation of Dynamics</title>

    <para>Deadlocks and contention do not occur uniformly across time.
    The occurrences will sharply increase after a certain
    application dependent load threshold is exceeded.</para>

    <para>Also, deadlocks will occur in batches. Several transactions
    will first wait for each other and then retry maybe several
    times, maybe only one succeeding at every round. In such worst
    cases, there will be many more deadlocks than successfully
    completed transactions. Optimize locking order and make the
    transactions smaller.</para>

    <para>Looking at how counts change, specially if they change in
    bursts is useful.</para>

	</sect4>

    <sect4 id="ptunemetricdiagsqdebug"><title>Tracing and Debugging</title>

    <para>The most detailed picture of a system's behavior, including
    deadlocks nd other exceptions cn be obtained with profiling. If
    the application is in C, Java or some other compiled language,
    one can use the language's test coverage facility to see
    execution counts for various branches of the code.</para>

    <para>For client applications, using the Virtuoso <link linkend="fn_trace"><function>trace()</function></link>
    function can be useful for seeing which statements signal
    errors. Also the profiling report produced by <link linkend="fn_prof_enable"><function>prof_enable ()</function></link>
    can give useful hints on execution times and error frequencies.
    See Profiling and <link linkend="fn_prof_enable"><function>prof_enable ()</function></link>.</para>

    <para>For PL applications, Virtuoso provides a profiler and test
    coverage facility. This is activated by setting PLDebug = 2 in
    the Parameters section of the ini file and starting the server.
    The functions cov_store () and cov_report are used for
    obtaining a report of the code execution profile thus far. See
    the documentation on "Branch Coverage" for this. The execution
    counts of the lines for exception handler code will show the
    frequency of exceptions. If in a linear block of code a
    specific line has an execution count lower than that of the
    line above it, then this means that the line with the higher
    count has signalled as many exceptions as the difference of the
    higher and lower count indicates.</para>

    <para>The times indicated in the flat and call graph profile
    reports for PL procedures are totals across all threads and
    include time spent waiting for locks. Thus a procedure that
    consumes almost no CPU can appear high on the list if it waits
    for locks, specially if this is so on multiple threads
    concurrently. The times are real times measured on threads
    separately and can thus exceed any elapsed real tiime.</para>

    <para>Single stepping is not generally useful for debugging
    locking since locking issues are timing sensitive.</para>
  </sect4>
  </sect3>
<!--
  Uncommented for Virtuoso 6 Release-->
    <sect3 id="clientlevelresourceaccounting"><title>Client Level Resource Accounting</title>
  <para>Starting with version 6, Virtuoso keeps track of the count of basic database operations performed on behalf of each connected client.
  The resource use statistics are incremented as work on the connection proceeds. The db_activity () function can be called to return the accumulated operation counts and to optionally reset these.
  </para>
  <para>The db_activity built-in function has one optional argument. The possible values are:</para>
  <itemizedlist>
  <listitem>0 - (default) - Return human readable string and reset the counts.</listitem>
  <listitem>1 - return an array of numbers and reset the counts.</listitem>
  <listitem>2 - return a human readable string and leave the counts.</listitem>
  <listitem>3 - return an array of numbers and leave the counts.</listitem>
  </itemizedlist>
  <para>The human readable string is of the form:</para>
  <programlisting><![CDATA[
   22.56MR rnd  1.102GR seq     10P disk  1.341GB /  102.7K messages
  ]]></programlisting>
  <para>The postfixes K, M, G, T mean  10^3 to 10^15, except when applied to bytes, where these mean consecutive powers of 1024.</para>
  <para>The numbers, left to right are the count of random row lookups,
  sequential row lookups, disk page reads, cluster inter-node traffic in
  bytes and cluster inter-node message count as an integer number of
  messages. If the configuration is a single server, the two last are 0.
  </para>
  <para>The random and sequential lookup counts are incremented regardless of whether the row was found or not or whether it matched search conditions.</para>
  <para>If the information is retrieved as an array, the array contains integer numbers representing these plus some more metrics.</para>
  <para>Index  - Meaning</para>
  <itemizedlist>
  <listitem>0 - Random lookups</listitem>
  <listitem>1 - sequential lookups</listitem>
  <listitem>3 - lock waits</listitem>
  <listitem>4 - total msec spent in lock wait on some thread. In a cluster situation, this may be more than elapsed real time.</listitem>
  <listitem>5 - Disk reads</listitem>
  <listitem>6 - Speculative disk reads. These are also counted in disk reads. A speculative read is a prefetch made on the basis of a read history of a disk extent.</listitem>
  <listitem>7 - Cluster inter node message count</listitem>
  <listitem>8 - Total bytes sent in cluster inter node traffic. A message is counted once, when sent.</listitem>
  </itemizedlist>
  <para>If the thread calling db_activity is a web server thread, the totals are automatically reset when beginning the processing of the current web request.</para>
    </sect3>

  </sect2>
